name: Bq table to dataset
description: Extract BQ table in GCS.
inputs:
- {name: bq_client_project_id, type: String, description: Project ID that will be
    used by the BQ client.}
- {name: source_project_id, type: String, description: Project id from where BQ table
    will be extracted.}
- {name: dataset_id, type: String, description: Dataset ID from where the BQ table
    will be extracted.}
- name: table_name
  type: String
  description: |-
    Table name (without project ID and dataset ID) from
    where the BQ table will be extracted.
- name: destination_gcs_uri
  type: String
  description: |-
    GCS URI to use for
    saving query results. Defaults to None.
  optional: true
- {name: dataset_location, type: String, description: BQ dataset location. Defaults
    to "europe-west2"., default: europe-west2, optional: true}
- name: extract_job_config
  type: JsonObject
  description: |-
    Dict containing optional
    parameters required by the bq extract operation. Defaults to None.
    See available parameters here
    https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa
  optional: true
- name: skip_if_exists
  type: Boolean
  description: |-
    If True, skip extracting the dataset if the
    output resource already exists.
  default: "True"
  optional: true
- name: file_pattern
  type: String
  description: |-
    File pattern to append to the
    output files (e.g. `.csv`). Defaults to None.
  optional: true
outputs:
- name: dataset
  type: Dataset
  description: |-
    Output dataset artifact generated by the operation,
    this parameter will be passed automatically by the orchestrator.
- {name: dataset_gcs_prefix, type: String}
- {name: dataset_gcs_uri, type: JsonArray}
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.1' 'loguru==0.7.0' 'kfp==1.8.22' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - |2+

      import kfp
      from kfp.v2 import dsl
      from kfp.v2.dsl import *
      from typing import *

      def bq_table_to_dataset(
          bq_client_project_id: str,
          source_project_id: str,
          dataset_id: str,
          table_name: str,
          dataset: Output[Dataset],
          destination_gcs_uri: Optional[str] = None,
          dataset_location: str = "europe-west2",
          extract_job_config: Optional[dict] = None,
          skip_if_exists: bool = True,
          file_pattern: Optional[str] = None,
      ) -> NamedTuple("Outputs", [("dataset_gcs_prefix", str), ("dataset_gcs_uri", list)]):
          """
          Extract BQ table in GCS.

          Args:
              bq_client_project_id (str): Project ID that will be used by the BQ client.
              source_project_id (str): Project id from where BQ table will be extracted.
              dataset_id (str): Dataset ID from where the BQ table will be extracted.
              table_name (str): Table name (without project ID and dataset ID) from
                  where the BQ table will be extracted.
              dataset (Output[Dataset]): Output dataset artifact generated by the operation,
                  this parameter will be passed automatically by the orchestrator.
              dataset_location (str): BQ dataset location. Defaults to "europe-west2".
              extract_job_config (Optional[dict], optional): Dict containing optional
                  parameters required by the bq extract operation. Defaults to None.
                  See available parameters here
                  https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa
              skip_if_exists (bool): If True, skip extracting the dataset if the
                  output resource already exists.
              file_pattern (Optional[str], optional): File pattern to append to the
                  output files (e.g. `.csv`). Defaults to None.
              destination_gcs_uri (Optional[str], optional): GCS URI to use for
                  saving query results. Defaults to None.

          Returns:
              NamedTuple (str, list): Output dataset directory and its GCS uri
          """
          import os
          from pathlib import Path

          from google.cloud import bigquery
          from google.cloud.exceptions import GoogleCloudError
          from loguru import logger

          # Set uri of output dataset if destination_gcs_uri is provided
          if destination_gcs_uri:
              dataset.uri = destination_gcs_uri

          logger.info(f"Checking if destination exists: {dataset.path}.")
          if Path(dataset.path).exists() and skip_if_exists:
              logger.warning("Destination already exists, skipping table extraction.")
              return

          full_table_id = f"{source_project_id}.{dataset_id}.{table_name}"
          table = bigquery.table.Table(table_ref=full_table_id)

          if extract_job_config is None:
              extract_job_config = {}
          job_config = bigquery.job.ExtractJobConfig(**extract_job_config)
          client = bigquery.client.Client(
              project=bq_client_project_id, location=dataset_location
          )

          # If file_pattern is provided, join dataset.uri with file_pattern
          dataset_uri = dataset.uri
          if file_pattern is not None:
              dataset_uri = os.path.join(dataset_uri, file_pattern)
          dataset_directory = os.path.dirname(dataset_uri)

          logger.info(f"Extract table {table} to {dataset_uri}.")
          extract_job = client.extract_table(
              table,
              dataset_uri,
              job_config=job_config,
          )

          try:
              result = extract_job.result()
              logger.info(f"Table extracted, result: {result}.")
          except GoogleCloudError as e:
              logger.error(e)
              logger.error(extract_job.error_result)
              logger.error(extract_job.errors)
              raise e

          return (dataset_directory, [dataset_uri])

    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - bq_table_to_dataset
