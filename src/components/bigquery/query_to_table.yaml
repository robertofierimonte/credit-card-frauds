name: Bq query to table
description: Run query and create a new BQ table.
inputs:
- {name: query, type: String, description: 'SQL query to execute, results are saved
    in a BQ table.'}
- {name: bq_client_project_id, type: String, description: Project ID that will be
    used by the BQ client.}
- {name: destination_project_id, type: String, description: Project ID where BQ table
    will be created.}
- {name: dataset_id, type: String, description: Dataset ID where BQ table will be
    created.}
- name: table_id
  type: String
  description: |-
    Table name (without project ID and dataset ID) that
    will be created.
- {name: dataset_location, type: String, description: BQ dataset location., default: europe-west2,
  optional: true}
- name: query_job_config
  type: JsonObject
  description: |-
    Dict containing optional parameters required
    by the bq query operation. No need to specify destination param.
    Defaults to {}.
    See available parameters here
    https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJobConfig.html
  default: '{}'
  optional: true
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.1' 'loguru==0.7.0' 'kfp==1.8.22' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - |2+

      import kfp
      from kfp.v2 import dsl
      from kfp.v2.dsl import *
      from typing import *

      def bq_query_to_table(
          query: str,
          bq_client_project_id: str,
          destination_project_id: str,
          dataset_id: str,
          table_id: str,
          dataset_location: str = "europe-west2",
          query_job_config: dict = {},
      ) -> None:
          """
          Run query and create a new BQ table.

          Args:
              query (str): SQL query to execute, results are saved in a BQ table.
              bq_client_project_id (str): Project ID that will be used by the BQ client.
              destination_project_id (str): Project ID where BQ table will be created.
              dataset_id (str): Dataset ID where BQ table will be created.
              table_id (str): Table name (without project ID and dataset ID) that
                  will be created.
              dataset_location (str): BQ dataset location.
              query_job_config (dict): Dict containing optional parameters required
                  by the bq query operation. No need to specify destination param.
                  Defaults to {}.
                  See available parameters here
                  https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJobConfig.html
          """
          from google.cloud import bigquery
          from google.cloud.exceptions import GoogleCloudError
          from loguru import logger

          if (dataset_id is not None) and (table_id is not None):
              dest_table_ref = f"{destination_project_id}.{dataset_id}.{table_id}"
          else:
              dest_table_ref = None
          job_config = bigquery.QueryJobConfig(destination=dest_table_ref, **query_job_config)

          bq_client = bigquery.client.Client(
              project=bq_client_project_id, location=dataset_location
          )
          query_job = bq_client.query(query, job_config=job_config)

          try:
              _ = query_job.result()
              logger.info(f"BQ table {dest_table_ref} created.")
          except GoogleCloudError as e:
              logger.error(e)
              logger.error(query_job.error_result)
              logger.error(query_job.errors)
              raise e

    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - bq_query_to_table
