name: Evaluate model
description: Evaluate a trained model on test data and report goodness metrics.
inputs:
- {name: test_data, type: Dataset, description: Evaluation data as a KFP Dataset object.}
- {name: target_column, type: String, description: Column containing the target column
    for classification.}
- {name: model, type: Model, description: Input trained model as a KFP Model object.}
outputs:
- name: predictions
  type: Dataset
  description: |-
    Model predictions including input columns
    as a KFP Dataset object. This parameter will be passed automatically
    by the orchestrator.
- {name: test_metrics, type: Metrics}
implementation:
  container:
    image: europe-west2-docker.pkg.dev/robertofierimonte-ml-pipe/docker-repo/credit-card-frauds:latest
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.22' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - |2+

      import kfp
      from kfp.v2 import dsl
      from kfp.v2.dsl import *
      from typing import *

      def evaluate_model(
          test_data: Input[Dataset],
          target_column: str,
          model: Input[Model],
          predictions: Output[Dataset],
          test_metrics: Output[Metrics],
      ) -> None:
          """Evaluate a trained model on test data and report goodness metrics.

          Args:
              test_data (Input[Dataset]): Evaluation data as a KFP Dataset object.
              target_column (str): Column containing the target column for classification.
              model (Input[Model]): Input trained model as a KFP Model object.
              predictions (Output[Dataset]): Model predictions including input columns
                  as a KFP Dataset object. This parameter will be passed automatically
                  by the orchestrator.
              metrics (Output[Metrics]): Output metrics for the trained model. This
                  parameter will be passed automatically by the orchestrator and it
                  can be referred to by clicking on the component's execution in
                  the pipeline.
              metrics_artifact (Output[Artifact]): Output metrics Artifact for the trained
                  model. This parameter will be passed automatically by the orchestrator.
          """
          import joblib
          import pandas as pd
          from loguru import logger

          from src.base.model import evaluate_model

          classifier = joblib.load(model.path)

          df_test = pd.read_parquet(test_data.path)
          df_test = df_test.drop(columns=["transaction_id"])
          y_test = df_test.pop(target_column)
          logger.info(f"Loaded test data, shape {df_test.shape}.")

          testing_metrics, _, _ = evaluate_model(classifier, df_test, y_test)
          logger.info("Evaluation completed.")
          for k, v in testing_metrics.items():
              if k != "Precision Recall Curve":
                  test_metrics.log_metric(k, v)

    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - evaluate_model
