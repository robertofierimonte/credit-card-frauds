{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b43938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set True, if you want to use a subset of the data for faster development. \n",
    "# Set False, if you want to use the entire dataset.\n",
    "use_partial_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bee4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import holidays\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# allow more data columns to be shown than by default\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "cwd_path = os.path.abspath(os.getcwd())\n",
    "project_root = os.path.dirname(cwd_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "105f622c",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c95eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_partial_data:\n",
    "    data_path = os.path.join(project_root, 'data/preprocessed_data_small_v001.csv')\n",
    "    # change Datetime from str to datetime\n",
    "    data = pd.read_csv(data_path, index_col=0)\n",
    "    data['Datetime'] = pd.to_datetime(data['Datetime'], yearfirst=True)\n",
    "else:\n",
    "    data_path = os.path.join(project_root, 'data/preprocessed_data_v001.csv')\n",
    "    # change Datetime from str to datetime\n",
    "    data = pd.read_csv(data_path, index_col=0)\n",
    "    data['Datetime'] = pd.to_datetime(data['Datetime'], yearfirst=True)\n",
    "display(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a813c539",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9235e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = data['User'].nunique()\n",
    "print(f'Number of unique users: {n_users}')\n",
    "\n",
    "mean_time_between_transactions = (data['Datetime'] - data['Datetime'].shift(1)).mean()\n",
    "print(f'Mean time between consecutive transactions: {mean_time_between_transactions}')\n",
    "\n",
    "mean_number_transactions_per_day = 1 / (mean_time_between_transactions / dt.timedelta(days=1))\n",
    "print(f'Mean number of transactions per day: {mean_number_transactions_per_day}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e2782f5",
   "metadata": {},
   "source": [
    "## MEAN ENCODING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8733def0",
   "metadata": {},
   "source": [
    "This calculates mean encoding for feature 'MCC' in such a manner that the mean is updated as time goes by and frauds occur. The easy (and fast) way to implement mean encoding here would be to just calculate the mean over the whole data in one go (see below), but this would in principle introduce data leakage, i.e., information from the future when training, which is generally not desirable (good test results, bad results once in production)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e00f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e1ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aux = data[['MCC', 'Is Fraud?']].copy()\n",
    "\n",
    "# one-hot-encode MCC\n",
    "one_hot_encoder = OneHotEncoder(sparse=True).fit(np.array(aux['MCC']).reshape(-1,1))\n",
    "mcc_ohe_sparse = one_hot_encoder.transform(np.array(aux['MCC']).reshape(-1,1))\n",
    "display(mcc_ohe_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(mcc_ohe_sparse) #/ 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ef228",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate expanding sum of MCC occurrences\n",
    "mcc_cumsum = np.cumsum(mcc_ohe_sparse.toarray(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d104ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(mcc_cumsum) / 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_columns = one_hot_encoder.categories_[0].shape[0]\n",
    "nr_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db064db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark where frauds occur for each mcc_ohe column\n",
    "is_fraud_sparse = csr_matrix(np.tile(aux['Is Fraud?'], (nr_columns, 1)).T)\n",
    "is_fraud_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(is_fraud_sparse) #/ 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a62de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(is_fraud_sparse.shape)\n",
    "display(mcc_ohe_sparse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baaddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate where given MCC and fraud co-occur\n",
    "mcc_is_fraud_sparse = is_fraud_sparse.multiply(mcc_ohe_sparse)\n",
    "mcc_is_fraud_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f14fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MCC-specific cumulative sum of frauds\n",
    "mcc_is_fraud_cumsum = np.cumsum(mcc_is_fraud_sparse.toarray(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a61f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(mcc_is_fraud_cumsum) / 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef6dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of zeros in mcc_is_fraud_cumsum\n",
    "nr_entries = mcc_is_fraud_cumsum.shape[0]*mcc_is_fraud_cumsum.shape[1]\n",
    "(nr_entries - np.count_nonzero(mcc_is_fraud_cumsum)) / nr_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate expanding proportion of frauds\n",
    "mcc_fraud_proportion = np.divide(mcc_is_fraud_cumsum, mcc_cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a081a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory by clearing variables\n",
    "del mcc_is_fraud_cumsum, mcc_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199969d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(mcc_fraud_proportion) / 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12272cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_columns = [col[3:] for col in one_hot_encoder.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_columns = np.array(mcc_columns)\n",
    "mcc_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60238e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fraud_proportions = [mcc_fraud_proportion[count, np.where(mcc_columns==str(aux.loc[ix,'MCC']))[0][0]] \n",
    "                     for count, ix in enumerate(aux.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047372a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MCC_mean_encoding'] = fraud_proportions\n",
    "display(data[['MCC', 'MCC_mean_encoding']].tail(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5064418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check, these should match each MCC's final entry in data['MCC_mean_encoding']\n",
    "display(data.groupby('MCC').mean()['Is Fraud?'].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad55c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory by clearing variable\n",
    "del mcc_fraud_proportion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41ce8f86",
   "metadata": {},
   "source": [
    "#### Below is an alternative way of calculating the mean encoding, but it is too memory demanding for use with full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CAN BE USED ONLY IF use_partial_data = True\n",
    "\n",
    "# time different parts of the cell execution\n",
    "t0 = time.time()\n",
    "aux = data[['MCC', 'Is Fraud?']].copy()\n",
    "\n",
    "# one-hot-encode MCC\n",
    "one_hot_encoder = OneHotEncoder(sparse=True).fit(np.array(aux['MCC']).reshape(-1,1))\n",
    "mcc_columns = [col[3:] for col in one_hot_encoder.get_feature_names()]\n",
    "aux[mcc_columns] = one_hot_encoder.transform(np.array(aux['MCC']).reshape(-1,1)).toarray()\n",
    "t1 = time.time()\n",
    "t_block = (t1-t0) / 60\n",
    "print(f'MCC columns one-hot-encoding done in {t_block} minutes.')\n",
    "\n",
    "# calculate expanding sum of MCC occurrences\n",
    "mcc_sum_columns = [col+'_sum' for col in mcc_columns]\n",
    "aux[mcc_sum_columns] = aux[mcc_columns].expanding().sum()\n",
    "t2 = time.time()\n",
    "t_block = (t2-t1) / 60\n",
    "print(f'mcc_sum_columns calculated in {t_block} minutes.')\n",
    "\n",
    "# calculate where given MCC and fraud co-occur\n",
    "mcc_is_fraud_columns = [col+'_is_fraud' for col in mcc_columns]\n",
    "is_fraud_tiled = np.tile(aux['Is Fraud?'], (len(mcc_columns), 1)).T\n",
    "aux[mcc_is_fraud_columns] = np.multiply(is_fraud_tiled, aux[mcc_columns])\n",
    "t3 = time.time()\n",
    "t_block = (t3-t2) / 60\n",
    "print(f'mcc_is_fraud_columns calculated in {t_block} minutes.')\n",
    "\n",
    "# calculate MCC-specific expanding sum of frauds    \n",
    "mcc_is_fraud_sum_columns = [col+'_is_fraud_sum' for col in mcc_columns]\n",
    "aux[mcc_is_fraud_sum_columns] = aux[mcc_is_fraud_columns].expanding().sum()\n",
    "t4 = time.time()\n",
    "t_block = (t4-t3) / 60\n",
    "print(f'mcc_is_fraud_sum_columns calculated {t_block} minutes.')\n",
    "\n",
    "# calculate expanding proportion of frauds\n",
    "mcc_proportion_frauds_columns = [col+'_proportion_frauds' for col in mcc_columns]\n",
    "aux[mcc_proportion_frauds_columns] = np.divide(aux[mcc_is_fraud_sum_columns], aux[mcc_sum_columns])\n",
    "t5 = time.time()\n",
    "t_block = (t5-t4) / 60\n",
    "t_cell = (t5-t0) / 60\n",
    "print(f'mcc_proportion_frauds_columns calculated {t_block} minutes.')\n",
    "\n",
    "# collect correct fraud proportion for each index matching the original data\n",
    "fraud_proportions = [aux.loc[ix, str(aux.loc[ix,'MCC'])+'_proportion_frauds'] for ix in aux.index]\n",
    "\n",
    "print(f'Cell executed in {t_cell} minutes.')\n",
    "\n",
    "display(aux[mcc_proportion_frauds_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check, these should match with the final row of aux['proportion_frauds']\n",
    "data.groupby('MCC').mean()['Is Fraud?'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671eb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mean encoding for MCC to the main dataframe\n",
    "data['MCC_mean_encoding'] = fraud_proportions\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "405b92e2",
   "metadata": {},
   "source": [
    "# Keep track of fraud proportions by transaction type over different moving time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9eec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine chip and swipe transactions into card_present transaction\n",
    "data['card_present_transaction'] = data[['Chip Transaction', 'Swipe Transaction']].max(axis=1)\n",
    "#display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure data is sorted by datetime\n",
    "data = data.sort_values(by=['Datetime'])\n",
    "#display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1aa599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary features\n",
    "data['fraud_swipe'] = ((data['Is Fraud?']==1) & (data['Swipe Transaction']==1)).astype(int)\n",
    "data['fraud_chip'] = ((data['Is Fraud?']==1) & (data['Chip Transaction']==1)).astype(int)\n",
    "data['fraud_online'] = ((data['Is Fraud?']==1) & (data['Online Transaction']==1)).astype(int)\n",
    "data['fraud_card_present'] = ((data['Is Fraud?']==1) & (data['card_present_transaction']==1)).astype(int)\n",
    "\n",
    "#display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# datetime needed for index with rolling calculations\n",
    "data = data.reset_index().set_index('Datetime')\n",
    "    \n",
    "# calculate rolling averages for different fraud types    \n",
    "data['fraud_rolling_mean_30_days'] = data['Is Fraud?'].rolling('30d', closed='left').mean()\n",
    "data['fraud_rolling_mean_60_days'] = data['Is Fraud?'].rolling('60d', closed='left').mean()\n",
    "data['fraud_rolling_mean_365_days'] = data['Is Fraud?'].rolling('365d', closed='left').mean()\n",
    "data['fraud_rolling_mean_2_years'] = data['Is Fraud?'].rolling('730d', closed='left').mean()\n",
    "\n",
    "data['fraud_swipe_rolling_mean_30_days'] = data['fraud_swipe'].rolling('30d', closed='left').mean()\n",
    "data['fraud_swipe_rolling_mean_60_days'] = data['fraud_swipe'].rolling('60d', closed='left').mean()\n",
    "data['fraud_swipe_rolling_mean_365_days'] = data['fraud_swipe'].rolling('365d', closed='left').mean()\n",
    "data['fraud_swipe_rolling_mean_2_years'] = data['fraud_swipe'].rolling('730d', closed='left').mean()\n",
    "\n",
    "data['fraud_chip_rolling_mean_30_days'] = data['fraud_chip'].rolling('30d', closed='left').mean()\n",
    "data['fraud_chip_rolling_mean_60_days'] = data['fraud_chip'].rolling('60d', closed='left').mean()\n",
    "data['fraud_chip_rolling_mean_365_days'] = data['fraud_chip'].rolling('365d', closed='left').mean()\n",
    "data['fraud_chip_rolling_mean_2_years'] = data['fraud_chip'].rolling('730d', closed='left').mean()\n",
    "\n",
    "data['fraud_online_rolling_mean_30_days'] = data['fraud_online'].rolling('30d', closed='left').mean()\n",
    "data['fraud_online_rolling_mean_60_days'] = data['fraud_online'].rolling('60d', closed='left').mean()\n",
    "data['fraud_online_rolling_mean_365_days'] = data['fraud_online'].rolling('365d', closed='left').mean()\n",
    "data['fraud_online_rolling_mean_2_years'] = data['fraud_online'].rolling('730d', closed='left').mean()\n",
    "\n",
    "data['fraud_card_present_rolling_mean_30_days'] = data['fraud_card_present'].rolling('30d', closed='left').mean()\n",
    "data['fraud_card_present_rolling_mean_60_days'] = data['fraud_card_present'].rolling('60d', closed='left').mean()\n",
    "data['fraud_card_present_rolling_mean_365_days'] = data['fraud_card_present'].rolling('365d', closed='left').mean()\n",
    "data['fraud_card_present_rolling_mean_2_years'] = data['fraud_card_present'].rolling('730d', closed='left').mean()\n",
    "\n",
    "# reset index back to original\n",
    "data = data.reset_index().set_index('index')\n",
    "\n",
    "# add delay in information (typically it would not be immediately known/verified whether any given transaction \n",
    "# was a fraud or not)\n",
    "delay_days = 7 # how many days information about frauds is delayed\n",
    "n_rows_to_shift = int(round(delay_days * mean_number_transactions_per_day, 0))\n",
    "\n",
    "# shift rows \n",
    "data['fraud_rolling_mean_30_days'] = data['fraud_rolling_mean_30_days'].shift(n_rows_to_shift)\n",
    "data['fraud_rolling_mean_60_days'] = data['fraud_rolling_mean_60_days'].shift(n_rows_to_shift)\n",
    "data['fraud_rolling_mean_365_days'] = data['fraud_rolling_mean_365_days'].shift(n_rows_to_shift)\n",
    "data['fraud_rolling_mean_2_years']  = data['fraud_rolling_mean_2_years'].shift(n_rows_to_shift)\n",
    "\n",
    "data['fraud_swipe_rolling_mean_30_days'] = data['fraud_swipe_rolling_mean_30_days'].shift(n_rows_to_shift)\n",
    "data['fraud_swipe_rolling_mean_60_days'] = data['fraud_swipe_rolling_mean_60_days'].shift(n_rows_to_shift)\n",
    "data['fraud_swipe_rolling_mean_365_days'] = data['fraud_swipe_rolling_mean_365_days'].shift(n_rows_to_shift)\n",
    "data['fraud_swipe_rolling_mean_2_years']  = data['fraud_swipe_rolling_mean_2_years'].shift(n_rows_to_shift)\n",
    "\n",
    "data['fraud_chip_rolling_mean_30_days'] = data['fraud_chip_rolling_mean_30_days'].shift(n_rows_to_shift)\n",
    "data['fraud_chip_rolling_mean_60_days'] = data['fraud_chip_rolling_mean_60_days'].shift(n_rows_to_shift)\n",
    "data['fraud_chip_rolling_mean_365_days'] = data['fraud_chip_rolling_mean_365_days'].shift(n_rows_to_shift)\n",
    "data['fraud_chip_rolling_mean_2_years']  = data['fraud_chip_rolling_mean_2_years'].shift(n_rows_to_shift)\n",
    "\n",
    "data['fraud_online_rolling_mean_30_days'] = data['fraud_online_rolling_mean_30_days'].shift(n_rows_to_shift)\n",
    "data['fraud_online_rolling_mean_60_days'] = data['fraud_online_rolling_mean_60_days'].shift(n_rows_to_shift)\n",
    "data['fraud_online_rolling_mean_365_days'] = data['fraud_online_rolling_mean_365_days'].shift(n_rows_to_shift)\n",
    "data['fraud_online_rolling_mean_2_years']  = data['fraud_online_rolling_mean_2_years'].shift(n_rows_to_shift)\n",
    "\n",
    "data['fraud_card_present_rolling_mean_30_days'] = data['fraud_card_present_rolling_mean_30_days'].shift(n_rows_to_shift)\n",
    "data['fraud_card_present_rolling_mean_60_days'] = data['fraud_card_present_rolling_mean_60_days'].shift(n_rows_to_shift)\n",
    "data['fraud_card_present_rolling_mean_365_days'] = data['fraud_card_present_rolling_mean_365_days'].shift(n_rows_to_shift)\n",
    "data['fraud_card_present_rolling_mean_2_years']  = data['fraud_card_present_rolling_mean_2_years'].shift(n_rows_to_shift)\n",
    "\n",
    "#display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60cd164",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# compare different fraud types' recent averages to longer term averages (find spikes and lows)\n",
    "data['fraud_rolling_30_days_relative_to_365_days'] = data['fraud_rolling_mean_30_days'] / data['fraud_rolling_mean_365_days']\n",
    "data['fraud_rolling_30_days_relative_to_2_years'] = data['fraud_rolling_mean_30_days'] / data['fraud_rolling_mean_2_years']\n",
    "data['fraud_rolling_60_days_relative_to_365_days'] = data['fraud_rolling_mean_60_days'] / data['fraud_rolling_mean_365_days']\n",
    "data['fraud_rolling_60_days_relative_to_2_years'] = data['fraud_rolling_mean_60_days'] / data['fraud_rolling_mean_2_years']\n",
    "\n",
    "data['fraud_swipe_rolling_30_days_relative_to_365_days'] = data['fraud_swipe_rolling_mean_30_days'] / data['fraud_swipe_rolling_mean_365_days']\n",
    "data['fraud_swipe_rolling_30_days_relative_to_2_years'] = data['fraud_swipe_rolling_mean_30_days'] / data['fraud_swipe_rolling_mean_2_years']\n",
    "data['fraud_swipe_rolling_60_days_relative_to_365_days'] = data['fraud_swipe_rolling_mean_60_days'] / data['fraud_swipe_rolling_mean_365_days']\n",
    "data['fraud_swipe_rolling_60_days_relative_to_2_years'] = data['fraud_swipe_rolling_mean_60_days'] / data['fraud_swipe_rolling_mean_2_years']\n",
    "\n",
    "data['fraud_chip_rolling_30_days_relative_to_365_days'] = data['fraud_chip_rolling_mean_30_days'] / data['fraud_chip_rolling_mean_365_days']\n",
    "data['fraud_chip_rolling_30_days_relative_to_2_years'] = data['fraud_chip_rolling_mean_30_days'] / data['fraud_chip_rolling_mean_2_years']\n",
    "data['fraud_chip_rolling_60_days_relative_to_365_days'] = data['fraud_chip_rolling_mean_60_days'] / data['fraud_chip_rolling_mean_365_days']\n",
    "data['fraud_chip_rolling_60_days_relative_to_2_years'] = data['fraud_chip_rolling_mean_60_days'] / data['fraud_chip_rolling_mean_2_years']\n",
    "\n",
    "data['fraud_online_rolling_30_days_relative_to_365_days'] = data['fraud_online_rolling_mean_30_days'] / data['fraud_online_rolling_mean_365_days']\n",
    "data['fraud_online_rolling_30_days_relative_to_2_years'] = data['fraud_online_rolling_mean_30_days'] / data['fraud_online_rolling_mean_2_years']\n",
    "data['fraud_online_rolling_60_days_relative_to_365_days'] = data['fraud_online_rolling_mean_60_days'] / data['fraud_online_rolling_mean_365_days']\n",
    "data['fraud_online_rolling_60_days_relative_to_2_years'] = data['fraud_online_rolling_mean_60_days'] / data['fraud_online_rolling_mean_2_years']\n",
    "\n",
    "data['fraud_card_present_rolling_30_days_relative_to_365_days'] = data['fraud_card_present_rolling_mean_30_days'] / data['fraud_card_present_rolling_mean_365_days']\n",
    "data['fraud_card_present_rolling_30_days_relative_to_2_years'] = data['fraud_card_present_rolling_mean_30_days'] / data['fraud_card_present_rolling_mean_2_years']\n",
    "data['fraud_card_present_rolling_60_days_relative_to_365_days'] = data['fraud_card_present_rolling_mean_60_days'] / data['fraud_card_present_rolling_mean_365_days']\n",
    "data['fraud_card_present_rolling_60_days_relative_to_2_years'] = data['fraud_card_present_rolling_mean_60_days'] / data['fraud_card_present_rolling_mean_2_years']\n",
    "\n",
    "# rolling proportions relative all frauds\n",
    "data['fraud_swipe_rolling_30_days_relative_to_all_frauds'] = data['fraud_swipe_rolling_mean_30_days'] / data['fraud_rolling_mean_30_days']\n",
    "data['fraud_chip_rolling_30_days_relative_to_all_frauds'] = data['fraud_chip_rolling_mean_30_days'] / data['fraud_rolling_mean_30_days']\n",
    "data['fraud_online_rolling_30_days_relative_to_all_frauds'] = data['fraud_online_rolling_mean_30_days'] / data['fraud_rolling_mean_30_days']\n",
    "data['fraud_card_present_rolling_30_days_relative_to_all_frauds'] = data['fraud_card_present_rolling_mean_30_days'] / data['fraud_rolling_mean_30_days']\n",
    "\n",
    "data['fraud_swipe_rolling_60_days_relative_to_all_frauds'] = data['fraud_swipe_rolling_mean_60_days'] / data['fraud_rolling_mean_60_days']\n",
    "data['fraud_chip_rolling_60_days_relative_to_all_frauds'] = data['fraud_chip_rolling_mean_60_days'] / data['fraud_rolling_mean_60_days']\n",
    "data['fraud_online_rolling_60_days_relative_to_all_frauds'] = data['fraud_online_rolling_mean_60_days'] / data['fraud_rolling_mean_60_days']\n",
    "data['fraud_card_present_rolling_60_days_relative_to_all_frauds'] = data['fraud_card_present_rolling_mean_60_days'] / data['fraud_rolling_mean_60_days']\n",
    "\n",
    "data['fraud_swipe_rolling_365_days_relative_to_all_frauds'] = data['fraud_swipe_rolling_mean_365_days'] / data['fraud_rolling_mean_365_days']\n",
    "data['fraud_chip_rolling_365_days_relative_to_all_frauds'] = data['fraud_chip_rolling_mean_365_days'] / data['fraud_rolling_mean_365_days']\n",
    "data['fraud_online_rolling_365_days_relative_to_all_frauds'] = data['fraud_online_rolling_mean_365_days'] / data['fraud_rolling_mean_365_days']\n",
    "data['fraud_card_present_rolling_365_days_relative_to_all_frauds'] = data['fraud_card_present_rolling_mean_365_days'] / data['fraud_rolling_mean_365_days']\n",
    "\n",
    "data['fraud_swipe_rolling_2_years_relative_to_all_frauds'] = data['fraud_swipe_rolling_mean_2_years'] / data['fraud_rolling_mean_2_years']\n",
    "data['fraud_chip_rolling_2_years_relative_to_all_frauds'] = data['fraud_chip_rolling_mean_2_years'] / data['fraud_rolling_mean_2_years']\n",
    "data['fraud_online_rolling_2_years_relative_to_all_frauds'] = data['fraud_online_rolling_mean_2_years'] / data['fraud_rolling_mean_2_years']\n",
    "data['fraud_card_present_rolling_2_years_relative_to_all_frauds'] = data['fraud_card_present_rolling_mean_2_years'] / data['fraud_rolling_mean_2_years']\n",
    "\n",
    "#display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fa84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop auxiliary features that should not be included in the data, e.g. 'fraud_swipe', that would indicate fraudulent transactions\n",
    "drop_features = ['fraud_swipe', 'fraud_chip', 'fraud_online', 'fraud_card_present']\n",
    "data = data.drop(drop_features, axis=1)\n",
    "#display(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f61b30ee",
   "metadata": {},
   "source": [
    "## Time-related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get the hour component of the time\n",
    "data['hour'] = data['Time'].apply(lambda x: x[0:2]).astype(int)\n",
    "# get day of week\n",
    "data['day_of_week'] = data['Datetime'].apply(lambda x: x.weekday())\n",
    "\n",
    "# create cyclical time features (e.g. hour 23 is closer to 0 than 21)\n",
    "data[\"hour_sin\"] = data[\"hour\"].apply(lambda x: np.sin((x / 24) * 2 * np.pi))\n",
    "data[\"hour_cos\"] = data[\"hour\"].apply(lambda x: np.cos((x / 24) * 2 * np.pi))\n",
    "\n",
    "data[\"month_sin\"] = data[\"Month\"].apply(lambda x: np.sin(((x - 1) / 12) * 2 * np.pi))\n",
    "data[\"month_cos\"] = data[\"Month\"].apply(lambda x: np.cos(((x - 1) / 12) * 2 * np.pi))\n",
    "\n",
    "data[\"day_of_week_sin\"] = data[\"day_of_week\"].apply(lambda x: np.sin((x / 7) * 2 * np.pi))\n",
    "data[\"day_of_week_cos\"] = data[\"day_of_week\"].apply(lambda x: np.cos((x / 7) * 2 * np.pi))\n",
    "\n",
    "# add indicator for holidays\n",
    "us_holidays = holidays.US()\n",
    "data[\"is_holiday\"] = data[\"Datetime\"].apply(lambda x: x in us_holidays)\n",
    "data[\"is_holiday\"] = data[\"is_holiday\"].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# add indicator for weekends\n",
    "data[\"weekend\"] = (data[\"day_of_week\"] == 5) | (data[\"day_of_week\"] == 6)\n",
    "data['weekend'] = data[\"weekend\"].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# add indicator for whether the year is 2015 or later (known change in transaction type)\n",
    "data['is_2015_or_later'] = data['Year']>=2015\n",
    "data['is_2015_or_later'] = data['is_2015_or_later'].apply(lambda x: 1 if x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7378895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure data is sorted by datetime\n",
    "data = data.sort_values(by=['Datetime'])\n",
    "#display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064997d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_vars = list(locals().items())\n",
    "#for var, obj in local_vars:\n",
    "#    print(var, sys.getsizeof(obj))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ede8915",
   "metadata": {},
   "source": [
    "## User-specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599cf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate user-specific features over time\n",
    "for user in tqdm(data['User'].unique()):\n",
    "    \n",
    "    # transaction history for a given user\n",
    "    aux = data.loc[data['User']==user, ['Datetime', 'Amount']].copy()\n",
    "    \n",
    "    ###\n",
    "    # AUXILIARY VARIABLES AND TRANSFORMATIONS\n",
    "    ###\n",
    "    \n",
    "    # datetime needed for index with rolling calculations\n",
    "    aux = aux.reset_index().set_index('Datetime')\n",
    "    # auxiliary variables for computing transaction frequency for given time range\n",
    "    aux['transaction_count_auxiliary'] = 1\n",
    "    aux['auxiliary_365'] = 365\n",
    "    aux['auxiliary_30'] = 30\n",
    "    aux['auxiliary_7'] = 7\n",
    "    aux['auxiliary_2'] = 2\n",
    "    \n",
    "    ###\n",
    "    # SPENDING\n",
    "    ###\n",
    "    \n",
    "    # calculate mean 'Amount' over time, i.e., given transaction, what's been the mean amount spent up to that point\n",
    "    # not including the current transaction itself\n",
    "    aux['mean_amount'] = aux['Amount'].expanding().mean()\n",
    "    # shift one row down so that the mean is given only for past transactions\n",
    "    aux['mean_amount'] = aux['mean_amount'].shift(1, fill_value=0)\n",
    "    # mean transaction amount for various time windows\n",
    "    aux['mean_amount_last_year'] = aux['Amount'].rolling('365d', closed='left').mean().fillna(0)\n",
    "    aux['mean_amount_last_30_days'] = aux['Amount'].rolling('30d', closed='left').mean().fillna(0)\n",
    "    aux['mean_amount_last_7_days'] = aux['Amount'].rolling('7d', closed='left').mean().fillna(0)\n",
    "    aux['mean_amount_last_2_days'] = aux['Amount'].rolling('2d', closed='left').mean().fillna(0)\n",
    "    aux['mean_amount_last_1_days'] = aux['Amount'].rolling('1d', closed='left').mean().fillna(0)\n",
    "    # mean amount spent over short time periods relative to longer time periods\n",
    "    aux['mean_amount_last_7_days_relative_to_last_year'] = aux['mean_amount_last_7_days'] / aux['mean_amount_last_year']\n",
    "    aux['mean_amount_last_2_days_relative_to_last_year'] = aux['mean_amount_last_2_days'] / aux['mean_amount_last_year']\n",
    "    aux['mean_amount_last_1_days_relative_to_last_year'] = aux['mean_amount_last_1_days'] / aux['mean_amount_last_year']\n",
    "    aux['mean_amount_last_7_days_relative_to_last_30_days'] = aux['mean_amount_last_7_days'] / aux['mean_amount_last_30_days']\n",
    "    aux['mean_amount_last_2_days_relative_to_last_30_days'] = aux['mean_amount_last_2_days'] / aux['mean_amount_last_30_days']\n",
    "    aux['mean_amount_last_1_days_relative_to_last_30_days'] = aux['mean_amount_last_1_days'] / aux['mean_amount_last_30_days']\n",
    "    \n",
    "    ###\n",
    "    # TRANSACTION FREQUENCY\n",
    "    ###\n",
    "    \n",
    "    # datetime for given customer's first ever transaction\n",
    "    first_transaction_datetime = aux.index[0]\n",
    "    # increasing count of total transactions for given customer\n",
    "    aux['transaction_count'] = np.arange(1,aux.shape[0]+1)\n",
    "    # auxiliary variable keeping track of time since first transaction\n",
    "    aux['days_since_first_transaction'] = (aux.index - first_transaction_datetime) / dt.timedelta(days=1)\n",
    "    # all time transaction frequency not including current transaction\n",
    "    aux['transaction_frequency_all'] = np.where(aux['days_since_first_transaction']>0, \n",
    "                                                (aux['transaction_count']-1) / aux['days_since_first_transaction'], \n",
    "                                                0)\n",
    "    \n",
    "    # transaction frequency (number of transactions per day) for the last year not including current transaction\n",
    "    aux['transaction_frequency_last_year'] = ( aux['transaction_count_auxiliary'].rolling('365d', closed='left').sum() \n",
    "                                              / (aux[['auxiliary_365', 'days_since_first_transaction']].min(axis=1)) ).fillna(0)\n",
    "    \n",
    "    # transaction frequency (number of transactions per day) for the last 30 days not including current transaction\n",
    "    aux['transaction_frequency_last_30_days'] = ( aux['transaction_count_auxiliary'].rolling('30d', closed='left').sum() \n",
    "                                                 / (aux[['auxiliary_30', 'days_since_first_transaction']].min(axis=1)) ).fillna(0)\n",
    "\n",
    "    # transaction frequency (number of transactions per day) for the last 7 days not including current transaction\n",
    "    aux['transaction_frequency_last_7_days'] = ( aux['transaction_count_auxiliary'].rolling('7d', closed='left').sum() \n",
    "                                                / (aux[['auxiliary_7', 'days_since_first_transaction']].min(axis=1)) ).fillna(0)\n",
    "    \n",
    "    # transaction frequency for the last 48 hours not including current transaction\n",
    "    aux['transaction_frequency_last_2_days'] = ( aux['transaction_count_auxiliary'].rolling('2d', closed='left').sum() \n",
    "                                                / (aux[['auxiliary_2', 'days_since_first_transaction']].min(axis=1)) ).fillna(0)\n",
    "    \n",
    "    # transaction frequency for the last 24 hours not including current transaction\n",
    "    aux['transaction_frequency_last_1_days'] = ( aux['transaction_count_auxiliary'].rolling('1d', closed='left').sum()\n",
    "                                                / (aux[['transaction_count_auxiliary', 'days_since_first_transaction']].min(axis=1)) ).fillna(0)\n",
    "    \n",
    "    # calculate statistic related to transaction frequencies\n",
    "    aux['1_days_transaction_frequency_relative_to_last_30_days'] = aux['transaction_frequency_last_1_days'] / aux['transaction_frequency_last_30_days']\n",
    "    aux['1_days_transaction_frequency_relative_to_last_year'] = aux['transaction_frequency_last_1_days'] / aux['transaction_frequency_last_year']\n",
    "    aux['2_days_transaction_frequency_relative_to_last_30_days'] = aux['transaction_frequency_last_2_days'] / aux['transaction_frequency_last_30_days']\n",
    "    aux['2_days_transaction_frequency_relative_to_last_year'] = aux['transaction_frequency_last_2_days'] / aux['transaction_frequency_last_year']\n",
    "    aux['7_days_transaction_frequency_relative_to_last_30_days'] = aux['transaction_frequency_last_7_days'] / aux['transaction_frequency_last_30_days']\n",
    "    aux['7_days_transaction_frequency_relative_to_last_year'] = aux['transaction_frequency_last_7_days'] / aux['transaction_frequency_last_year']\n",
    "    \n",
    "    new_features = ['mean_amount', 'mean_amount_last_year', 'mean_amount_last_30_days', 'mean_amount_last_7_days',\n",
    "                    'mean_amount_last_2_days', 'mean_amount_last_1_days', 'mean_amount_last_7_days_relative_to_last_year', \n",
    "                    'mean_amount_last_2_days_relative_to_last_year', 'mean_amount_last_1_days_relative_to_last_year',\n",
    "                    'mean_amount_last_7_days_relative_to_last_30_days', 'mean_amount_last_2_days_relative_to_last_30_days',\n",
    "                    'mean_amount_last_1_days_relative_to_last_30_days', 'transaction_count', 'days_since_first_transaction', \n",
    "                    'transaction_frequency_all', 'transaction_frequency_last_year', 'transaction_frequency_last_30_days', \n",
    "                    'transaction_frequency_last_7_days', 'transaction_frequency_last_2_days', \n",
    "                    'transaction_frequency_last_1_days', '1_days_transaction_frequency_relative_to_last_30_days', \n",
    "                    '1_days_transaction_frequency_relative_to_last_year', '2_days_transaction_frequency_relative_to_last_30_days', \n",
    "                    '2_days_transaction_frequency_relative_to_last_year', '7_days_transaction_frequency_relative_to_last_30_days', \n",
    "                    '7_days_transaction_frequency_relative_to_last_year']\n",
    "    \n",
    "    # reset index to original index for adding features to main dataframe\n",
    "    aux = aux.reset_index().set_index('index')\n",
    "    # add features to main dataframe\n",
    "    data.loc[aux.index, new_features] = aux[new_features]\n",
    "\n",
    "#display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861004b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_partial_data:\n",
    "    data_path = os.path.join(project_root, 'data/preprocessed_data_with_feature_engineering_small.csv')\n",
    "else:\n",
    "    data_path = os.path.join(project_root, 'data/preprocessed_data_with_feature_engineering.csv')\n",
    "data.to_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f44f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
