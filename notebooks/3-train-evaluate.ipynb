{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from google.cloud import aiplatform, storage\n",
    "from kfp.dsl import Artifact, Dataset, Metrics, Model\n",
    "from loguru import logger\n",
    "\n",
    "module_path = os.path.abspath(\"..\")\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from src.base.utilities import generate_query, read_yaml\n",
    "from src.utils.notebooks import patch_kfp\n",
    "\n",
    "patch_kfp()\n",
    "\n",
    "from src.components.bigquery import execute_query, bq_table_to_dataset\n",
    "from src.components.data import get_data_version\n",
    "from src.components.model import train_evaluate_model\n",
    "\n",
    "# Remove default logger and set level to DEBUG\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../src/pipelines/training/payloads/dev.json\", \"r\") as f:\n",
    "    payload = json.load(f)\n",
    "    payload = payload[\"data\"]\n",
    "\n",
    "project_id = os.environ.get(\"VERTEX_PROJECT_ID\")\n",
    "dataset_id = payload[\"dataset_id\"]\n",
    "dataset_location = payload[\"dataset_location\"]\n",
    "data_version = payload[\"data_version\"]\n",
    "create_replace_tables = payload[\"create_replace_tables\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_params = read_yaml(\"../src/pipelines/configuration/params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_folder = Path.cwd().parent / \"src/pipelines/training/queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_version = get_data_version(\n",
    "    payload_data_version=data_version,\n",
    "    project_id=project_id,\n",
    "    dataset_id=dataset_id,\n",
    "    dataset_location=dataset_location,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = f\"{project_id}.{dataset_id}_{data_version}\"\n",
    "train_set_table = f\"{dataset_name}.training\"\n",
    "valid_set_table = f\"{dataset_name}.validation\"\n",
    "test_set_table = f\"{dataset_name}.testing\"\n",
    "preprocessed_table = f\"{dataset_name}.preprocessed\"\n",
    "\n",
    "train_valid_test_query = generate_query(\n",
    "    queries_folder / \"q_train_valid_test_split.sql\",\n",
    "    source_table=preprocessed_table,\n",
    "    valid_size=0.15,\n",
    "    test_size=0.15,\n",
    "    training_table=train_set_table,\n",
    "    validation_table=valid_set_table,\n",
    "    testing_table=test_set_table,\n",
    "    create_replace_table=create_replace_tables,\n",
    ")\n",
    "\n",
    "query_job_config = json.dumps(dict(use_query_cache=True))\n",
    "\n",
    "\n",
    "execute_query(\n",
    "    query=train_valid_test_query,\n",
    "    bq_client_project_id=project_id,\n",
    "    query_job_config=query_job_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_files_path = os.environ[\"VERTEX_PIPELINE_FILES_GCS_PATH\"].replace(\"gs://\", \"\")\n",
    "bucket = pipeline_files_path.split(\"/\")[0]\n",
    "logger.debug(f\"GCS bucket: gs://{bucket} .\")\n",
    "prefix = pipeline_files_path.split(\"/\", 1)[1] + \"/local_run\"\n",
    "logger.debug(f\"GCS prefix: {prefix} .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(\n",
    "    name=\"training_set\", uri=f\"gs://{bucket}/{prefix}/training_set/\"\n",
    ")\n",
    "validation_set = Dataset(\n",
    "    name=\"validation_set\",uri=f\"gs://{bucket}/{prefix}/validation_set/\"\n",
    ")\n",
    "test_set = Dataset(\n",
    "    name=\"test_set\", uri=f\"gs://{bucket}/{prefix}/test_set/\"\n",
    ")\n",
    "\n",
    "_ = bq_table_to_dataset(\n",
    "    bq_client_project_id=project_id,\n",
    "    source_project_id=project_id,\n",
    "    dataset_id=f\"{dataset_id}_{data_version}\",\n",
    "    table_name=train_set_table.rsplit(\".\", 1)[1],\n",
    "    dataset_location=dataset_location,\n",
    "    file_pattern=\"file_*\",\n",
    "    extract_job_config=dict(destination_format=\"PARQUET\"),\n",
    "    skip_if_exists=True,\n",
    "    # The arguments below should not be supplied when running the component on Vertex AI\n",
    "    dataset=training_set,\n",
    ")\n",
    "\n",
    "_ = bq_table_to_dataset(\n",
    "    bq_client_project_id=project_id,\n",
    "    source_project_id=project_id,\n",
    "    dataset_id=f\"{dataset_id}_{data_version}\",\n",
    "    table_name=valid_set_table.rsplit(\".\", 1)[1],\n",
    "    dataset_location=dataset_location,\n",
    "    file_pattern=\"file_*\",\n",
    "    extract_job_config=dict(destination_format=\"PARQUET\"),\n",
    "    skip_if_exists=True,\n",
    "    # The arguments below should not be supplied when running the component on Vertex AI\n",
    "    dataset=validation_set,\n",
    ")\n",
    "\n",
    "_ = bq_table_to_dataset(\n",
    "    bq_client_project_id=project_id,\n",
    "    source_project_id=project_id,\n",
    "    dataset_id=f\"{dataset_id}_{data_version}\",\n",
    "    table_name=test_set_table.rsplit(\".\", 1)[1],\n",
    "    dataset_location=dataset_location,\n",
    "    file_pattern=\"file_*\",\n",
    "    extract_job_config=dict(destination_format=\"PARQUET\"),\n",
    "    skip_if_exists=True,\n",
    "    # The arguments below should not be supplied when running the component on Vertex AI\n",
    "    dataset=test_set,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_algorithm = \"random_forest\"\n",
    "config_params[\"models_params\"][model_algorithm][\"n_estimators\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_path = Path.cwd().parent / \"artifacts\"\n",
    "\n",
    "model = Model(name=\"credit-card-frauds\", uri=artifacts_path)\n",
    "train_metrics = Metrics(name=\"train_metrics\")\n",
    "validation_metrics = Metrics(name=\"validation_metrics\")\n",
    "test_metrics = Metrics(name=\"test_metrics\")\n",
    "validation_pr_curve = Artifact(name=\"validation_pr_curve\", uri=artifacts_path)\n",
    "test_pr_curve = Artifact(name=\"test_pr_curve\", uri=artifacts_path)\n",
    "\n",
    "train_evaluate_model(\n",
    "    training_data=training_set,\n",
    "    validation_data=validation_set,\n",
    "    test_data=test_set,\n",
    "    target_column=config_params[\"target_column\"],\n",
    "    model_name=model_algorithm,\n",
    "    models_params=config_params[\"models_params\"],\n",
    "    fit_args=config_params[\"fit_args\"],\n",
    "    data_processing_args=config_params[\"data_processing_args\"],\n",
    "    model_gcs_folder_path=None,\n",
    "    # The arguments below should not be supplied when running the component on Vertex AI\n",
    "    model=model,\n",
    "    train_metrics=train_metrics,\n",
    "    valid_metrics=validation_metrics,\n",
    "    test_metrics=test_metrics,\n",
    "    valid_pr_curve=validation_pr_curve,\n",
    "    test_pr_curve=test_pr_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=project_id)\n",
    "gcs_bucket = storage_client.bucket(bucket)\n",
    "\n",
    "model_gcs = gcs_bucket.blob(f\"{prefix}/{str(model.uri).split('/')[-1]}\")\n",
    "model_gcs.upload_from_filename(model.uri)\n",
    "\n",
    "validation_pr_curve_gcs = gcs_bucket.blob(f\"{prefix}/{str(validation_pr_curve.uri).split('/')[-1]}\")\n",
    "validation_pr_curve_gcs.upload_from_filename(validation_pr_curve.uri)\n",
    "\n",
    "test_pr_curve_gcs = gcs_bucket.blob(f\"{prefix}/{str(test_pr_curve.uri).split('/')[-1]}\")\n",
    "test_pr_curve_gcs.upload_from_filename(test_pr_curve.uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = aiplatform.Experiment.get_or_create(\n",
    "    experiment_name=\"credit-card-frauds\",\n",
    "    project=os.environ.get(\"VERTEX_PROJECT_ID\"),\n",
    "    location=os.environ.get(\"VERTEX_LOCATION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=os.environ.get(\"VERTEX_PROJECT_ID\"),\n",
    "    location=os.environ.get(\"VERTEX_LOCATION\"),\n",
    "    experiment=experiment.name,\n",
    "    experiment_tensorboard=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"notebook-execution-{model_algorithm}-\".replace(\"_\", \"-\") \n",
    "run_name += datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "with aiplatform.start_run(run=run_name) as run:\n",
    "    aiplatform.log_params(config_params[\"models_params\"][model_algorithm])\n",
    "    aiplatform.log_metrics({k + \"_train\": v for k, v in train_metrics.metadata.items()})\n",
    "    aiplatform.log_metrics(\n",
    "        {k + \"_validation\": v for k, v in validation_metrics.metadata.items()}\n",
    "    )\n",
    "    aiplatform.log_metrics({k + \"_test\": v for k, v in test_metrics.metadata.items()})\n",
    "    with aiplatform.start_execution(\n",
    "        schema_title=\"system.ContainerExecution\",\n",
    "        display_name=\"train-evaluate-model\",\n",
    "    ) as exc:\n",
    "        training_set_api = aiplatform.Artifact.create(\n",
    "            uri=training_set.uri,\n",
    "            schema_title=\"system.Dataset\",\n",
    "            display_name=\"training-set\",\n",
    "        )\n",
    "        validation_set_aip = aiplatform.Artifact.create(\n",
    "            uri=validation_set.uri,\n",
    "            schema_title=\"system.Dataset\",\n",
    "            display_name=\"validation-set\",\n",
    "        )\n",
    "        test_set_aip = aiplatform.Artifact.create(\n",
    "            uri=test_set.uri,\n",
    "            schema_title=\"system.Dataset\",\n",
    "            display_name=\"test-set\",\n",
    "        )\n",
    "        exc.assign_input_artifacts([training_set_api, validation_set_aip, test_set_aip])\n",
    "\n",
    "        model_aip = aiplatform.Artifact.create(\n",
    "            uri=f\"gs://{model_gcs.bucket.name}/{model_gcs.name}\",\n",
    "            schema_title=\"system.Model\",\n",
    "            display_name=f\"{model_algorithm}-model\".replace(\"_\", \"-\"),\n",
    "        )\n",
    "        validation_pr_curve_aip = aiplatform.Artifact.create(\n",
    "            uri=f\"gs://{validation_pr_curve_gcs.bucket.name}/{validation_pr_curve_gcs.name}\",\n",
    "            schema_title=\"system.Artifact\",\n",
    "            display_name=\"validation-pr-curve\",\n",
    "        )\n",
    "        test_pr_curve_aip = aiplatform.Artifact.create(\n",
    "            uri=f\"gs://{test_pr_curve_gcs.bucket.name}/{test_pr_curve_gcs.name}\",\n",
    "            schema_title=\"system.Artifact\",\n",
    "            display_name=\"test-pr-curve\",\n",
    "        )\n",
    "        exc.assign_output_artifacts(\n",
    "            [model_aip, validation_pr_curve_aip, test_pr_curve_aip]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-card-frauds-venv",
   "language": "python",
   "name": "credit-card-frauds-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
