{
  "pipelineSpec": {
    "components": {
      "comp-bq-table-to-dataset": {
        "executorLabel": "exec-bq-table-to-dataset",
        "inputDefinitions": {
          "parameters": {
            "bq_client_project_id": {
              "type": "STRING"
            },
            "dataset_id": {
              "type": "STRING"
            },
            "dataset_location": {
              "type": "STRING"
            },
            "extract_job_config": {
              "type": "STRING"
            },
            "file_pattern": {
              "type": "STRING"
            },
            "skip_if_exists": {
              "type": "STRING"
            },
            "source_project_id": {
              "type": "STRING"
            },
            "table_name": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "dataset_gcs_prefix": {
              "type": "STRING"
            },
            "dataset_gcs_uri": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-bq-table-to-dataset-2": {
        "executorLabel": "exec-bq-table-to-dataset-2",
        "inputDefinitions": {
          "parameters": {
            "bq_client_project_id": {
              "type": "STRING"
            },
            "dataset_id": {
              "type": "STRING"
            },
            "dataset_location": {
              "type": "STRING"
            },
            "extract_job_config": {
              "type": "STRING"
            },
            "file_pattern": {
              "type": "STRING"
            },
            "skip_if_exists": {
              "type": "STRING"
            },
            "source_project_id": {
              "type": "STRING"
            },
            "table_name": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "dataset_gcs_prefix": {
              "type": "STRING"
            },
            "dataset_gcs_uri": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-bq-table-to-dataset-3": {
        "executorLabel": "exec-bq-table-to-dataset-3",
        "inputDefinitions": {
          "parameters": {
            "bq_client_project_id": {
              "type": "STRING"
            },
            "dataset_id": {
              "type": "STRING"
            },
            "dataset_location": {
              "type": "STRING"
            },
            "extract_job_config": {
              "type": "STRING"
            },
            "file_pattern": {
              "type": "STRING"
            },
            "skip_if_exists": {
              "type": "STRING"
            },
            "source_project_id": {
              "type": "STRING"
            },
            "table_name": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "dataset_gcs_prefix": {
              "type": "STRING"
            },
            "dataset_gcs_uri": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-evaluate-model": {
        "executorLabel": "exec-evaluate-model",
        "inputDefinitions": {
          "artifacts": {
            "model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            },
            "test_data": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "target_column": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "predictions": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "test_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-execute-query": {
        "executorLabel": "exec-execute-query",
        "inputDefinitions": {
          "parameters": {
            "bq_client_project_id": {
              "type": "STRING"
            },
            "dataset_location": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            },
            "query_job_config": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-execute-query-2": {
        "executorLabel": "exec-execute-query-2",
        "inputDefinitions": {
          "parameters": {
            "bq_client_project_id": {
              "type": "STRING"
            },
            "dataset_location": {
              "type": "STRING"
            },
            "query": {
              "type": "STRING"
            },
            "query_job_config": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-exit-handler-1": {
        "dag": {
          "outputs": {
            "artifacts": {
              "evaluate-model-test_metrics": {
                "artifactSelectors": [
                  {
                    "outputArtifactKey": "evaluate-model-test_metrics",
                    "producerSubtask": "for-loop-2"
                  }
                ]
              },
              "train-evaluate-model-train_metrics": {
                "artifactSelectors": [
                  {
                    "outputArtifactKey": "train-evaluate-model-train_metrics",
                    "producerSubtask": "for-loop-2"
                  }
                ]
              },
              "train-evaluate-model-valid_metrics": {
                "artifactSelectors": [
                  {
                    "outputArtifactKey": "train-evaluate-model-valid_metrics",
                    "producerSubtask": "for-loop-2"
                  }
                ]
              }
            }
          },
          "tasks": {
            "bq-table-to-dataset": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-bq-table-to-dataset"
              },
              "dependentTasks": [
                "execute-query-2",
                "get-data-version"
              ],
              "inputs": {
                "parameters": {
                  "bq_client_project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "dataset_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}"
                      }
                    }
                  },
                  "dataset_location": {
                    "componentInputParameter": "pipelineparam--dataset_location"
                  },
                  "extract_job_config": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{\"destination_format\": \"PARQUET\"}"
                      }
                    }
                  },
                  "file_pattern": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "file_*"
                      }
                    }
                  },
                  "pipelineparam--dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "pipelineparam--get-data-version-Output": {
                    "taskOutputParameter": {
                      "outputParameterKey": "Output",
                      "producerTask": "get-data-version"
                    }
                  },
                  "skip_if_exists": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "True"
                      }
                    }
                  },
                  "source_project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "table_name": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "training"
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "Extract training data"
              }
            },
            "bq-table-to-dataset-2": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-bq-table-to-dataset-2"
              },
              "dependentTasks": [
                "execute-query-2",
                "get-data-version"
              ],
              "inputs": {
                "parameters": {
                  "bq_client_project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "dataset_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}"
                      }
                    }
                  },
                  "dataset_location": {
                    "componentInputParameter": "pipelineparam--dataset_location"
                  },
                  "extract_job_config": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{\"destination_format\": \"PARQUET\"}"
                      }
                    }
                  },
                  "file_pattern": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "file_*"
                      }
                    }
                  },
                  "pipelineparam--dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "pipelineparam--get-data-version-Output": {
                    "taskOutputParameter": {
                      "outputParameterKey": "Output",
                      "producerTask": "get-data-version"
                    }
                  },
                  "skip_if_exists": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "True"
                      }
                    }
                  },
                  "source_project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "table_name": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "validation"
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "Extract validation data"
              }
            },
            "bq-table-to-dataset-3": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-bq-table-to-dataset-3"
              },
              "dependentTasks": [
                "execute-query-2",
                "get-data-version"
              ],
              "inputs": {
                "parameters": {
                  "bq_client_project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "dataset_id": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}"
                      }
                    }
                  },
                  "dataset_location": {
                    "componentInputParameter": "pipelineparam--dataset_location"
                  },
                  "extract_job_config": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{\"destination_format\": \"PARQUET\"}"
                      }
                    }
                  },
                  "file_pattern": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "file_*"
                      }
                    }
                  },
                  "pipelineparam--dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "pipelineparam--get-data-version-Output": {
                    "taskOutputParameter": {
                      "outputParameterKey": "Output",
                      "producerTask": "get-data-version"
                    }
                  },
                  "skip_if_exists": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "True"
                      }
                    }
                  },
                  "source_project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "table_name": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "testing"
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "Extract test data"
              }
            },
            "execute-query": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-execute-query"
              },
              "dependentTasks": [
                "get-data-version"
              ],
              "inputs": {
                "parameters": {
                  "bq_client_project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "dataset_location": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "europe-west2"
                      }
                    }
                  },
                  "pipelineparam--dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "pipelineparam--get-data-version-Output": {
                    "taskOutputParameter": {
                      "outputParameterKey": "Output",
                      "producerTask": "get-data-version"
                    }
                  },
                  "pipelineparam--project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "CREATE TEMP TABLE merged\nCLUSTER BY mcc, datetime_unix_seconds\nAS (\n    WITH transactions AS (\n        /* Features related to transactions */\n        SELECT\n            ROW_NUMBER() OVER() AS transaction_id,\n            DATETIME(t.year, t.month, t.day, CAST(SUBSTR(t.time, 1, 2) AS INT), CAST(SUBSTR(t.time, 4, 2) AS INT), 0) AS datetime,\n            t.year,\n            SIN((CAST(SUBSTR(t.time, 1, 2) AS INT) / 24) * 2 * ACOS(-1)) AS hour_sin,\n            COS((CAST(SUBSTR(t.time, 1, 2) AS INT) / 24) * 2 * ACOS(-1)) AS hour_cos,\n            SIN(((t.month - 1) / 12) * 2 * ACOS(-1)) AS month_sin,\n            COS(((t.month - 1) / 12) * 2 * ACOS(-1)) AS month_cos,\n            IF(t.year >= 2015, 1, 0) AS is_2015_or_later,\n            t.amount,\n            IF(t.use_chip = \"Chip Transaction\", 1, 0) AS chip_transaction,\n            IF(t.use_chip = \"Swipe Transaction\", 1, 0) AS swipe_transaction,\n            IF(t.use_chip = \"Online Transaction\", 1, 0) AS online_transaction,\n            IF(t.use_chip = \"Chip Transaction\" OR t.use_chip = \"Swipe Transaction\", 1, 0) AS card_present_transaction,\n            t.card,\n            t.user,\n            t.mcc,\n            CAST(t.Is_Fraud_ AS INT) AS is_fraud\n\n        FROM `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}.transactions` t\n    )\n\n    , users AS (\n        /* Features related to users */\n        SELECT\n            u.user,\n            IF(u.gender = \"Male\", 0, 1) AS gender,\n            CAST(REPLACE(u.`Per Capita Income - Zipcode`, \"$\", \"\") AS NUMERIC) AS per_capita_income_zipcode,\n            CAST(REPLACE(u.`Yearly Income - Person`, \"$\", \"\") AS NUMERIC) AS yearly_income_person,\n            CAST(REPLACE(u.`Total Debt`, \"$\", \"\") AS NUMERIC) AS total_debt,\n\n        FROM `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}.users` u\n    )\n\n    , cards AS (\n        /* Features related to cards */\n        SELECT\n            user,\n            card_index,\n            IF(card_brand = \"Amex\", 1, 0) AS amex,\n            IF(card_brand = \"Discover\", 1, 0) AS discover,\n            IF(card_brand = \"Mastercard\", 1, 0) AS mastercard,\n            IF(card_brand = \"Visa\", 1, 0) AS visa,\n            IF(card_type = \"Credit\", 1, 0) AS credit,\n            IF(card_type = \"Debit\", 1, 0) AS debit,\n            IF(card_type = \"Debit (Prepaid)\", 1, 0) AS debit_prepaid,\n            credit_limit,\n            IF(has_chip IS TRUE, 1, 0) AS has_chip,\n            IF(card_on_dark_web IS TRUE, 1, 0) AS card_on_dark_web,\n\n        FROM `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}.cards`\n    )\n\n    /* Merging all features together */\n    SELECT\n        t.transaction_id,\n        t.datetime,\n        t.year,\n        UNIX_SECONDS(CAST(t.datetime AS TIMESTAMP)) AS datetime_unix_seconds,\n        t.hour_sin,\n        t.hour_cos,\n        t.month_sin,\n        t.month_cos,\n        EXTRACT(DAYOFWEEK FROM t.datetime) AS day_of_week,\n        IF(EXTRACT(DAYOFWEEK FROM t.datetime) IN (1, 7), 1, 0) AS weekend,\n        IF(EXTRACT(DATE FROM t.datetime) IN (SELECT date FROM `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}.holidays`), 1, 0) AS is_holiday,\n        SIN((IF(EXTRACT(DAYOFWEEK FROM t.datetime) = 1, 6, EXTRACT(DAYOFWEEK FROM t.datetime) - 2) / 7) * 2 * ACOS(-1)) AS day_of_week_sin,\n        COS((IF(EXTRACT(DAYOFWEEK FROM t.datetime) = 1, 6, EXTRACT(DAYOFWEEK FROM t.datetime) - 2) / 7) * 2 * ACOS(-1)) AS day_of_week_cos,\n        t.is_2015_or_later,\n        t.amount,\n        t.swipe_transaction,\n        t.chip_transaction,\n        t.online_transaction,\n        t.card_present_transaction,\n        IF(t.is_fraud = 1 AND t.chip_transaction = 1, 1, 0) AS fraud_chip,\n        IF(t.is_fraud = 1 AND t.swipe_transaction = 1, 1, 0) AS fraud_swipe,\n        IF(t.is_fraud = 1 AND t.online_transaction = 1, 1, 0) AS fraud_online,\n        IF(t.is_fraud = 1 AND t.card_present_transaction = 1, 1, 0) AS fraud_card_present,\n        t.user,\n        t.mcc,\n        u.gender,\n        c.amex,\n        c.discover,\n        c.mastercard,\n        c.visa,\n        c.credit,\n        c.debit,\n        c.debit_prepaid,\n        c.credit_limit,\n        c.has_chip,\n        c.card_on_dark_web,\n        u.per_capita_income_zipcode,\n        u.yearly_income_person,\n        u.total_debt,\n        t.is_fraud,\n\n    FROM transactions t\n\n    INNER JOIN cards c\n        ON t.card = c.card_index\n        AND t.user = c.user\n\n    INNER JOIN users u\n        ON t.user = u.user\n)\n;\n\nCREATE TEMP TABLE mcc_aux AS (\n    /* Rolling features related to MCC */\n    SELECT\n        m.transaction_id,\n        COALESCE(SUM(m.is_fraud) OVER(mcc_window) / COUNT(m.is_fraud) OVER(mcc_window), 0) AS mcc_mean_encoding\n\n    FROM merged m\n    WINDOW mcc_window AS (PARTITION BY m.mcc ORDER BY m.datetime_unix_seconds RANGE BETWEEN UNBOUNDED PRECEDING AND 604800 PRECEDING)\n)\n;\n\nCREATE TEMP TABLE user_aux\nCLUSTER BY user, datetime_unix_seconds\n/* Rolling features related to users */\nAS (\n    SELECT\n        m.*,\n        COALESCE(AVG(m.amount) OVER(user_window), 0) AS mean_amount,\n        COUNT(m.transaction_id) OVER(user_window) AS transaction_count,\n        DATE_DIFF(m.datetime, MIN(m.datetime) OVER(PARTITION BY m.user), DAY) AS days_since_first_transaction,\n\n    FROM merged m\n    WINDOW user_window AS (PARTITION BY m.user ORDER BY m.datetime ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING)\n)\n;\n\nCREATE TEMP TABLE rolling_aux_amount_frequency AS (\n    /* Rolling features related to transaction amount and frquency */\n    WITH tmp AS (\n        SELECT\n            r.*,\n            COALESCE(AVG(r.amount) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 86400 PRECEDING AND 1 PRECEDING), 0) AS mean_amount_last_1_days,\n            COALESCE(AVG(r.amount) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 172800 PRECEDING AND 1 PRECEDING), 0) AS mean_amount_last_2_days,\n            COALESCE(AVG(r.amount) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 604800 PRECEDING AND 1 PRECEDING), 0) AS mean_amount_last_7_days,\n            COALESCE(AVG(r.amount) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 1 PRECEDING), 0) AS mean_amount_last_30_days,\n            COALESCE(AVG(r.amount) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 1 PRECEDING), 0) AS mean_amount_last_year,\n            IF(\n                r.days_since_first_transaction > 0,\n                (r.transaction_count - 1) / r.days_since_first_transaction,\n                0\n            ) AS transaction_frequency_all,\n            IF(\n                r.days_since_first_transaction > 0,\n                COUNT(r.transaction_id) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 86400 PRECEDING AND 1 PRECEDING)\n                    / LEAST(1, r.days_since_first_transaction),\n                0\n            ) AS transaction_frequency_last_1_days,\n            IF(\n                r.days_since_first_transaction > 0,\n                COUNT(r.transaction_id) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 172800 PRECEDING AND 1 PRECEDING)\n                    / LEAST(2, r.days_since_first_transaction),\n                0\n            ) AS transaction_frequency_last_2_days,\n            IF(\n                r.days_since_first_transaction > 0,\n                COUNT(r.transaction_id) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 604800 PRECEDING AND 1 PRECEDING)\n                    / LEAST(7, r.days_since_first_transaction),\n                0\n            ) AS transaction_frequency_last_7_days,\n            IF(\n                r.days_since_first_transaction > 0,\n                COUNT(r.transaction_id) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 1 PRECEDING)\n                    / LEAST(30, r.days_since_first_transaction),\n                0\n            ) AS transaction_frequency_last_30_days,\n            IF(\n                r.days_since_first_transaction > 0,\n                COUNT(r.transaction_id) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 1 PRECEDING)\n                    / LEAST(365, r.days_since_first_transaction),\n                0\n            ) AS transaction_frequency_last_year\n\n        FROM user_aux r\n    )\n    SELECT\n        t.transaction_id,\n        t.mean_amount,\n        t.transaction_count,\n        days_since_first_transaction,\n        mean_amount_last_year,\n        mean_amount_last_30_days,\n        mean_amount_last_7_days,\n        mean_amount_last_2_days,\n        mean_amount_last_1_days,\n        transaction_frequency_all,\n        transaction_frequency_last_year,\n        transaction_frequency_last_30_days,\n        transaction_frequency_last_7_days,\n        transaction_frequency_last_2_days,\n        transaction_frequency_last_1_days,\n\n        COALESCE(SAFE_DIVIDE(mean_amount_last_7_days, mean_amount_last_year), 0) AS mean_amount_last_7_days_relative_to_last_year,\n        COALESCE(SAFE_DIVIDE(mean_amount_last_2_days, mean_amount_last_year), 0) AS mean_amount_last_2_days_relative_to_last_year,\n        COALESCE(SAFE_DIVIDE(mean_amount_last_1_days, mean_amount_last_year), 0) AS mean_amount_last_1_days_relative_to_last_year,\n\n        COALESCE(SAFE_DIVIDE(mean_amount_last_7_days, mean_amount_last_30_days), 0) AS mean_amount_last_7_days_relative_to_last_30_days,\n        COALESCE(SAFE_DIVIDE(mean_amount_last_2_days, mean_amount_last_30_days), 0) AS mean_amount_last_2_days_relative_to_last_30_days,\n        COALESCE(SAFE_DIVIDE(mean_amount_last_1_days, mean_amount_last_30_days), 0) AS mean_amount_last_1_days_relative_to_last_30_days,\n\n        COALESCE(SAFE_DIVIDE(transaction_frequency_last_7_days, transaction_frequency_last_year), 0) AS `7_days_transaction_frequency_relative_to_last_year`,\n        COALESCE(SAFE_DIVIDE(transaction_frequency_last_2_days, transaction_frequency_last_year), 0) AS `2_days_transaction_frequency_relative_to_last_year`,\n        COALESCE(SAFE_DIVIDE(transaction_frequency_last_1_days, transaction_frequency_last_year), 0) AS `1_days_transaction_frequency_relative_to_last_year`,\n\n        COALESCE(SAFE_DIVIDE(transaction_frequency_last_7_days, transaction_frequency_last_30_days), 0) AS `7_days_transaction_frequency_relative_to_last_30_days`,\n        COALESCE(SAFE_DIVIDE(transaction_frequency_last_2_days, transaction_frequency_last_30_days), 0) AS `2_days_transaction_frequency_relative_to_last_30_days`,\n        COALESCE(SAFE_DIVIDE(transaction_frequency_last_1_days, transaction_frequency_last_30_days), 0) AS `1_days_transaction_frequency_relative_to_last_30_days`,\n\n    FROM tmp t\n)\n;\n\nCREATE TEMP TABLE rolling_aux_frauds\n/* Rolling features related to amount of frauds */\nAS (\n    WITH tmp1 AS (\n        SELECT\n            m.transaction_id,\n            m.year,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_2_years,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_365_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_60_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_30_days,\n\n        FROM merged m\n\n        WHERE m.year <= 2000\n    )\n\n    , tmp2 AS (\n        SELECT\n            m.transaction_id,\n            m.year,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_2_years,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_365_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_60_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_30_days,\n\n        FROM merged m\n\n        WHERE m.year BETWEEN 1998 AND 2010\n    )\n\n    , tmp3 AS (\n        SELECT\n            m.transaction_id,\n            m.year,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_2_years,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_365_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_60_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_30_days,\n\n        FROM merged m\n\n        WHERE m.year BETWEEN 2008 AND 2015\n    )\n\n    , tmp4 AS (\n        SELECT\n            m.transaction_id,\n            m.year,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_2_years,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_365_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_60_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_30_days,\n\n        FROM merged m\n\n        WHERE m.year >= 2013\n    )\n\n    , all_years AS (\n        SELECT * EXCEPT(year) FROM tmp1\n\n        UNION ALL\n\n        SELECT * EXCEPT(year) FROM tmp2 WHERE year BETWEEN 2001 AND 2010\n\n        UNION ALL\n\n        SELECT * EXCEPT(year) FROM tmp3 WHERE year BETWEEN 2011 AND 2015\n\n        UNION ALL\n\n        SELECT * EXCEPT(year) FROM tmp4 WHERE year >= 2016\n    )\n\n    SELECT\n        a.*,\n        COALESCE(SAFE_DIVIDE(fraud_rolling_mean_30_days, fraud_rolling_mean_365_days), 0) AS fraud_rolling_30_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_rolling_mean_30_days, fraud_rolling_mean_2_years), 0) AS fraud_rolling_30_days_relative_to_2_years,\n        COALESCE(SAFE_DIVIDE(fraud_rolling_mean_60_days, fraud_rolling_mean_365_days), 0) AS fraud_rolling_60_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_rolling_mean_60_days, fraud_rolling_mean_2_years), 0) AS fraud_rolling_60_days_relative_to_2_years,\n\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_30_days, fraud_rolling_mean_365_days), 0) AS fraud_swipe_rolling_30_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_30_days, fraud_rolling_mean_2_years), 0) AS fraud_swipe_rolling_30_days_relative_to_2_years,\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_60_days, fraud_rolling_mean_365_days), 0) AS fraud_swipe_rolling_60_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_60_days, fraud_rolling_mean_2_years), 0) AS fraud_swipe_rolling_60_days_relative_to_2_years,\n\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_30_days, fraud_rolling_mean_365_days), 0) AS fraud_chip_rolling_30_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_30_days, fraud_rolling_mean_2_years), 0) AS fraud_chip_rolling_30_days_relative_to_2_years,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_60_days, fraud_rolling_mean_365_days), 0) AS fraud_chip_rolling_60_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_60_days, fraud_rolling_mean_2_years), 0) AS fraud_chip_rolling_60_days_relative_to_2_years,\n\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_30_days, fraud_rolling_mean_365_days), 0) AS fraud_online_rolling_30_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_30_days, fraud_rolling_mean_2_years), 0) AS fraud_online_rolling_30_days_relative_to_2_years,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_60_days, fraud_rolling_mean_365_days), 0) AS fraud_online_rolling_60_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_60_days, fraud_rolling_mean_2_years), 0) AS fraud_online_rolling_60_days_relative_to_2_years,\n\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_30_days, fraud_rolling_mean_365_days), 0) AS fraud_card_present_rolling_30_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_30_days, fraud_rolling_mean_2_years), 0) AS fraud_card_present_rolling_30_days_relative_to_2_years,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_60_days, fraud_rolling_mean_365_days), 0) AS fraud_card_present_rolling_60_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_60_days, fraud_rolling_mean_2_years), 0) AS fraud_card_present_rolling_60_days_relative_to_2_years,\n\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_30_days, fraud_rolling_mean_30_days), 0) AS fraud_swipe_rolling_30_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_30_days, fraud_rolling_mean_30_days), 0) AS fraud_chip_rolling_30_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_30_days, fraud_rolling_mean_30_days), 0) AS fraud_online_rolling_30_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_30_days, fraud_rolling_mean_30_days), 0) AS fraud_card_present_rolling_30_days_relative_to_all_frauds,\n\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_60_days, fraud_rolling_mean_60_days), 0) AS fraud_swipe_rolling_60_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_60_days, fraud_rolling_mean_60_days), 0) AS fraud_chip_rolling_60_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_60_days, fraud_rolling_mean_60_days), 0) AS fraud_online_rolling_60_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_60_days, fraud_rolling_mean_60_days), 0) AS fraud_card_present_rolling_60_days_relative_to_all_frauds,\n\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_365_days, fraud_rolling_mean_365_days), 0) AS fraud_swipe_rolling_365_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_365_days, fraud_rolling_mean_365_days), 0) AS fraud_chip_rolling_365_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_365_days, fraud_rolling_mean_365_days), 0) AS fraud_online_rolling_365_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_365_days, fraud_rolling_mean_365_days), 0) AS fraud_card_present_rolling_365_days_relative_to_all_frauds,\n\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_2_years, fraud_rolling_mean_2_years), 0) AS fraud_swipe_rolling_2_years_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_2_years, fraud_rolling_mean_2_years), 0) AS fraud_chip_rolling_2_years_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_2_years, fraud_rolling_mean_2_years), 0) AS fraud_online_rolling_2_years_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_2_years, fraud_rolling_mean_2_years), 0) AS fraud_card_present_rolling_2_years_relative_to_all_frauds,\n\n    FROM all_years a\n)\n;\n\nCREATE OR REPLACE TABLE `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}.preprocessed`\nCLUSTER BY datetime_unix_seconds\nAS (\n    SELECT\n        m.transaction_id,\n        m.datetime_unix_seconds,\n        `amount`,\n`has_chip`,\n`gender`,\n`online_transaction`,\n`amex`,\n`discover`,\n`mastercard`,\n`visa`,\n`credit`,\n`debit`,\n`debit_prepaid`,\n`mcc_mean_encoding`,\n`card_present_transaction`,\n`fraud_rolling_mean_30_days`,\n`fraud_rolling_mean_60_days`,\n`fraud_rolling_mean_365_days`,\n`fraud_rolling_mean_2_years`,\n`fraud_online_rolling_mean_30_days`,\n`fraud_online_rolling_mean_60_days`,\n`fraud_online_rolling_mean_365_days`,\n`fraud_online_rolling_mean_2_years`,\n`fraud_card_present_rolling_mean_30_days`,\n`fraud_card_present_rolling_mean_60_days`,\n`fraud_card_present_rolling_mean_365_days`,\n`fraud_card_present_rolling_mean_2_years`,\n`fraud_rolling_30_days_relative_to_365_days`,\n`fraud_rolling_30_days_relative_to_2_years`,\n`fraud_rolling_60_days_relative_to_365_days`,\n`fraud_rolling_60_days_relative_to_2_years`,\n`fraud_online_rolling_30_days_relative_to_365_days`,\n`fraud_online_rolling_30_days_relative_to_2_years`,\n`fraud_online_rolling_60_days_relative_to_365_days`,\n`fraud_online_rolling_60_days_relative_to_2_years`,\n`fraud_card_present_rolling_30_days_relative_to_365_days`,\n`fraud_card_present_rolling_30_days_relative_to_2_years`,\n`fraud_card_present_rolling_60_days_relative_to_365_days`,\n`fraud_card_present_rolling_60_days_relative_to_2_years`,\n`fraud_online_rolling_30_days_relative_to_all_frauds`,\n`fraud_card_present_rolling_30_days_relative_to_all_frauds`,\n`fraud_online_rolling_60_days_relative_to_all_frauds`,\n`fraud_card_present_rolling_60_days_relative_to_all_frauds`,\n`fraud_online_rolling_365_days_relative_to_all_frauds`,\n`fraud_card_present_rolling_365_days_relative_to_all_frauds`,\n`fraud_online_rolling_2_years_relative_to_all_frauds`,\n`fraud_card_present_rolling_2_years_relative_to_all_frauds`,\n`hour_sin`,\n`hour_cos`,\n`month_sin`,\n`month_cos`,\n`day_of_week_sin`,\n`day_of_week_cos`,\n`is_holiday`,\n`weekend`,\n`is_2015_or_later`,\n`mean_amount`,\n`mean_amount_last_year`,\n`mean_amount_last_30_days`,\n`mean_amount_last_7_days`,\n`mean_amount_last_2_days`,\n`mean_amount_last_1_days`,\n`mean_amount_last_7_days_relative_to_last_year`,\n`mean_amount_last_2_days_relative_to_last_year`,\n`mean_amount_last_1_days_relative_to_last_year`,\n`mean_amount_last_7_days_relative_to_last_30_days`,\n`mean_amount_last_2_days_relative_to_last_30_days`,\n`mean_amount_last_1_days_relative_to_last_30_days`,\n`transaction_count`,\n`days_since_first_transaction`,\n`transaction_frequency_all`,\n`transaction_frequency_last_year`,\n`transaction_frequency_last_30_days`,\n`transaction_frequency_last_7_days`,\n`transaction_frequency_last_2_days`,\n`transaction_frequency_last_1_days`,\n`1_days_transaction_frequency_relative_to_last_30_days`,\n`1_days_transaction_frequency_relative_to_last_year`,\n`2_days_transaction_frequency_relative_to_last_30_days`,\n`2_days_transaction_frequency_relative_to_last_year`,\n`7_days_transaction_frequency_relative_to_last_30_days`,\n`7_days_transaction_frequency_relative_to_last_year`,\n        is_fraud\n\n    FROM merged m\n\n    INNER JOIN mcc_aux mc\n        ON m.transaction_id = mc.transaction_id\n\n    INNER JOIN rolling_aux_amount_frequency raf\n        ON m.transaction_id = raf.transaction_id\n\n    INNER JOIN rolling_aux_frauds rf\n        ON m.transaction_id = rf.transaction_id\n)\n;"
                      }
                    }
                  },
                  "query_job_config": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{\"use_query_cache\": true}"
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "Preprocess input data"
              }
            },
            "execute-query-2": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-execute-query-2"
              },
              "dependentTasks": [
                "execute-query",
                "get-data-version"
              ],
              "inputs": {
                "parameters": {
                  "bq_client_project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "dataset_location": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "europe-west2"
                      }
                    }
                  },
                  "pipelineparam--dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "pipelineparam--get-data-version-Output": {
                    "taskOutputParameter": {
                      "outputParameterKey": "Output",
                      "producerTask": "get-data-version"
                    }
                  },
                  "pipelineparam--project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "query": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "DECLARE train_limit INT64 DEFAULT (\n    SELECT APPROX_QUANTILES(datetime_unix_seconds, 100)[OFFSET(CAST(100 * (1 - 0.15 - 0.15) AS INT))] train_limit\n    FROM `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}.preprocessed`\n)\n;\n\nCREATE OR REPLACE TABLE `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}.training` AS (\n    SELECT t.* EXCEPT(datetime_unix_seconds)\n\n    FROM `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}.preprocessed` t\n\n    WHERE t.datetime_unix_seconds < train_limit\n)\n;\n\nCREATE TEMP TABLE validation_testing AS (\n    SELECT t.* EXCEPT(datetime_unix_seconds)\n\n    FROM `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}.preprocessed` t\n\n    WHERE t.datetime_unix_seconds >= train_limit\n)\n;\n\nCREATE OR REPLACE TABLE `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}.validation` AS (\n    SELECT t.*\n\n    FROM validation_testing t\n\n    WHERE ABS(MOD(t.transaction_id, 100)) < CAST(0.15 / (0.15 + 0.15) * 100 AS INT)\n)\n;\n\nCREATE OR REPLACE TABLE `{{$.inputs.parameters['pipelineparam--project_id']}}.{{$.inputs.parameters['pipelineparam--dataset_id']}}_{{$.inputs.parameters['pipelineparam--get-data-version-Output']}}.testing` AS (\n    SELECT t.*\n\n    FROM validation_testing t\n\n    WHERE ABS(MOD(t.transaction_id, 100)) >= CAST(0.15 / (0.15 + 0.15) * 100 AS INT)\n)\n;"
                      }
                    }
                  },
                  "query_job_config": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{\"use_query_cache\": true}"
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "Train / validation / test split"
              }
            },
            "for-loop-2": {
              "componentRef": {
                "name": "comp-for-loop-2"
              },
              "dependentTasks": [
                "bq-table-to-dataset",
                "bq-table-to-dataset-2",
                "bq-table-to-dataset-3"
              ],
              "inputs": {
                "artifacts": {
                  "pipelineparam--bq-table-to-dataset-2-dataset": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "dataset",
                      "producerTask": "bq-table-to-dataset-2"
                    }
                  },
                  "pipelineparam--bq-table-to-dataset-3-dataset": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "dataset",
                      "producerTask": "bq-table-to-dataset-3"
                    }
                  },
                  "pipelineparam--bq-table-to-dataset-dataset": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "dataset",
                      "producerTask": "bq-table-to-dataset"
                    }
                  }
                },
                "parameters": {
                  "pipelineparam--models": {
                    "componentInputParameter": "pipelineparam--models"
                  },
                  "pipelineparam--project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "pipelineparam--project_location": {
                    "componentInputParameter": "pipelineparam--project_location"
                  }
                }
              },
              "iteratorPolicy": {
                "parallelismLimit": 1.0
              },
              "parameterIterator": {
                "itemInput": "pipelineparam--models-loop-item",
                "items": {
                  "inputParameter": "pipelineparam--models"
                }
              },
              "taskInfo": {
                "name": "for-loop-2"
              }
            },
            "get-data-version": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-get-data-version"
              },
              "inputs": {
                "parameters": {
                  "dataset_id": {
                    "componentInputParameter": "pipelineparam--dataset_id"
                  },
                  "dataset_location": {
                    "componentInputParameter": "pipelineparam--dataset_location"
                  },
                  "payload_data_version": {
                    "componentInputParameter": "pipelineparam--data_version"
                  },
                  "project_id": {
                    "componentInputParameter": "pipelineparam--project_id"
                  }
                }
              },
              "taskInfo": {
                "name": "Get data version"
              }
            }
          }
        },
        "inputDefinitions": {
          "parameters": {
            "pipelineparam--data_version": {
              "type": "STRING"
            },
            "pipelineparam--dataset_id": {
              "type": "STRING"
            },
            "pipelineparam--dataset_location": {
              "type": "STRING"
            },
            "pipelineparam--models": {
              "type": "STRING"
            },
            "pipelineparam--project_id": {
              "type": "STRING"
            },
            "pipelineparam--project_location": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "evaluate-model-test_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "train-evaluate-model-train_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "train-evaluate-model-valid_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-for-loop-2": {
        "dag": {
          "outputs": {
            "artifacts": {
              "evaluate-model-test_metrics": {
                "artifactSelectors": [
                  {
                    "outputArtifactKey": "test_metrics",
                    "producerSubtask": "evaluate-model"
                  }
                ]
              },
              "train-evaluate-model-train_metrics": {
                "artifactSelectors": [
                  {
                    "outputArtifactKey": "train_metrics",
                    "producerSubtask": "train-evaluate-model"
                  }
                ]
              },
              "train-evaluate-model-valid_metrics": {
                "artifactSelectors": [
                  {
                    "outputArtifactKey": "valid_metrics",
                    "producerSubtask": "train-evaluate-model"
                  }
                ]
              }
            }
          },
          "tasks": {
            "evaluate-model": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-evaluate-model"
              },
              "dependentTasks": [
                "train-evaluate-model"
              ],
              "inputs": {
                "artifacts": {
                  "model": {
                    "taskOutputArtifact": {
                      "outputArtifactKey": "model",
                      "producerTask": "train-evaluate-model"
                    }
                  },
                  "test_data": {
                    "componentInputArtifact": "pipelineparam--bq-table-to-dataset-3-dataset"
                  }
                },
                "parameters": {
                  "target_column": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "is_fraud"
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "Predict and evaluate models"
              }
            },
            "train-evaluate-model": {
              "cachingOptions": {
                "enableCache": true
              },
              "componentRef": {
                "name": "comp-train-evaluate-model"
              },
              "inputs": {
                "artifacts": {
                  "training_data": {
                    "componentInputArtifact": "pipelineparam--bq-table-to-dataset-dataset"
                  },
                  "validation_data": {
                    "componentInputArtifact": "pipelineparam--bq-table-to-dataset-2-dataset"
                  }
                },
                "parameters": {
                  "base_output_directory": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": ""
                      }
                    }
                  },
                  "data_processing_args": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{\"data_sampling\": \"rose\", \"rose_shrinkage\": 0.5, \"rose_upsampled_minority_proportion\": 0.3}"
                      }
                    }
                  },
                  "fit_args": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{\"xgboost\": {\"verbose\": 0}}"
                      }
                    }
                  },
                  "location": {
                    "componentInputParameter": "pipelineparam--project_location"
                  },
                  "model_name": {
                    "componentInputParameter": "pipelineparam--models-loop-item"
                  },
                  "models_params": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "{\"logistic_regression\": {\"max_iter\": 500}, \"sgd_classifier\": {\"loss\": \"modified_huber\"}, \"xgboost\": {\"eval_metric\": \"logloss\", \"scale_pos_weight\": 10, \"verbosity\": 0}, \"lightgbm\": {\"verbosity\": 0, \"is_unbalance\": true}}"
                      }
                    }
                  },
                  "network": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": ""
                      }
                    }
                  },
                  "project": {
                    "componentInputParameter": "pipelineparam--project_id"
                  },
                  "service_account": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": ""
                      }
                    }
                  },
                  "target_column": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": "is_fraud"
                      }
                    }
                  },
                  "tensorboard": {
                    "runtimeValue": {
                      "constantValue": {
                        "stringValue": ""
                      }
                    }
                  }
                }
              },
              "taskInfo": {
                "name": "Train and evaluate models"
              }
            }
          }
        },
        "inputDefinitions": {
          "artifacts": {
            "pipelineparam--bq-table-to-dataset-2-dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "pipelineparam--bq-table-to-dataset-3-dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "pipelineparam--bq-table-to-dataset-dataset": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "pipelineparam--models": {
              "type": "STRING"
            },
            "pipelineparam--models-loop-item": {
              "type": "STRING"
            },
            "pipelineparam--project_id": {
              "type": "STRING"
            },
            "pipelineparam--project_location": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "evaluate-model-test_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "train-evaluate-model-train_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "train-evaluate-model-valid_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-get-data-version": {
        "executorLabel": "exec-get-data-version",
        "inputDefinitions": {
          "parameters": {
            "dataset_id": {
              "type": "STRING"
            },
            "dataset_location": {
              "type": "STRING"
            },
            "payload_data_version": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "Output": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-train-evaluate-model": {
        "executorLabel": "exec-train-evaluate-model",
        "inputDefinitions": {
          "artifacts": {
            "training_data": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "validation_data": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "base_output_directory": {
              "type": "STRING"
            },
            "data_processing_args": {
              "type": "STRING"
            },
            "fit_args": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "model_name": {
              "type": "STRING"
            },
            "models_params": {
              "type": "STRING"
            },
            "network": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "service_account": {
              "type": "STRING"
            },
            "target_column": {
              "type": "STRING"
            },
            "tensorboard": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            },
            "train_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            },
            "valid_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "gcp_resources": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-vertex-pipelines-notification-email": {
        "executorLabel": "exec-vertex-pipelines-notification-email",
        "inputDefinitions": {
          "parameters": {
            "pipeline_task_final_status": {
              "type": "STRING"
            },
            "recipients": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-bq-table-to-dataset": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "bq_table_to_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.0' 'loguru==0.7.0' 'kfp==1.8.22' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef bq_table_to_dataset(\n    bq_client_project_id: str,\n    source_project_id: str,\n    dataset_id: str,\n    table_name: str,\n    dataset: Output[Dataset],\n    destination_gcs_uri: Optional[str] = None,\n    dataset_location: str = \"europe-west2\",\n    extract_job_config: Optional[dict] = None,\n    skip_if_exists: bool = True,\n    file_pattern: Optional[str] = None,\n) -> NamedTuple(\"Outputs\", [(\"dataset_gcs_prefix\", str), (\"dataset_gcs_uri\", list)]):\n    \"\"\"\n    Extract BQ table in GCS.\n\n    Args:\n        bq_client_project_id (str): Project ID that will be used by the BQ client.\n        source_project_id (str): Project id from where BQ table will be extracted.\n        dataset_id (str): Dataset ID from where the BQ table will be extracted.\n        table_name (str): Table name (without project ID and dataset ID) from\n            where the BQ table will be extracted.\n        dataset (Output[Dataset]): Output dataset artifact generated by the operation,\n            this parameter will be passed automatically by the orchestrator.\n        dataset_location (str): BQ dataset location. Defaults to \"europe-west2\".\n        extract_job_config (Optional[dict], optional): Dict containing optional\n            parameters required by the bq extract operation. Defaults to None.\n            See available parameters here\n            https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa\n        skip_if_exists (bool): If True, skip extracting the dataset if the\n            output resource already exists.\n        file_pattern (Optional[str], optional): File pattern to append to the\n            output files (e.g. `.csv`). Defaults to None.\n        destination_gcs_uri (Optional[str], optional): GCS URI to use for\n            saving query results. Defaults to None.\n\n    Returns:\n        NamedTuple (str, list): Output dataset directory and its GCS uri\n    \"\"\"\n    import os\n    from pathlib import Path\n\n    from google.cloud import bigquery\n    from google.cloud.exceptions import GoogleCloudError\n    from loguru import logger\n\n    # Set uri of output dataset if destination_gcs_uri is provided\n    if destination_gcs_uri:\n        dataset.uri = destination_gcs_uri\n\n    logger.info(f\"Checking if destination exists: {dataset.path}.\")\n    if Path(dataset.path).exists() and skip_if_exists:\n        logger.warning(\"Destination already exists, skipping table extraction.\")\n        return\n\n    full_table_id = f\"{source_project_id}.{dataset_id}.{table_name}\"\n    table = bigquery.table.Table(table_ref=full_table_id)\n\n    if extract_job_config is None:\n        extract_job_config = {}\n    job_config = bigquery.job.ExtractJobConfig(**extract_job_config)\n    client = bigquery.client.Client(\n        project=bq_client_project_id, location=dataset_location\n    )\n\n    # If file_pattern is provided, join dataset.uri with file_pattern\n    dataset_uri = dataset.uri\n    if file_pattern is not None:\n        dataset_uri = os.path.join(dataset_uri, file_pattern)\n    dataset_directory = os.path.dirname(dataset_uri)\n\n    logger.info(f\"Extract table {table} to {dataset_uri}.\")\n    extract_job = client.extract_table(\n        table,\n        dataset_uri,\n        job_config=job_config,\n    )\n\n    try:\n        result = extract_job.result()\n        logger.info(f\"Table extracted, result: {result}.\")\n    except GoogleCloudError as e:\n        logger.error(e)\n        logger.error(extract_job.error_result)\n        logger.error(extract_job.errors)\n        raise e\n\n    return (dataset_directory, [dataset_uri])\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-bq-table-to-dataset-2": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "bq_table_to_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.0' 'loguru==0.7.0' 'kfp==1.8.22' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef bq_table_to_dataset(\n    bq_client_project_id: str,\n    source_project_id: str,\n    dataset_id: str,\n    table_name: str,\n    dataset: Output[Dataset],\n    destination_gcs_uri: Optional[str] = None,\n    dataset_location: str = \"europe-west2\",\n    extract_job_config: Optional[dict] = None,\n    skip_if_exists: bool = True,\n    file_pattern: Optional[str] = None,\n) -> NamedTuple(\"Outputs\", [(\"dataset_gcs_prefix\", str), (\"dataset_gcs_uri\", list)]):\n    \"\"\"\n    Extract BQ table in GCS.\n\n    Args:\n        bq_client_project_id (str): Project ID that will be used by the BQ client.\n        source_project_id (str): Project id from where BQ table will be extracted.\n        dataset_id (str): Dataset ID from where the BQ table will be extracted.\n        table_name (str): Table name (without project ID and dataset ID) from\n            where the BQ table will be extracted.\n        dataset (Output[Dataset]): Output dataset artifact generated by the operation,\n            this parameter will be passed automatically by the orchestrator.\n        dataset_location (str): BQ dataset location. Defaults to \"europe-west2\".\n        extract_job_config (Optional[dict], optional): Dict containing optional\n            parameters required by the bq extract operation. Defaults to None.\n            See available parameters here\n            https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa\n        skip_if_exists (bool): If True, skip extracting the dataset if the\n            output resource already exists.\n        file_pattern (Optional[str], optional): File pattern to append to the\n            output files (e.g. `.csv`). Defaults to None.\n        destination_gcs_uri (Optional[str], optional): GCS URI to use for\n            saving query results. Defaults to None.\n\n    Returns:\n        NamedTuple (str, list): Output dataset directory and its GCS uri\n    \"\"\"\n    import os\n    from pathlib import Path\n\n    from google.cloud import bigquery\n    from google.cloud.exceptions import GoogleCloudError\n    from loguru import logger\n\n    # Set uri of output dataset if destination_gcs_uri is provided\n    if destination_gcs_uri:\n        dataset.uri = destination_gcs_uri\n\n    logger.info(f\"Checking if destination exists: {dataset.path}.\")\n    if Path(dataset.path).exists() and skip_if_exists:\n        logger.warning(\"Destination already exists, skipping table extraction.\")\n        return\n\n    full_table_id = f\"{source_project_id}.{dataset_id}.{table_name}\"\n    table = bigquery.table.Table(table_ref=full_table_id)\n\n    if extract_job_config is None:\n        extract_job_config = {}\n    job_config = bigquery.job.ExtractJobConfig(**extract_job_config)\n    client = bigquery.client.Client(\n        project=bq_client_project_id, location=dataset_location\n    )\n\n    # If file_pattern is provided, join dataset.uri with file_pattern\n    dataset_uri = dataset.uri\n    if file_pattern is not None:\n        dataset_uri = os.path.join(dataset_uri, file_pattern)\n    dataset_directory = os.path.dirname(dataset_uri)\n\n    logger.info(f\"Extract table {table} to {dataset_uri}.\")\n    extract_job = client.extract_table(\n        table,\n        dataset_uri,\n        job_config=job_config,\n    )\n\n    try:\n        result = extract_job.result()\n        logger.info(f\"Table extracted, result: {result}.\")\n    except GoogleCloudError as e:\n        logger.error(e)\n        logger.error(extract_job.error_result)\n        logger.error(extract_job.errors)\n        raise e\n\n    return (dataset_directory, [dataset_uri])\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-bq-table-to-dataset-3": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "bq_table_to_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.0' 'loguru==0.7.0' 'kfp==1.8.22' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef bq_table_to_dataset(\n    bq_client_project_id: str,\n    source_project_id: str,\n    dataset_id: str,\n    table_name: str,\n    dataset: Output[Dataset],\n    destination_gcs_uri: Optional[str] = None,\n    dataset_location: str = \"europe-west2\",\n    extract_job_config: Optional[dict] = None,\n    skip_if_exists: bool = True,\n    file_pattern: Optional[str] = None,\n) -> NamedTuple(\"Outputs\", [(\"dataset_gcs_prefix\", str), (\"dataset_gcs_uri\", list)]):\n    \"\"\"\n    Extract BQ table in GCS.\n\n    Args:\n        bq_client_project_id (str): Project ID that will be used by the BQ client.\n        source_project_id (str): Project id from where BQ table will be extracted.\n        dataset_id (str): Dataset ID from where the BQ table will be extracted.\n        table_name (str): Table name (without project ID and dataset ID) from\n            where the BQ table will be extracted.\n        dataset (Output[Dataset]): Output dataset artifact generated by the operation,\n            this parameter will be passed automatically by the orchestrator.\n        dataset_location (str): BQ dataset location. Defaults to \"europe-west2\".\n        extract_job_config (Optional[dict], optional): Dict containing optional\n            parameters required by the bq extract operation. Defaults to None.\n            See available parameters here\n            https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa\n        skip_if_exists (bool): If True, skip extracting the dataset if the\n            output resource already exists.\n        file_pattern (Optional[str], optional): File pattern to append to the\n            output files (e.g. `.csv`). Defaults to None.\n        destination_gcs_uri (Optional[str], optional): GCS URI to use for\n            saving query results. Defaults to None.\n\n    Returns:\n        NamedTuple (str, list): Output dataset directory and its GCS uri\n    \"\"\"\n    import os\n    from pathlib import Path\n\n    from google.cloud import bigquery\n    from google.cloud.exceptions import GoogleCloudError\n    from loguru import logger\n\n    # Set uri of output dataset if destination_gcs_uri is provided\n    if destination_gcs_uri:\n        dataset.uri = destination_gcs_uri\n\n    logger.info(f\"Checking if destination exists: {dataset.path}.\")\n    if Path(dataset.path).exists() and skip_if_exists:\n        logger.warning(\"Destination already exists, skipping table extraction.\")\n        return\n\n    full_table_id = f\"{source_project_id}.{dataset_id}.{table_name}\"\n    table = bigquery.table.Table(table_ref=full_table_id)\n\n    if extract_job_config is None:\n        extract_job_config = {}\n    job_config = bigquery.job.ExtractJobConfig(**extract_job_config)\n    client = bigquery.client.Client(\n        project=bq_client_project_id, location=dataset_location\n    )\n\n    # If file_pattern is provided, join dataset.uri with file_pattern\n    dataset_uri = dataset.uri\n    if file_pattern is not None:\n        dataset_uri = os.path.join(dataset_uri, file_pattern)\n    dataset_directory = os.path.dirname(dataset_uri)\n\n    logger.info(f\"Extract table {table} to {dataset_uri}.\")\n    extract_job = client.extract_table(\n        table,\n        dataset_uri,\n        job_config=job_config,\n    )\n\n    try:\n        result = extract_job.result()\n        logger.info(f\"Table extracted, result: {result}.\")\n    except GoogleCloudError as e:\n        logger.error(e)\n        logger.error(extract_job.error_result)\n        logger.error(extract_job.errors)\n        raise e\n\n    return (dataset_directory, [dataset_uri])\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-evaluate-model": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "evaluate_model"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.22' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef evaluate_model(\n    test_data: Input[Dataset],\n    target_column: str,\n    model: Input[Model],\n    predictions: Output[Dataset],\n    test_metrics: Output[Metrics],\n) -> None:\n    \"\"\"Evaluate a trained model on test data and report goodness metrics.\n\n    Args:\n        test_data (Input[Dataset]): Evaluation data as a KFP Dataset object.\n        target_column (str): Column containing the target column for classification.\n        model (Input[Model]): Input trained model as a KFP Model object.\n        predictions (Output[Dataset]): Model predictions including input columns\n            as a KFP Dataset object. This parameter will be passed automatically\n            by the orchestrator.\n        metrics (Output[Metrics]): Output metrics for the trained model. This\n            parameter will be passed automatically by the orchestrator and it\n            can be referred to by clicking on the component's execution in\n            the pipeline.\n        metrics_artifact (Output[Artifact]): Output metrics Artifact for the trained\n            model. This parameter will be passed automatically by the orchestrator.\n    \"\"\"\n    import joblib\n    import pandas as pd\n    from loguru import logger\n\n    from src.base.model import evaluate_model\n\n    classifier = joblib.load(model.path)\n\n    df_test = pd.read_parquet(test_data.path)\n    df_test = df_test.drop(columns=[\"transaction_id\"])\n    y_test = df_test.pop(target_column)\n    logger.info(f\"Loaded test data, shape {df_test.shape}.\")\n\n    testing_metrics, _, _ = evaluate_model(classifier, df_test, y_test)\n    logger.info(\"Evaluation completed.\")\n    for k, v in testing_metrics.items():\n        if k != \"Precision Recall Curve\":\n            test_metrics.log_metric(k, v)\n\n"
            ],
            "image": "europe-west2-docker.pkg.dev/robertofierimonte-ml-pipe/docker-repo/credit-card-frauds:latest"
          }
        },
        "exec-execute-query": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "execute_query"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.0' 'loguru==0.7.0' 'kfp==1.8.22' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef execute_query(\n    query: str,\n    bq_client_project_id: str,\n    dataset_location: str = \"europe-west2\",\n    query_job_config: dict = {},\n) -> None:\n    \"\"\"\n    Run a BQ query.\n\n    Args:\n        query (str): SQL query to execute.\n        bq_client_project_id (str): Project ID that will be used by the BQ client.\n        dataset_location (str): BQ dataset location.\n        query_job_config (dict): Dict containing optional parameters required\n            by the bq query operation. No need to specify destination param.\n            Defaults to {}.\n            See available parameters here\n            https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJobConfig.html\n    \"\"\"\n    from google.cloud import bigquery\n    from google.cloud.exceptions import GoogleCloudError\n    from loguru import logger\n\n    job_config = bigquery.QueryJobConfig(**query_job_config)\n\n    bq_client = bigquery.client.Client(\n        project=bq_client_project_id, location=dataset_location\n    )\n    query_job = bq_client.query(query, job_config=job_config)\n\n    try:\n        _ = query_job.result()\n        logger.info(\"BQ query executed.\")\n    except GoogleCloudError as e:\n        logger.error(e)\n        logger.error(query_job.error_result)\n        logger.error(query_job.errors)\n        raise e\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-execute-query-2": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "execute_query"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.0' 'loguru==0.7.0' 'kfp==1.8.22' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef execute_query(\n    query: str,\n    bq_client_project_id: str,\n    dataset_location: str = \"europe-west2\",\n    query_job_config: dict = {},\n) -> None:\n    \"\"\"\n    Run a BQ query.\n\n    Args:\n        query (str): SQL query to execute.\n        bq_client_project_id (str): Project ID that will be used by the BQ client.\n        dataset_location (str): BQ dataset location.\n        query_job_config (dict): Dict containing optional parameters required\n            by the bq query operation. No need to specify destination param.\n            Defaults to {}.\n            See available parameters here\n            https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJobConfig.html\n    \"\"\"\n    from google.cloud import bigquery\n    from google.cloud.exceptions import GoogleCloudError\n    from loguru import logger\n\n    job_config = bigquery.QueryJobConfig(**query_job_config)\n\n    bq_client = bigquery.client.Client(\n        project=bq_client_project_id, location=dataset_location\n    )\n    query_job = bq_client.query(query, job_config=job_config)\n\n    try:\n        _ = query_job.result()\n        logger.info(\"BQ query executed.\")\n    except GoogleCloudError as e:\n        logger.error(e)\n        logger.error(query_job.error_result)\n        logger.error(query_job.errors)\n        raise e\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-get-data-version": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "get_data_version"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.0' 'loguru==0.7.0' 'kfp==1.8.22' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef get_data_version(\n    payload_data_version: str,\n    project_id: str,\n    dataset_id: str,\n    dataset_location: str = \"europe-west2\",\n) -> str:\n    \"\"\"Get data version to use in the pipeline.\n\n    Args:\n        payload_data_version (str): Data version provided in the payload file.\n        project_id (str): Bigquery project ID.\n        dataset_id (str): Bigquery dataset ID. This function will look for the\n            most recent BQ dataset that has the pattern of\n            {dataset_id}_%Y%m%dT%H%M%S.\n        dataset_location (str, optional): Bigquery dataset location.\n            Defaults to \"europe-west2\".\n    \"\"\"\n    import re\n\n    from google.cloud import bigquery\n    from loguru import logger\n\n    if payload_data_version == \"\":\n        bq_client = bigquery.client.Client(\n            project=project_id, location=dataset_location\n        )\n        datasets = [d.dataset_id for d in list(bq_client.list_datasets())]\n        matches = [\n            re.search(rf\"(?<={dataset_id}_)(\\d{{8}}T\\d{{6}})\", d) for d in datasets\n        ]\n        versions = sorted([m.group(0) for m in matches if m is not None])\n        logger.debug(f\"Found {len(versions)} versions of the data.\")\n\n        try:\n            res = versions[-1]\n            logger.info(f\"Most recent data version retrieved: {res}.\")\n            return res\n        except IndexError as e:\n            logger.error(\n                f\"No datasets matching the expected pattern in project {project_id}.\"\n            )\n            raise e\n    else:\n        logger.info(f\"Data version {payload_data_version} provided in payload.\")\n        return payload_data_version\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-train-evaluate-model": {
          "container": {
            "args": [
              "--type",
              "CustomJob",
              "--payload",
              "{\"display_name\": \"Train evaluate model\", \"job_spec\": {\"worker_pool_specs\": [{\"machine_spec\": {\"machine_type\": \"n1-standard-32\"}, \"replica_count\": 1, \"container_spec\": {\"image_uri\": \"europe-west2-docker.pkg.dev/robertofierimonte-ml-pipe/docker-repo/credit-card-frauds:latest\", \"command\": [\"sh\", \"-c\", \"\\nif ! [ -x \\\"$(command -v pip)\\\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.22' && \\\"$0\\\" \\\"$@\\\"\\n\", \"sh\", \"-ec\", \"program_path=$(mktemp -d)\\nprintf \\\"%s\\\" \\\"$0\\\" > \\\"$program_path/ephemeral_component.py\\\"\\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \\\"$program_path/ephemeral_component.py\\\"                         \\\"$@\\\"\\n\", \"\\nimport kfp\\nfrom kfp.v2 import dsl\\nfrom kfp.v2.dsl import *\\nfrom typing import *\\n\\ndef train_evaluate_model(\\n    training_data: Input[Dataset],\\n    validation_data: Input[Dataset],\\n    target_column: str,\\n    model_name: str,\\n    train_metrics: Output[Metrics],\\n    valid_metrics: Output[Metrics],\\n    model: Output[Model],\\n    models_params: dict = {},\\n    fit_args: dict = {},\\n    data_processing_args: dict = {},\\n) -> None:\\n    \\\"\\\"\\\"Train a classification model on the training data.\\n\\n    Args:\\n        training_data (Input[Dataset]): Training data as a KFP Dataset object.\\n        validation_data (Input[Dataset]): Validation data (used to prevent overfitting)\\n            as a KFP Dataset object.\\n        target_column (str): Column containing the target column for classification.\\n        model_name (str): Name of the classifier that will be trained. Must be one of\\n            'logistic_regression', 'sgd_classifier', 'random_forest', 'lightgbm',\\n            'xgboost'.\\n        models_params (dict): Hyperparameters of the model. Default to an empty dict.\\n        fit_args (dict): Arguments used when fitting the model.\\n            Default to an empty dict.\\n        data_processing_args (dict): Arguments used when running extra processing on\\n            the data (such as scaling or oversampling). Default to an empty dict.\\n        train_metrics (Output[Metrics]):\\n        valid_metrics (Output[Metrics]):\\n        model (Output[Model]): Output model as a KFP Model object, this parameter\\n            will be passed automatically by the orchestrator. The .path\\n            attribute is the location of the joblib file in GCS.\\n    \\\"\\\"\\\"\\n    import os\\n    from pathlib import Path\\n\\n    import joblib\\n    import pandas as pd\\n    from lightgbm import LGBMClassifier\\n    from loguru import logger\\n    from sklearn.ensemble import RandomForestClassifier\\n    from sklearn.linear_model import LogisticRegression, SGDClassifier\\n    from xgboost import XGBClassifier\\n\\n    from src.base.model import evaluate_model, train_model\\n\\n    df_train = pd.read_parquet(training_data.path)\\n    df_train = df_train.drop(columns=[\\\"transaction_id\\\"])\\n    y_train = df_train.pop(target_column)\\n    logger.info(f\\\"Loaded training data, shape {df_train.shape}.\\\")\\n\\n    df_valid = pd.read_parquet(validation_data.path)\\n    df_valid = df_valid.drop(columns=[\\\"transaction_id\\\"])\\n    y_valid = df_valid.pop(target_column)\\n    logger.info(f\\\"Loaded evaluation data, shape {df_valid.shape}.\\\")\\n\\n    use_eval_set = False\\n    model_params = models_params.get(model_name, {})\\n    if model_name == \\\"logistic_regression\\\":\\n        classifier = LogisticRegression(random_state=42, **model_params)\\n    elif model_name == \\\"sgd_classifier\\\":\\n        classifier = SGDClassifier(random_state=42, **model_params)\\n    elif model_name == \\\"random_forest\\\":\\n        classifier = RandomForestClassifier(random_state=42, **model_params)\\n    elif model_name == \\\"lightgbm\\\":\\n        classifier = LGBMClassifier(random_state=42, **model_params)\\n        use_eval_set = True\\n    elif model_name == \\\"xgboost\\\":\\n        classifier = XGBClassifier(\\n            use_label_encoder=False, random_state=42, **model_params\\n        )\\n        use_eval_set = True\\n    else:\\n        msg = (\\n            \\\"`model_name` must be one of 'logistic_regression', 'sgd_classifier', \\\"\\n            \\\"'random_forest', 'lightgbm', 'xgboost'.\\\"\\n        )\\n        logger.error(msg)\\n        raise ValueError(msg)\\n\\n    logger.info(f\\\"Training model {model_name}.\\\")\\n    classifier, training_metrics = train_model(\\n        classifier,\\n        X_train=df_train,\\n        y_train=y_train,\\n        X_valid=df_valid,\\n        y_valid=y_valid,\\n        use_eval_set=use_eval_set,\\n        fit_args=fit_args.get(model_name, {}),\\n        **data_processing_args,\\n    )\\n    logger.info(\\\"Training completed.\\\")\\n    logger.debug(f\\\"Type of classifier: {type(classifier)}.\\\")\\n    logger.debug(f\\\"Classifier: {classifier}.\\\")\\n    for k, v in training_metrics.items():\\n        if k != \\\"Precision Recall Curve\\\":\\n            train_metrics.log_metric(k, v)\\n\\n    validation_metrics, _, _ = evaluate_model(classifier, df_valid, y_valid)\\n    logger.info(\\\"Evaluation completed.\\\")\\n    for k, v in validation_metrics.items():\\n        if k != \\\"Precision Recall Curve\\\":\\n            valid_metrics.log_metric(k, v)\\n\\n    logger.debug(f\\\"Type of classifier: {type(classifier)}.\\\")\\n    logger.debug(f\\\"Classifier: {classifier}.\\\")\\n    model.path = model.path + f\\\"/{model_name}\\\"\\n    model_dir = Path(model.path).parent.absolute()\\n    os.makedirs(model_dir, exist_ok=True)\\n\\n    logger.info(f\\\"Saving model to {model.path}.\\\")\\n    joblib.dump(classifier, model.path)\\n\\n\"], \"args\": [\"--executor_input\", \"{{$.json_escape[1]}}\", \"--function_to_execute\", \"train_evaluate_model\"]}, \"disk_spec\": {\"boot_disk_type\": \"pd-ssd\", \"boot_disk_size_gb\": 100}}], \"service_account\": \"{{$.inputs.parameters['service_account']}}\", \"network\": \"{{$.inputs.parameters['network']}}\", \"tensorboard\": \"{{$.inputs.parameters['tensorboard']}}\", \"base_output_directory\": {\"output_uri_prefix\": \"{{$.inputs.parameters['base_output_directory']}}\"}}}",
              "--project",
              "{{$.inputs.parameters['project']}}",
              "--location",
              "{{$.inputs.parameters['location']}}",
              "--gcp_resources",
              "{{$.outputs.parameters['gcp_resources'].output_file}}"
            ],
            "command": [
              "python3",
              "-u",
              "-m",
              "google_cloud_pipeline_components.container.v1.custom_job.launcher"
            ],
            "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.44"
          }
        },
        "exec-vertex-pipelines-notification-email": {
          "container": {
            "args": [
              "--type",
              "VertexNotificationEmail",
              "--payload",
              ""
            ],
            "command": [
              "python3",
              "-u",
              "-m",
              "google_cloud_pipeline_components.container.experimental.vertex_notification_email.executor"
            ],
            "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:1.0.44"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "frauds-training-pipeline-dev"
    },
    "root": {
      "dag": {
        "outputs": {
          "artifacts": {
            "evaluate-model-test_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "evaluate-model-test_metrics",
                  "producerSubtask": "exit-handler-1"
                }
              ]
            },
            "train-evaluate-model-train_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "train-evaluate-model-train_metrics",
                  "producerSubtask": "exit-handler-1"
                }
              ]
            },
            "train-evaluate-model-valid_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "train-evaluate-model-valid_metrics",
                  "producerSubtask": "exit-handler-1"
                }
              ]
            }
          }
        },
        "tasks": {
          "exit-handler-1": {
            "componentRef": {
              "name": "comp-exit-handler-1"
            },
            "inputs": {
              "parameters": {
                "pipelineparam--data_version": {
                  "componentInputParameter": "data_version"
                },
                "pipelineparam--dataset_id": {
                  "componentInputParameter": "dataset_id"
                },
                "pipelineparam--dataset_location": {
                  "componentInputParameter": "dataset_location"
                },
                "pipelineparam--models": {
                  "componentInputParameter": "models"
                },
                "pipelineparam--project_id": {
                  "componentInputParameter": "project_id"
                },
                "pipelineparam--project_location": {
                  "componentInputParameter": "project_location"
                }
              }
            },
            "taskInfo": {
              "name": "Notify pipeline result"
            }
          },
          "vertex-pipelines-notification-email": {
            "componentRef": {
              "name": "comp-vertex-pipelines-notification-email"
            },
            "dependentTasks": [
              "exit-handler-1"
            ],
            "inputs": {
              "parameters": {
                "pipeline_task_final_status": {
                  "taskFinalStatus": {
                    "producerTask": "exit-handler-1"
                  }
                },
                "recipients": {
                  "componentInputParameter": "email_notification_recipients"
                }
              }
            },
            "taskInfo": {
              "name": "vertex-pipelines-notification-email"
            },
            "triggerPolicy": {
              "strategy": "ALL_UPSTREAM_TASKS_COMPLETED"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "data_version": {
            "type": "STRING"
          },
          "dataset_id": {
            "type": "STRING"
          },
          "dataset_location": {
            "type": "STRING"
          },
          "email_notification_recipients": {
            "type": "STRING"
          },
          "models": {
            "type": "STRING"
          },
          "pipeline_files_gcs_path": {
            "type": "STRING"
          },
          "project_id": {
            "type": "STRING"
          },
          "project_location": {
            "type": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "evaluate-model-test_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "train-evaluate-model-train_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "train-evaluate-model-valid_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.22"
  },
  "runtimeConfig": {
    "parameters": {
      "email_notification_recipients": {
        "stringValue": "[\"roberto.fierimonte.spam@gmail.com\"]"
      },
      "models": {
        "stringValue": "[\"logistic_regression\", \"sgd_classifier\"]"
      }
    }
  }
}
