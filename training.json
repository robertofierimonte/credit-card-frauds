{
  "components": {
    "comp-bq-table-to-dataset": {
      "executorLabel": "exec-bq-table-to-dataset",
      "inputDefinitions": {
        "parameters": {
          "bq_client_project_id": {
            "description": "Project ID that will be used by the BQ client.",
            "parameterType": "STRING"
          },
          "dataset_id": {
            "description": "Dataset ID from where the BQ table will be extracted.",
            "parameterType": "STRING"
          },
          "dataset_location": {
            "defaultValue": "europe-west2",
            "description": "BQ dataset location. Defaults to \"europe-west2\".",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "destination_gcs_uri": {
            "description": "GCS URI to use for\nsaving query results. Defaults to None.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "extract_job_config": {
            "description": "Dict containing optional\nparameters required by the bq extract operation. Defaults to None.\nSee available parameters here\nhttps://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa",
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "file_pattern": {
            "description": "File pattern to append to the\noutput files (e.g. `.csv`). Defaults to None.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "skip_if_exists": {
            "defaultValue": true,
            "description": "If True, skip extracting the dataset if the\noutput resource already exists.",
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "source_project_id": {
            "description": "Project id from where BQ table will be extracted.",
            "parameterType": "STRING"
          },
          "table_name": {
            "description": "Table name (without project ID and dataset ID) from\nwhere the BQ table will be extracted.",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "dataset_gcs_prefix": {
            "parameterType": "STRING"
          },
          "dataset_gcs_uri": {
            "parameterType": "LIST"
          }
        }
      }
    },
    "comp-bq-table-to-dataset-2": {
      "executorLabel": "exec-bq-table-to-dataset-2",
      "inputDefinitions": {
        "parameters": {
          "bq_client_project_id": {
            "description": "Project ID that will be used by the BQ client.",
            "parameterType": "STRING"
          },
          "dataset_id": {
            "description": "Dataset ID from where the BQ table will be extracted.",
            "parameterType": "STRING"
          },
          "dataset_location": {
            "defaultValue": "europe-west2",
            "description": "BQ dataset location. Defaults to \"europe-west2\".",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "destination_gcs_uri": {
            "description": "GCS URI to use for\nsaving query results. Defaults to None.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "extract_job_config": {
            "description": "Dict containing optional\nparameters required by the bq extract operation. Defaults to None.\nSee available parameters here\nhttps://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa",
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "file_pattern": {
            "description": "File pattern to append to the\noutput files (e.g. `.csv`). Defaults to None.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "skip_if_exists": {
            "defaultValue": true,
            "description": "If True, skip extracting the dataset if the\noutput resource already exists.",
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "source_project_id": {
            "description": "Project id from where BQ table will be extracted.",
            "parameterType": "STRING"
          },
          "table_name": {
            "description": "Table name (without project ID and dataset ID) from\nwhere the BQ table will be extracted.",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "dataset_gcs_prefix": {
            "parameterType": "STRING"
          },
          "dataset_gcs_uri": {
            "parameterType": "LIST"
          }
        }
      }
    },
    "comp-bq-table-to-dataset-3": {
      "executorLabel": "exec-bq-table-to-dataset-3",
      "inputDefinitions": {
        "parameters": {
          "bq_client_project_id": {
            "description": "Project ID that will be used by the BQ client.",
            "parameterType": "STRING"
          },
          "dataset_id": {
            "description": "Dataset ID from where the BQ table will be extracted.",
            "parameterType": "STRING"
          },
          "dataset_location": {
            "defaultValue": "europe-west2",
            "description": "BQ dataset location. Defaults to \"europe-west2\".",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "destination_gcs_uri": {
            "description": "GCS URI to use for\nsaving query results. Defaults to None.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "extract_job_config": {
            "description": "Dict containing optional\nparameters required by the bq extract operation. Defaults to None.\nSee available parameters here\nhttps://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa",
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "file_pattern": {
            "description": "File pattern to append to the\noutput files (e.g. `.csv`). Defaults to None.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "skip_if_exists": {
            "defaultValue": true,
            "description": "If True, skip extracting the dataset if the\noutput resource already exists.",
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "source_project_id": {
            "description": "Project id from where BQ table will be extracted.",
            "parameterType": "STRING"
          },
          "table_name": {
            "description": "Table name (without project ID and dataset ID) from\nwhere the BQ table will be extracted.",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "dataset_gcs_prefix": {
            "parameterType": "STRING"
          },
          "dataset_gcs_uri": {
            "parameterType": "LIST"
          }
        }
      }
    },
    "comp-compare-champion-challenger": {
      "executorLabel": "exec-compare-champion-challenger",
      "inputDefinitions": {
        "artifacts": {
          "challenger_model": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            },
            "description": "_description_"
          },
          "champion_model": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            },
            "description": "_description_"
          },
          "test_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "_description_"
          }
        },
        "parameters": {
          "absolute_threshold": {
            "defaultValue": 0.0,
            "isOptional": true,
            "parameterType": "NUMBER_DOUBLE"
          },
          "higher_is_better": {
            "defaultValue": true,
            "description": "_description_. Defaults to True.",
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "metric_to_optimise": {
            "description": "_description_",
            "parameterType": "STRING"
          },
          "target_column": {
            "description": "_description_",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "challeger_metric": {
            "parameterType": "NUMBER_DOUBLE"
          },
          "challenger_better": {
            "parameterType": "BOOLEAN"
          },
          "champion_metric": {
            "parameterType": "NUMBER_DOUBLE"
          }
        }
      }
    },
    "comp-compare-models": {
      "executorLabel": "exec-compare-models",
      "inputDefinitions": {
        "artifacts": {
          "models": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            },
            "description": "_description_",
            "isArtifactList": true
          },
          "test_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "_description_"
          }
        },
        "parameters": {
          "higher_is_better": {
            "defaultValue": true,
            "description": "_description_. Defaults to True.",
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "metric_to_optimise": {
            "description": "_description_",
            "parameterType": "STRING"
          },
          "target_column": {
            "description": "_description_",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "best_model": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "Output": {
            "parameterType": "LIST"
          }
        }
      }
    },
    "comp-condition-4": {
      "dag": {
        "tasks": {
          "compare-champion-challenger": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-compare-champion-challenger"
            },
            "inputs": {
              "artifacts": {
                "challenger_model": {
                  "componentInputArtifact": "pipelinechannel--compare-models-best_model"
                },
                "champion_model": {
                  "componentInputArtifact": "pipelinechannel--lookup-model-model"
                },
                "test_data": {
                  "componentInputArtifact": "pipelinechannel--bq-table-to-dataset-3-dataset"
                }
              },
              "parameters": {
                "absolute_threshold": {
                  "runtimeValue": {
                    "constant": 0.1
                  }
                },
                "higher_is_better": {
                  "runtimeValue": {
                    "constant": true
                  }
                },
                "metric_to_optimise": {
                  "runtimeValue": {
                    "constant": "Average Precision"
                  }
                },
                "target_column": {
                  "runtimeValue": {
                    "constant": "is_fraud"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Compare challenger to champion"
            }
          }
        }
      },
      "inputDefinitions": {
        "artifacts": {
          "pipelinechannel--bq-table-to-dataset-3-dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "pipelinechannel--compare-models-best_model": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            }
          },
          "pipelinechannel--lookup-model-model": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "pipelinechannel--lookup-model-Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-evaluate-model": {
      "executorLabel": "exec-evaluate-model",
      "inputDefinitions": {
        "artifacts": {
          "model": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            },
            "description": "Input trained model as a KFP Model object."
          },
          "test_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            },
            "description": "Evaluation data as a KFP Dataset object."
          }
        },
        "parameters": {
          "target_column": {
            "description": "Column containing the target column for classification.",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "predictions": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "test_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-execute-query": {
      "executorLabel": "exec-execute-query",
      "inputDefinitions": {
        "parameters": {
          "bq_client_project_id": {
            "description": "Project ID that will be used by the BQ client.",
            "parameterType": "STRING"
          },
          "dataset_location": {
            "defaultValue": "europe-west2",
            "description": "BQ dataset location.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "query": {
            "description": "SQL query to execute.",
            "parameterType": "STRING"
          },
          "query_job_config": {
            "defaultValue": {},
            "description": "Dict containing optional parameters required\nby the bq query operation. No need to specify destination param.\nDefaults to {}.\nSee available parameters here\nhttps://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJobConfig.html",
            "isOptional": true,
            "parameterType": "STRUCT"
          }
        }
      }
    },
    "comp-execute-query-2": {
      "executorLabel": "exec-execute-query-2",
      "inputDefinitions": {
        "parameters": {
          "bq_client_project_id": {
            "description": "Project ID that will be used by the BQ client.",
            "parameterType": "STRING"
          },
          "dataset_location": {
            "defaultValue": "europe-west2",
            "description": "BQ dataset location.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "query": {
            "description": "SQL query to execute.",
            "parameterType": "STRING"
          },
          "query_job_config": {
            "defaultValue": {},
            "description": "Dict containing optional parameters required\nby the bq query operation. No need to specify destination param.\nDefaults to {}.\nSee available parameters here\nhttps://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJobConfig.html",
            "isOptional": true,
            "parameterType": "STRUCT"
          }
        }
      }
    },
    "comp-exit-handler-1": {
      "dag": {
        "outputs": {
          "artifacts": {
            "evaluate-model-test_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "evaluate-model-test_metrics",
                  "producerSubtask": "for-loop-3"
                }
              ]
            },
            "train-evaluate-model-train_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "train-evaluate-model-train_metrics",
                  "producerSubtask": "for-loop-3"
                }
              ]
            },
            "train-evaluate-model-valid_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "train-evaluate-model-valid_metrics",
                  "producerSubtask": "for-loop-3"
                }
              ]
            }
          }
        },
        "tasks": {
          "bq-table-to-dataset": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-bq-table-to-dataset"
            },
            "dependentTasks": [
              "execute-query-2",
              "get-data-version"
            ],
            "inputs": {
              "parameters": {
                "bq_client_project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "dataset_id": {
                  "runtimeValue": {
                    "constant": "{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}"
                  }
                },
                "dataset_location": {
                  "componentInputParameter": "pipelinechannel--dataset_location"
                },
                "extract_job_config": {
                  "runtimeValue": {
                    "constant": {
                      "destination_format": "PARQUET"
                    }
                  }
                },
                "file_pattern": {
                  "runtimeValue": {
                    "constant": "file_*"
                  }
                },
                "pipelinechannel--dataset_id": {
                  "componentInputParameter": "pipelinechannel--dataset_id"
                },
                "pipelinechannel--get-data-version-Output": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "get-data-version"
                  }
                },
                "source_project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "table_name": {
                  "runtimeValue": {
                    "constant": "training"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Extract training data"
            }
          },
          "bq-table-to-dataset-2": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-bq-table-to-dataset-2"
            },
            "dependentTasks": [
              "execute-query-2",
              "get-data-version"
            ],
            "inputs": {
              "parameters": {
                "bq_client_project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "dataset_id": {
                  "runtimeValue": {
                    "constant": "{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}"
                  }
                },
                "dataset_location": {
                  "componentInputParameter": "pipelinechannel--dataset_location"
                },
                "extract_job_config": {
                  "runtimeValue": {
                    "constant": {
                      "destination_format": "PARQUET"
                    }
                  }
                },
                "file_pattern": {
                  "runtimeValue": {
                    "constant": "file_*"
                  }
                },
                "pipelinechannel--dataset_id": {
                  "componentInputParameter": "pipelinechannel--dataset_id"
                },
                "pipelinechannel--get-data-version-Output": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "get-data-version"
                  }
                },
                "source_project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "table_name": {
                  "runtimeValue": {
                    "constant": "validation"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Extract validation data"
            }
          },
          "bq-table-to-dataset-3": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-bq-table-to-dataset-3"
            },
            "dependentTasks": [
              "execute-query-2",
              "get-data-version"
            ],
            "inputs": {
              "parameters": {
                "bq_client_project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "dataset_id": {
                  "runtimeValue": {
                    "constant": "{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}"
                  }
                },
                "dataset_location": {
                  "componentInputParameter": "pipelinechannel--dataset_location"
                },
                "extract_job_config": {
                  "runtimeValue": {
                    "constant": {
                      "destination_format": "PARQUET"
                    }
                  }
                },
                "file_pattern": {
                  "runtimeValue": {
                    "constant": "file_*"
                  }
                },
                "pipelinechannel--dataset_id": {
                  "componentInputParameter": "pipelinechannel--dataset_id"
                },
                "pipelinechannel--get-data-version-Output": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "get-data-version"
                  }
                },
                "source_project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "table_name": {
                  "runtimeValue": {
                    "constant": "testing"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Extract test data"
            }
          },
          "compare-models": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-compare-models"
            },
            "dependentTasks": [
              "bq-table-to-dataset-3",
              "for-loop-3"
            ],
            "inputs": {
              "artifacts": {
                "models": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "pipelinechannel--train-evaluate-model-model",
                    "producerTask": "for-loop-3"
                  }
                },
                "test_data": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dataset",
                    "producerTask": "bq-table-to-dataset-3"
                  }
                }
              },
              "parameters": {
                "higher_is_better": {
                  "runtimeValue": {
                    "constant": true
                  }
                },
                "metric_to_optimise": {
                  "runtimeValue": {
                    "constant": "Average Precision"
                  }
                },
                "target_column": {
                  "runtimeValue": {
                    "constant": "is_fraud"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Select best model"
            }
          },
          "condition-4": {
            "componentRef": {
              "name": "comp-condition-4"
            },
            "dependentTasks": [
              "bq-table-to-dataset-3",
              "compare-models",
              "lookup-model"
            ],
            "inputs": {
              "artifacts": {
                "pipelinechannel--bq-table-to-dataset-3-dataset": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dataset",
                    "producerTask": "bq-table-to-dataset-3"
                  }
                },
                "pipelinechannel--compare-models-best_model": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "best_model",
                    "producerTask": "compare-models"
                  }
                },
                "pipelinechannel--lookup-model-model": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "model",
                    "producerTask": "lookup-model"
                  }
                }
              },
              "parameters": {
                "pipelinechannel--lookup-model-Output": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "lookup-model"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Champion model exists"
            },
            "triggerPolicy": {
              "condition": "inputs.parameter_values['pipelinechannel--lookup-model-Output'] != ''"
            }
          },
          "execute-query": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-execute-query"
            },
            "dependentTasks": [
              "get-data-version"
            ],
            "inputs": {
              "parameters": {
                "bq_client_project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "pipelinechannel--dataset_id": {
                  "componentInputParameter": "pipelinechannel--dataset_id"
                },
                "pipelinechannel--get-data-version-Output": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "get-data-version"
                  }
                },
                "pipelinechannel--project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "query": {
                  "runtimeValue": {
                    "constant": "CREATE TEMP TABLE merged\nCLUSTER BY mcc, datetime_unix_seconds\nAS (\n    WITH transactions AS (\n        /* Features related to transactions */\n        SELECT\n            ROW_NUMBER() OVER() AS transaction_id,\n            DATETIME(t.year, t.month, t.day, CAST(SUBSTR(t.time, 1, 2) AS INT), CAST(SUBSTR(t.time, 4, 2) AS INT), 0) AS datetime,\n            t.year,\n            SIN((CAST(SUBSTR(t.time, 1, 2) AS INT) / 24) * 2 * ACOS(-1)) AS hour_sin,\n            COS((CAST(SUBSTR(t.time, 1, 2) AS INT) / 24) * 2 * ACOS(-1)) AS hour_cos,\n            SIN(((t.month - 1) / 12) * 2 * ACOS(-1)) AS month_sin,\n            COS(((t.month - 1) / 12) * 2 * ACOS(-1)) AS month_cos,\n            IF(t.year >= 2015, 1, 0) AS is_2015_or_later,\n            t.amount,\n            IF(t.use_chip = \"Chip Transaction\", 1, 0) AS chip_transaction,\n            IF(t.use_chip = \"Swipe Transaction\", 1, 0) AS swipe_transaction,\n            IF(t.use_chip = \"Online Transaction\", 1, 0) AS online_transaction,\n            IF(t.use_chip = \"Chip Transaction\" OR t.use_chip = \"Swipe Transaction\", 1, 0) AS card_present_transaction,\n            t.card,\n            t.user,\n            t.mcc,\n            CAST(t.Is_Fraud_ AS INT) AS is_fraud\n\n        FROM `{{$.inputs.parameters['pipelinechannel--project_id']}}.{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}.transactions` t\n    )\n\n    , users AS (\n        /* Features related to users */\n        SELECT\n            u.user,\n            IF(u.gender = \"Male\", 0, 1) AS gender,\n            CAST(REPLACE(u.`Per Capita Income - Zipcode`, \"$\", \"\") AS NUMERIC) AS per_capita_income_zipcode,\n            CAST(REPLACE(u.`Yearly Income - Person`, \"$\", \"\") AS NUMERIC) AS yearly_income_person,\n            CAST(REPLACE(u.`Total Debt`, \"$\", \"\") AS NUMERIC) AS total_debt,\n\n        FROM `{{$.inputs.parameters['pipelinechannel--project_id']}}.{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}.users` u\n    )\n\n    , cards AS (\n        /* Features related to cards */\n        SELECT\n            user,\n            card_index,\n            IF(card_brand = \"Amex\", 1, 0) AS amex,\n            IF(card_brand = \"Discover\", 1, 0) AS discover,\n            IF(card_brand = \"Mastercard\", 1, 0) AS mastercard,\n            IF(card_brand = \"Visa\", 1, 0) AS visa,\n            IF(card_type = \"Credit\", 1, 0) AS credit,\n            IF(card_type = \"Debit\", 1, 0) AS debit,\n            IF(card_type = \"Debit (Prepaid)\", 1, 0) AS debit_prepaid,\n            credit_limit,\n            IF(has_chip IS TRUE, 1, 0) AS has_chip,\n            IF(card_on_dark_web IS TRUE, 1, 0) AS card_on_dark_web,\n\n        FROM `{{$.inputs.parameters['pipelinechannel--project_id']}}.{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}.cards`\n    )\n\n    /* Merging all features together */\n    SELECT\n        t.transaction_id,\n        t.datetime,\n        t.year,\n        UNIX_SECONDS(CAST(t.datetime AS TIMESTAMP)) AS datetime_unix_seconds,\n        t.hour_sin,\n        t.hour_cos,\n        t.month_sin,\n        t.month_cos,\n        EXTRACT(DAYOFWEEK FROM t.datetime) AS day_of_week,\n        IF(EXTRACT(DAYOFWEEK FROM t.datetime) IN (1, 7), 1, 0) AS weekend,\n        IF(EXTRACT(DATE FROM t.datetime) IN (SELECT date FROM `{{$.inputs.parameters['pipelinechannel--project_id']}}.{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}.holidays`), 1, 0) AS is_holiday,\n        SIN((IF(EXTRACT(DAYOFWEEK FROM t.datetime) = 1, 6, EXTRACT(DAYOFWEEK FROM t.datetime) - 2) / 7) * 2 * ACOS(-1)) AS day_of_week_sin,\n        COS((IF(EXTRACT(DAYOFWEEK FROM t.datetime) = 1, 6, EXTRACT(DAYOFWEEK FROM t.datetime) - 2) / 7) * 2 * ACOS(-1)) AS day_of_week_cos,\n        t.is_2015_or_later,\n        t.amount,\n        t.swipe_transaction,\n        t.chip_transaction,\n        t.online_transaction,\n        t.card_present_transaction,\n        IF(t.is_fraud = 1 AND t.chip_transaction = 1, 1, 0) AS fraud_chip,\n        IF(t.is_fraud = 1 AND t.swipe_transaction = 1, 1, 0) AS fraud_swipe,\n        IF(t.is_fraud = 1 AND t.online_transaction = 1, 1, 0) AS fraud_online,\n        IF(t.is_fraud = 1 AND t.card_present_transaction = 1, 1, 0) AS fraud_card_present,\n        t.user,\n        t.mcc,\n        u.gender,\n        c.amex,\n        c.discover,\n        c.mastercard,\n        c.visa,\n        c.credit,\n        c.debit,\n        c.debit_prepaid,\n        c.credit_limit,\n        c.has_chip,\n        c.card_on_dark_web,\n        u.per_capita_income_zipcode,\n        u.yearly_income_person,\n        u.total_debt,\n        t.is_fraud,\n\n    FROM transactions t\n\n    INNER JOIN cards c\n        ON t.card = c.card_index\n        AND t.user = c.user\n\n    INNER JOIN users u\n        ON t.user = u.user\n)\n;\n\nCREATE TEMP TABLE mcc_aux AS (\n    /* Rolling features related to MCC */\n    SELECT\n        m.transaction_id,\n        COALESCE(SUM(m.is_fraud) OVER(mcc_window) / COUNT(m.is_fraud) OVER(mcc_window), 0) AS mcc_mean_encoding\n\n    FROM merged m\n    WINDOW mcc_window AS (PARTITION BY m.mcc ORDER BY m.datetime_unix_seconds RANGE BETWEEN UNBOUNDED PRECEDING AND 604800 PRECEDING)\n)\n;\n\nCREATE TEMP TABLE user_aux\nCLUSTER BY user, datetime_unix_seconds\n/* Rolling features related to users */\nAS (\n    SELECT\n        m.*,\n        COALESCE(AVG(m.amount) OVER(user_window), 0) AS mean_amount,\n        COUNT(m.transaction_id) OVER(user_window) AS transaction_count,\n        DATE_DIFF(m.datetime, MIN(m.datetime) OVER(PARTITION BY m.user), DAY) AS days_since_first_transaction,\n\n    FROM merged m\n    WINDOW user_window AS (PARTITION BY m.user ORDER BY m.datetime ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING)\n)\n;\n\nCREATE TEMP TABLE rolling_aux_amount_frequency AS (\n    /* Rolling features related to transaction amount and frquency */\n    WITH tmp AS (\n        SELECT\n            r.*,\n            COALESCE(AVG(r.amount) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 86400 PRECEDING AND 1 PRECEDING), 0) AS mean_amount_last_1_days,\n            COALESCE(AVG(r.amount) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 172800 PRECEDING AND 1 PRECEDING), 0) AS mean_amount_last_2_days,\n            COALESCE(AVG(r.amount) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 604800 PRECEDING AND 1 PRECEDING), 0) AS mean_amount_last_7_days,\n            COALESCE(AVG(r.amount) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 1 PRECEDING), 0) AS mean_amount_last_30_days,\n            COALESCE(AVG(r.amount) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 1 PRECEDING), 0) AS mean_amount_last_year,\n            IF(\n                r.days_since_first_transaction > 0,\n                (r.transaction_count - 1) / r.days_since_first_transaction,\n                0\n            ) AS transaction_frequency_all,\n            IF(\n                r.days_since_first_transaction > 0,\n                COUNT(r.transaction_id) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 86400 PRECEDING AND 1 PRECEDING)\n                    / LEAST(1, r.days_since_first_transaction),\n                0\n            ) AS transaction_frequency_last_1_days,\n            IF(\n                r.days_since_first_transaction > 0,\n                COUNT(r.transaction_id) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 172800 PRECEDING AND 1 PRECEDING)\n                    / LEAST(2, r.days_since_first_transaction),\n                0\n            ) AS transaction_frequency_last_2_days,\n            IF(\n                r.days_since_first_transaction > 0,\n                COUNT(r.transaction_id) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 604800 PRECEDING AND 1 PRECEDING)\n                    / LEAST(7, r.days_since_first_transaction),\n                0\n            ) AS transaction_frequency_last_7_days,\n            IF(\n                r.days_since_first_transaction > 0,\n                COUNT(r.transaction_id) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 1 PRECEDING)\n                    / LEAST(30, r.days_since_first_transaction),\n                0\n            ) AS transaction_frequency_last_30_days,\n            IF(\n                r.days_since_first_transaction > 0,\n                COUNT(r.transaction_id) OVER(PARTITION BY r.user ORDER BY r.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 1 PRECEDING)\n                    / LEAST(365, r.days_since_first_transaction),\n                0\n            ) AS transaction_frequency_last_year\n\n        FROM user_aux r\n    )\n    SELECT\n        t.transaction_id,\n        t.mean_amount,\n        t.transaction_count,\n        days_since_first_transaction,\n        mean_amount_last_year,\n        mean_amount_last_30_days,\n        mean_amount_last_7_days,\n        mean_amount_last_2_days,\n        mean_amount_last_1_days,\n        transaction_frequency_all,\n        transaction_frequency_last_year,\n        transaction_frequency_last_30_days,\n        transaction_frequency_last_7_days,\n        transaction_frequency_last_2_days,\n        transaction_frequency_last_1_days,\n\n        COALESCE(SAFE_DIVIDE(mean_amount_last_7_days, mean_amount_last_year), 0) AS mean_amount_last_7_days_relative_to_last_year,\n        COALESCE(SAFE_DIVIDE(mean_amount_last_2_days, mean_amount_last_year), 0) AS mean_amount_last_2_days_relative_to_last_year,\n        COALESCE(SAFE_DIVIDE(mean_amount_last_1_days, mean_amount_last_year), 0) AS mean_amount_last_1_days_relative_to_last_year,\n\n        COALESCE(SAFE_DIVIDE(mean_amount_last_7_days, mean_amount_last_30_days), 0) AS mean_amount_last_7_days_relative_to_last_30_days,\n        COALESCE(SAFE_DIVIDE(mean_amount_last_2_days, mean_amount_last_30_days), 0) AS mean_amount_last_2_days_relative_to_last_30_days,\n        COALESCE(SAFE_DIVIDE(mean_amount_last_1_days, mean_amount_last_30_days), 0) AS mean_amount_last_1_days_relative_to_last_30_days,\n\n        COALESCE(SAFE_DIVIDE(transaction_frequency_last_7_days, transaction_frequency_last_year), 0) AS `7_days_transaction_frequency_relative_to_last_year`,\n        COALESCE(SAFE_DIVIDE(transaction_frequency_last_2_days, transaction_frequency_last_year), 0) AS `2_days_transaction_frequency_relative_to_last_year`,\n        COALESCE(SAFE_DIVIDE(transaction_frequency_last_1_days, transaction_frequency_last_year), 0) AS `1_days_transaction_frequency_relative_to_last_year`,\n\n        COALESCE(SAFE_DIVIDE(transaction_frequency_last_7_days, transaction_frequency_last_30_days), 0) AS `7_days_transaction_frequency_relative_to_last_30_days`,\n        COALESCE(SAFE_DIVIDE(transaction_frequency_last_2_days, transaction_frequency_last_30_days), 0) AS `2_days_transaction_frequency_relative_to_last_30_days`,\n        COALESCE(SAFE_DIVIDE(transaction_frequency_last_1_days, transaction_frequency_last_30_days), 0) AS `1_days_transaction_frequency_relative_to_last_30_days`,\n\n    FROM tmp t\n)\n;\n\nCREATE TEMP TABLE rolling_aux_frauds\n/* Rolling features related to amount of frauds */\nAS (\n    WITH tmp1 AS (\n        SELECT\n            m.transaction_id,\n            m.year,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_2_years,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_365_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_60_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_30_days,\n\n        FROM merged m\n\n        WHERE m.year <= 2000\n    )\n\n    , tmp2 AS (\n        SELECT\n            m.transaction_id,\n            m.year,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_2_years,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_365_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_60_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_30_days,\n\n        FROM merged m\n\n        WHERE m.year BETWEEN 1998 AND 2010\n    )\n\n    , tmp3 AS (\n        SELECT\n            m.transaction_id,\n            m.year,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_2_years,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_365_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_60_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_30_days,\n\n        FROM merged m\n\n        WHERE m.year BETWEEN 2008 AND 2015\n    )\n\n    , tmp4 AS (\n        SELECT\n            m.transaction_id,\n            m.year,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_2_years,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_365_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_60_days,\n            COALESCE(AVG(m.is_fraud) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_swipe) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_swipe_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_chip) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_chip_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_online) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_online_rolling_mean_30_days,\n\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 63072000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_2_years,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 31536000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_365_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 5184000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_60_days,\n            COALESCE(AVG(m.fraud_card_present) OVER (ORDER BY m.datetime_unix_seconds RANGE BETWEEN 2592000 PRECEDING AND 604800 PRECEDING), 0) AS fraud_card_present_rolling_mean_30_days,\n\n        FROM merged m\n\n        WHERE m.year >= 2013\n    )\n\n    , all_years AS (\n        SELECT * EXCEPT(year) FROM tmp1\n\n        UNION ALL\n\n        SELECT * EXCEPT(year) FROM tmp2 WHERE year BETWEEN 2001 AND 2010\n\n        UNION ALL\n\n        SELECT * EXCEPT(year) FROM tmp3 WHERE year BETWEEN 2011 AND 2015\n\n        UNION ALL\n\n        SELECT * EXCEPT(year) FROM tmp4 WHERE year >= 2016\n    )\n\n    SELECT\n        a.*,\n        COALESCE(SAFE_DIVIDE(fraud_rolling_mean_30_days, fraud_rolling_mean_365_days), 0) AS fraud_rolling_30_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_rolling_mean_30_days, fraud_rolling_mean_2_years), 0) AS fraud_rolling_30_days_relative_to_2_years,\n        COALESCE(SAFE_DIVIDE(fraud_rolling_mean_60_days, fraud_rolling_mean_365_days), 0) AS fraud_rolling_60_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_rolling_mean_60_days, fraud_rolling_mean_2_years), 0) AS fraud_rolling_60_days_relative_to_2_years,\n\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_30_days, fraud_rolling_mean_365_days), 0) AS fraud_swipe_rolling_30_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_30_days, fraud_rolling_mean_2_years), 0) AS fraud_swipe_rolling_30_days_relative_to_2_years,\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_60_days, fraud_rolling_mean_365_days), 0) AS fraud_swipe_rolling_60_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_60_days, fraud_rolling_mean_2_years), 0) AS fraud_swipe_rolling_60_days_relative_to_2_years,\n\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_30_days, fraud_rolling_mean_365_days), 0) AS fraud_chip_rolling_30_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_30_days, fraud_rolling_mean_2_years), 0) AS fraud_chip_rolling_30_days_relative_to_2_years,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_60_days, fraud_rolling_mean_365_days), 0) AS fraud_chip_rolling_60_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_60_days, fraud_rolling_mean_2_years), 0) AS fraud_chip_rolling_60_days_relative_to_2_years,\n\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_30_days, fraud_rolling_mean_365_days), 0) AS fraud_online_rolling_30_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_30_days, fraud_rolling_mean_2_years), 0) AS fraud_online_rolling_30_days_relative_to_2_years,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_60_days, fraud_rolling_mean_365_days), 0) AS fraud_online_rolling_60_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_60_days, fraud_rolling_mean_2_years), 0) AS fraud_online_rolling_60_days_relative_to_2_years,\n\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_30_days, fraud_rolling_mean_365_days), 0) AS fraud_card_present_rolling_30_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_30_days, fraud_rolling_mean_2_years), 0) AS fraud_card_present_rolling_30_days_relative_to_2_years,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_60_days, fraud_rolling_mean_365_days), 0) AS fraud_card_present_rolling_60_days_relative_to_365_days,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_60_days, fraud_rolling_mean_2_years), 0) AS fraud_card_present_rolling_60_days_relative_to_2_years,\n\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_30_days, fraud_rolling_mean_30_days), 0) AS fraud_swipe_rolling_30_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_30_days, fraud_rolling_mean_30_days), 0) AS fraud_chip_rolling_30_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_30_days, fraud_rolling_mean_30_days), 0) AS fraud_online_rolling_30_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_30_days, fraud_rolling_mean_30_days), 0) AS fraud_card_present_rolling_30_days_relative_to_all_frauds,\n\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_60_days, fraud_rolling_mean_60_days), 0) AS fraud_swipe_rolling_60_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_60_days, fraud_rolling_mean_60_days), 0) AS fraud_chip_rolling_60_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_60_days, fraud_rolling_mean_60_days), 0) AS fraud_online_rolling_60_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_60_days, fraud_rolling_mean_60_days), 0) AS fraud_card_present_rolling_60_days_relative_to_all_frauds,\n\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_365_days, fraud_rolling_mean_365_days), 0) AS fraud_swipe_rolling_365_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_365_days, fraud_rolling_mean_365_days), 0) AS fraud_chip_rolling_365_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_365_days, fraud_rolling_mean_365_days), 0) AS fraud_online_rolling_365_days_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_365_days, fraud_rolling_mean_365_days), 0) AS fraud_card_present_rolling_365_days_relative_to_all_frauds,\n\n        COALESCE(SAFE_DIVIDE(fraud_swipe_rolling_mean_2_years, fraud_rolling_mean_2_years), 0) AS fraud_swipe_rolling_2_years_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_chip_rolling_mean_2_years, fraud_rolling_mean_2_years), 0) AS fraud_chip_rolling_2_years_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_online_rolling_mean_2_years, fraud_rolling_mean_2_years), 0) AS fraud_online_rolling_2_years_relative_to_all_frauds,\n        COALESCE(SAFE_DIVIDE(fraud_card_present_rolling_mean_2_years, fraud_rolling_mean_2_years), 0) AS fraud_card_present_rolling_2_years_relative_to_all_frauds,\n\n    FROM all_years a\n)\n;\n\nCREATE OR REPLACE TABLE `{{$.inputs.parameters['pipelinechannel--project_id']}}.{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}.preprocessed`\nCLUSTER BY datetime_unix_seconds\nAS (\n    SELECT\n        m.transaction_id,\n        m.datetime_unix_seconds,\n        `amount`,\n`has_chip`,\n`gender`,\n`online_transaction`,\n`amex`,\n`discover`,\n`mastercard`,\n`visa`,\n`credit`,\n`debit`,\n`debit_prepaid`,\n`mcc_mean_encoding`,\n`card_present_transaction`,\n`fraud_rolling_mean_30_days`,\n`fraud_rolling_mean_60_days`,\n`fraud_rolling_mean_365_days`,\n`fraud_rolling_mean_2_years`,\n`fraud_online_rolling_mean_30_days`,\n`fraud_online_rolling_mean_60_days`,\n`fraud_online_rolling_mean_365_days`,\n`fraud_online_rolling_mean_2_years`,\n`fraud_card_present_rolling_mean_30_days`,\n`fraud_card_present_rolling_mean_60_days`,\n`fraud_card_present_rolling_mean_365_days`,\n`fraud_card_present_rolling_mean_2_years`,\n`fraud_rolling_30_days_relative_to_365_days`,\n`fraud_rolling_30_days_relative_to_2_years`,\n`fraud_rolling_60_days_relative_to_365_days`,\n`fraud_rolling_60_days_relative_to_2_years`,\n`fraud_online_rolling_30_days_relative_to_365_days`,\n`fraud_online_rolling_30_days_relative_to_2_years`,\n`fraud_online_rolling_60_days_relative_to_365_days`,\n`fraud_online_rolling_60_days_relative_to_2_years`,\n`fraud_card_present_rolling_30_days_relative_to_365_days`,\n`fraud_card_present_rolling_30_days_relative_to_2_years`,\n`fraud_card_present_rolling_60_days_relative_to_365_days`,\n`fraud_card_present_rolling_60_days_relative_to_2_years`,\n`fraud_online_rolling_30_days_relative_to_all_frauds`,\n`fraud_card_present_rolling_30_days_relative_to_all_frauds`,\n`fraud_online_rolling_60_days_relative_to_all_frauds`,\n`fraud_card_present_rolling_60_days_relative_to_all_frauds`,\n`fraud_online_rolling_365_days_relative_to_all_frauds`,\n`fraud_card_present_rolling_365_days_relative_to_all_frauds`,\n`fraud_online_rolling_2_years_relative_to_all_frauds`,\n`fraud_card_present_rolling_2_years_relative_to_all_frauds`,\n`hour_sin`,\n`hour_cos`,\n`month_sin`,\n`month_cos`,\n`day_of_week_sin`,\n`day_of_week_cos`,\n`is_holiday`,\n`weekend`,\n`is_2015_or_later`,\n`mean_amount`,\n`mean_amount_last_year`,\n`mean_amount_last_30_days`,\n`mean_amount_last_7_days`,\n`mean_amount_last_2_days`,\n`mean_amount_last_1_days`,\n`mean_amount_last_7_days_relative_to_last_year`,\n`mean_amount_last_2_days_relative_to_last_year`,\n`mean_amount_last_1_days_relative_to_last_year`,\n`mean_amount_last_7_days_relative_to_last_30_days`,\n`mean_amount_last_2_days_relative_to_last_30_days`,\n`mean_amount_last_1_days_relative_to_last_30_days`,\n`transaction_count`,\n`days_since_first_transaction`,\n`transaction_frequency_all`,\n`transaction_frequency_last_year`,\n`transaction_frequency_last_30_days`,\n`transaction_frequency_last_7_days`,\n`transaction_frequency_last_2_days`,\n`transaction_frequency_last_1_days`,\n`1_days_transaction_frequency_relative_to_last_30_days`,\n`1_days_transaction_frequency_relative_to_last_year`,\n`2_days_transaction_frequency_relative_to_last_30_days`,\n`2_days_transaction_frequency_relative_to_last_year`,\n`7_days_transaction_frequency_relative_to_last_30_days`,\n`7_days_transaction_frequency_relative_to_last_year`,\n        is_fraud\n\n    FROM merged m\n\n    INNER JOIN mcc_aux mc\n        ON m.transaction_id = mc.transaction_id\n\n    INNER JOIN rolling_aux_amount_frequency raf\n        ON m.transaction_id = raf.transaction_id\n\n    INNER JOIN rolling_aux_frauds rf\n        ON m.transaction_id = rf.transaction_id\n)\n;"
                  }
                },
                "query_job_config": {
                  "runtimeValue": {
                    "constant": {
                      "use_query_cache": true
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Preprocess input data"
            }
          },
          "execute-query-2": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-execute-query-2"
            },
            "dependentTasks": [
              "execute-query",
              "get-data-version"
            ],
            "inputs": {
              "parameters": {
                "bq_client_project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "pipelinechannel--dataset_id": {
                  "componentInputParameter": "pipelinechannel--dataset_id"
                },
                "pipelinechannel--get-data-version-Output": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "get-data-version"
                  }
                },
                "pipelinechannel--project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "query": {
                  "runtimeValue": {
                    "constant": "DECLARE train_limit INT64 DEFAULT (\n    SELECT APPROX_QUANTILES(datetime_unix_seconds, 100)[OFFSET(CAST(100 * (1 - 0.15 - 0.15) AS INT))] train_limit\n    FROM `{{$.inputs.parameters['pipelinechannel--project_id']}}.{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}.preprocessed`\n)\n;\n\nCREATE OR REPLACE TABLE `{{$.inputs.parameters['pipelinechannel--project_id']}}.{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}.training` AS (\n    SELECT t.* EXCEPT(datetime_unix_seconds)\n\n    FROM `{{$.inputs.parameters['pipelinechannel--project_id']}}.{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}.preprocessed` t\n\n    WHERE t.datetime_unix_seconds < train_limit\n)\n;\n\nCREATE TEMP TABLE validation_testing AS (\n    SELECT t.* EXCEPT(datetime_unix_seconds)\n\n    FROM `{{$.inputs.parameters['pipelinechannel--project_id']}}.{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}.preprocessed` t\n\n    WHERE t.datetime_unix_seconds >= train_limit\n)\n;\n\nCREATE OR REPLACE TABLE `{{$.inputs.parameters['pipelinechannel--project_id']}}.{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}.validation` AS (\n    SELECT t.*\n\n    FROM validation_testing t\n\n    WHERE ABS(MOD(t.transaction_id, 100)) < CAST(0.15 / (0.15 + 0.15) * 100 AS INT)\n)\n;\n\nCREATE OR REPLACE TABLE `{{$.inputs.parameters['pipelinechannel--project_id']}}.{{$.inputs.parameters['pipelinechannel--dataset_id']}}_{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}.testing` AS (\n    SELECT t.*\n\n    FROM validation_testing t\n\n    WHERE ABS(MOD(t.transaction_id, 100)) >= CAST(0.15 / (0.15 + 0.15) * 100 AS INT)\n)\n;"
                  }
                },
                "query_job_config": {
                  "runtimeValue": {
                    "constant": {
                      "use_query_cache": true
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Train / validation / test split"
            }
          },
          "for-loop-3": {
            "componentRef": {
              "name": "comp-for-loop-3"
            },
            "dependentTasks": [
              "bq-table-to-dataset",
              "bq-table-to-dataset-2",
              "bq-table-to-dataset-3",
              "get-current-time",
              "get-data-version"
            ],
            "inputs": {
              "artifacts": {
                "pipelinechannel--bq-table-to-dataset-2-dataset": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dataset",
                    "producerTask": "bq-table-to-dataset-2"
                  }
                },
                "pipelinechannel--bq-table-to-dataset-3-dataset": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dataset",
                    "producerTask": "bq-table-to-dataset-3"
                  }
                },
                "pipelinechannel--bq-table-to-dataset-dataset": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dataset",
                    "producerTask": "bq-table-to-dataset"
                  }
                }
              },
              "parameters": {
                "pipelinechannel--get-current-time-Output": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "get-current-time"
                  }
                },
                "pipelinechannel--get-data-version-Output": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "get-data-version"
                  }
                },
                "pipelinechannel--project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "pipelinechannel--project_location": {
                  "componentInputParameter": "pipelinechannel--project_location"
                }
              }
            },
            "iteratorPolicy": {
              "parallelismLimit": 2
            },
            "parameterIterator": {
              "itemInput": "pipelinechannel--loop-item-param-2",
              "items": {
                "raw": "[\"logistic_regression\", \"sgd_classifier\", \"random_forest\", \"xgboost\", \"lightgbm\"]"
              }
            },
            "taskInfo": {
              "name": "Train and evaluate models"
            }
          },
          "generate-training-stats-schema": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-generate-training-stats-schema"
            },
            "dependentTasks": [
              "bq-table-to-dataset"
            ],
            "inputs": {
              "artifacts": {
                "training_data": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "dataset",
                    "producerTask": "bq-table-to-dataset"
                  }
                }
              },
              "parameters": {
                "artifacts_gcs_folder_path": {
                  "runtimeValue": {
                    "constant": "gs://robertofierimonte-ml-pipe/credit-card-frauds/artifacts/master/586abc1"
                  }
                },
                "location": {
                  "componentInputParameter": "pipelinechannel--project_location"
                },
                "project": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "target_column": {
                  "runtimeValue": {
                    "constant": "is_fraud"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Extract TFDV training stats"
            }
          },
          "get-current-time": {
            "cachingOptions": {},
            "componentRef": {
              "name": "comp-get-current-time"
            },
            "inputs": {
              "parameters": {
                "format_str": {
                  "runtimeValue": {
                    "constant": "%Y%m%d%H%M%S"
                  }
                },
                "timestamp": {
                  "runtimeValue": {
                    "constant": "{{$.pipeline_job_create_time_utc}}"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Format current timestamp"
            }
          },
          "get-data-version": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-get-data-version"
            },
            "inputs": {
              "parameters": {
                "dataset_id": {
                  "componentInputParameter": "pipelinechannel--dataset_id"
                },
                "dataset_location": {
                  "componentInputParameter": "pipelinechannel--dataset_location"
                },
                "payload_data_version": {
                  "componentInputParameter": "pipelinechannel--data_version"
                },
                "project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                }
              }
            },
            "taskInfo": {
              "name": "Get data version"
            }
          },
          "lookup-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-lookup-model"
            },
            "inputs": {
              "parameters": {
                "model_label": {
                  "runtimeValue": {
                    "constant": "champion"
                  }
                },
                "model_name": {
                  "runtimeValue": {
                    "constant": "credit-card-frauds"
                  }
                },
                "project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "project_location": {
                  "componentInputParameter": "pipelinechannel--project_location"
                }
              }
            },
            "taskInfo": {
              "name": "Lookup champion model"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "pipelinechannel--data_version": {
            "parameterType": "STRING"
          },
          "pipelinechannel--dataset_id": {
            "parameterType": "STRING"
          },
          "pipelinechannel--dataset_location": {
            "parameterType": "STRING"
          },
          "pipelinechannel--project_id": {
            "parameterType": "STRING"
          },
          "pipelinechannel--project_location": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "evaluate-model-test_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "train-evaluate-model-train_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "train-evaluate-model-valid_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-for-loop-3": {
      "dag": {
        "outputs": {
          "artifacts": {
            "evaluate-model-test_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "test_metrics",
                  "producerSubtask": "evaluate-model"
                }
              ]
            },
            "pipelinechannel--train-evaluate-model-model": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "model",
                  "producerSubtask": "train-evaluate-model"
                }
              ]
            },
            "train-evaluate-model-train_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "train_metrics",
                  "producerSubtask": "train-evaluate-model"
                }
              ]
            },
            "train-evaluate-model-valid_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "valid_metrics",
                  "producerSubtask": "train-evaluate-model"
                }
              ]
            }
          }
        },
        "tasks": {
          "evaluate-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-evaluate-model"
            },
            "dependentTasks": [
              "train-evaluate-model"
            ],
            "inputs": {
              "artifacts": {
                "model": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "model",
                    "producerTask": "train-evaluate-model"
                  }
                },
                "test_data": {
                  "componentInputArtifact": "pipelinechannel--bq-table-to-dataset-3-dataset"
                }
              },
              "parameters": {
                "target_column": {
                  "runtimeValue": {
                    "constant": "is_fraud"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Evaluate model on test set"
            }
          },
          "train-evaluate-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-evaluate-model"
            },
            "inputs": {
              "artifacts": {
                "training_data": {
                  "componentInputArtifact": "pipelinechannel--bq-table-to-dataset-dataset"
                },
                "validation_data": {
                  "componentInputArtifact": "pipelinechannel--bq-table-to-dataset-2-dataset"
                }
              },
              "parameters": {
                "data_processing_args": {
                  "runtimeValue": {
                    "constant": {
                      "data_sampling": "rose",
                      "rose_shrinkage": 0.5,
                      "rose_upsampled_minority_proportion": 0.3
                    }
                  }
                },
                "fit_args": {
                  "runtimeValue": {
                    "constant": {
                      "xgboost": {
                        "verbose": 0.0
                      }
                    }
                  }
                },
                "location": {
                  "componentInputParameter": "pipelinechannel--project_location"
                },
                "model_gcs_folder_path": {
                  "runtimeValue": {
                    "constant": "gs://robertofierimonte-ml-pipe/credit-card-frauds/models/master/586abc1"
                  }
                },
                "model_name": {
                  "componentInputParameter": "pipelinechannel--loop-item-param-2"
                },
                "models_params": {
                  "runtimeValue": {
                    "constant": {
                      "lightgbm": {
                        "is_unbalance": true,
                        "verbosity": 0.0
                      },
                      "logistic_regression": {
                        "max_iter": 500.0
                      },
                      "random_forest": {
                        "bootstrap": true,
                        "class_weight": "balanced",
                        "max_depth": 6.0,
                        "max_samples": 0.3,
                        "n_jobs": -1.0
                      },
                      "sgd_classifier": {
                        "loss": "modified_huber"
                      },
                      "xgboost": {
                        "eval_metric": "logloss",
                        "scale_pos_weight": 10.0,
                        "verbosity": 0.0
                      }
                    }
                  }
                },
                "project": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "target_column": {
                  "runtimeValue": {
                    "constant": "is_fraud"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Train and evaluate model"
            }
          },
          "upload-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-upload-model"
            },
            "dependentTasks": [
              "train-evaluate-model"
            ],
            "inputs": {
              "artifacts": {
                "model": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "model",
                    "producerTask": "train-evaluate-model"
                  }
                }
              },
              "parameters": {
                "data_version": {
                  "runtimeValue": {
                    "constant": "{{$.inputs.parameters['pipelinechannel--get-data-version-Output']}}"
                  }
                },
                "description": {
                  "runtimeValue": {
                    "constant": "Credit card frauds model"
                  }
                },
                "display_name": {
                  "runtimeValue": {
                    "constant": "credit-card-frauds"
                  }
                },
                "is_default_version": {
                  "runtimeValue": {
                    "constant": false
                  }
                },
                "labels": {
                  "runtimeValue": {
                    "constant": {
                      "branch_name": "master",
                      "commit_hash": "586abc1",
                      "pipeline_tag": "dev"
                    }
                  }
                },
                "model_id": {
                  "runtimeValue": {
                    "constant": "credit-card-frauds"
                  }
                },
                "model_name": {
                  "componentInputParameter": "pipelinechannel--loop-item-param-2"
                },
                "pipeline_timestamp": {
                  "runtimeValue": {
                    "constant": "{{$.inputs.parameters['pipelinechannel--get-current-time-Output']}}"
                  }
                },
                "pipelinechannel--get-current-time-Output": {
                  "componentInputParameter": "pipelinechannel--get-current-time-Output"
                },
                "pipelinechannel--get-data-version-Output": {
                  "componentInputParameter": "pipelinechannel--get-data-version-Output"
                },
                "project_id": {
                  "componentInputParameter": "pipelinechannel--project_id"
                },
                "project_location": {
                  "componentInputParameter": "pipelinechannel--project_location"
                },
                "serving_container_image_uri": {
                  "runtimeValue": {
                    "constant": "europe-west2-docker.pkg.dev/robertofierimonte-ml-pipe/docker-repo/credit-card-frauds:latest"
                  }
                },
                "version_description": {
                  "runtimeValue": {
                    "constant": "Credit card frauds model"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Upload model"
            }
          }
        }
      },
      "inputDefinitions": {
        "artifacts": {
          "pipelinechannel--bq-table-to-dataset-2-dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "pipelinechannel--bq-table-to-dataset-3-dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "pipelinechannel--bq-table-to-dataset-dataset": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "pipelinechannel--get-current-time-Output": {
            "parameterType": "STRING"
          },
          "pipelinechannel--get-data-version-Output": {
            "parameterType": "STRING"
          },
          "pipelinechannel--loop-item-param-2": {
            "parameterType": "STRING"
          },
          "pipelinechannel--project_id": {
            "parameterType": "STRING"
          },
          "pipelinechannel--project_location": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "evaluate-model-test_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "pipelinechannel--train-evaluate-model-model": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            },
            "isArtifactList": true
          },
          "train-evaluate-model-train_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "train-evaluate-model-valid_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "comp-generate-training-stats-schema": {
      "executorLabel": "exec-generate-training-stats-schema",
      "inputDefinitions": {
        "artifacts": {
          "training_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "artifacts_gcs_folder_path": {
            "isOptional": true,
            "parameterType": "STRING"
          },
          "base_output_directory": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "display_name": {
            "defaultValue": "generate-training-stats-schema",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "enable_web_access": {
            "defaultValue": false,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "encryption_spec_key_name": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "labels": {
            "defaultValue": {},
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "location": {
            "defaultValue": "us-central1",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "network": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "project": {
            "parameterType": "STRING"
          },
          "reserved_ip_ranges": {
            "defaultValue": [],
            "isOptional": true,
            "parameterType": "LIST"
          },
          "restart_job_on_worker_restart": {
            "defaultValue": false,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "service_account": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "target_column": {
            "parameterType": "STRING"
          },
          "tensorboard": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "timeout": {
            "defaultValue": "604800s",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "worker_pool_specs": {
            "defaultValue": [
              {
                "container_spec": {
                  "args": [
                    "--executor_input",
                    "{{$.json_escape[1]}}",
                    "--function_to_execute",
                    "generate_training_stats_schema"
                  ],
                  "command": [
                    "sh",
                    "-c",
                    "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'loguru==0.7.0' 'pyarrow==6.0.1' 'pandas==1.4.3' 'tensorflow-data-validation==1.13.0' 'kfp==2.0.1' && \"$0\" \"$@\"\n",
                    "sh",
                    "-ec",
                    "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
                    "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef generate_training_stats_schema(\n    training_data: Input[Dataset],\n    training_stats: Output[Artifact],\n    training_schema: Output[Artifact],\n    target_column: str,\n    artifacts_gcs_folder_path: str = None,\n) -> None:\n    \"\"\"_summary_\n\n    Args:\n        training_data (Input[Dataset]): _description_\n        training_stats (Output[Artifact]): _description_\n        training_schema (Output[Artifact]): _description_\n        target_column (str): _description_\n        artifacts_gcs_folder_path (str, optional): _description_. Defaults to None.\n    \"\"\"\n    from pathlib import Path\n\n    import pandas as pd\n    import tensorflow_data_validation as tfdv\n    from loguru import logger\n\n    df_train = pd.read_parquet(training_data.path)\n    df_train = df_train.drop(columns=[\"transaction_id\"])\n    logger.info(f\"Loaded training data, shape {df_train.shape}.\")\n\n    stats_train = tfdv.generate_statistics_from_dataframe(df_train)\n    schema_train = tfdv.infer_schema(stats_train)\n    schema_train.default_environment.append(\"TRAINING\")\n    schema_train.default_environment.append(\"SERVING\")\n    logger.info(\"Create training stats and schema.\")\n\n    tfdv.get_feature(schema_train, target_column).not_in_environment.append(\"SERVING\")\n    logger.info(\"Excluded target column from serving environment.\")\n\n    if artifacts_gcs_folder_path is not None:\n        artifacts_gcs_folder_path = artifacts_gcs_folder_path.replace(\"gs://\", \"/gcs/\")\n        training_stats.path = artifacts_gcs_folder_path\n        training_schema.path = artifacts_gcs_folder_path\n\n    training_stats.path = f\"{training_stats.path}/training_stats.pbtxt\"\n    directory = Path(training_stats.path).parent.absolute()\n    directory.mkdir(parents=True, exist_ok=True)\n\n    training_schema.path = f\"{training_schema.path}/training_schema.pbtxt\"\n    directory = Path(training_schema.path).parent.absolute()\n    directory.mkdir(parents=True, exist_ok=True)\n\n    tfdv.write_stats_text(stats_train, training_stats.path)\n    tfdv.write_schema_text(schema_train, training_schema.path)\n\n"
                  ],
                  "image_uri": "python:3.9"
                },
                "disk_spec": {
                  "boot_disk_size_gb": 100.0,
                  "boot_disk_type": "pd-ssd"
                },
                "machine_spec": {
                  "machine_type": "n1-standard-16"
                },
                "replica_count": 1.0
              }
            ],
            "isOptional": true,
            "parameterType": "LIST"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "training_schema": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          },
          "training_stats": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "gcp_resources": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-get-current-time": {
      "executorLabel": "exec-get-current-time",
      "inputDefinitions": {
        "parameters": {
          "format_str": {
            "description": "Formatting string for the output,\nmust be compatible with datetime. Defaults to None.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "subtract_days": {
            "defaultValue": 0.0,
            "description": "Number of days to subtract from the\ncurrent timestamp. Only used if `timestamp` is None. Defaults to 0.",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "timestamp": {
            "description": "Timestamp in ISO 8601 format.\nDefaults to None.",
            "isOptional": true,
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-get-data-version": {
      "executorLabel": "exec-get-data-version",
      "inputDefinitions": {
        "parameters": {
          "dataset_id": {
            "description": "Bigquery dataset ID. This function will look for the\nmost recent BQ dataset that has the pattern of\n{dataset_id}_%Y%m%dT%H%M%S.",
            "parameterType": "STRING"
          },
          "dataset_location": {
            "defaultValue": "europe-west2",
            "description": "Bigquery dataset location.\nDefaults to \"europe-west2\".",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "payload_data_version": {
            "description": "Data version provided in the payload file.",
            "parameterType": "STRING"
          },
          "project_id": {
            "description": "Bigquery project ID.",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-lookup-model": {
      "executorLabel": "exec-lookup-model",
      "inputDefinitions": {
        "parameters": {
          "fail_on_model_not_found": {
            "defaultValue": false,
            "description": "If set to True, raise an error\nif the model is not found. Defaults to False.",
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "model_label": {
            "description": "Version alias of the model. Defaults to None.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "model_name": {
            "description": "The ID (name) of the model.",
            "parameterType": "STRING"
          },
          "project_id": {
            "description": "GCP Project ID where the model is stored.",
            "parameterType": "STRING"
          },
          "project_location": {
            "description": "Location where the model is stored.",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "model": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-train-evaluate-model": {
      "executorLabel": "exec-train-evaluate-model",
      "inputDefinitions": {
        "artifacts": {
          "training_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          },
          "validation_data": {
            "artifactType": {
              "schemaTitle": "system.Dataset",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "base_output_directory": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "data_processing_args": {
            "defaultValue": {},
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "display_name": {
            "defaultValue": "train-evaluate-model",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "enable_web_access": {
            "defaultValue": false,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "encryption_spec_key_name": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "fit_args": {
            "defaultValue": {},
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "labels": {
            "defaultValue": {},
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "location": {
            "defaultValue": "us-central1",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "model_gcs_folder_path": {
            "isOptional": true,
            "parameterType": "STRING"
          },
          "model_name": {
            "parameterType": "STRING"
          },
          "models_params": {
            "defaultValue": {},
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "network": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "project": {
            "parameterType": "STRING"
          },
          "reserved_ip_ranges": {
            "defaultValue": [],
            "isOptional": true,
            "parameterType": "LIST"
          },
          "restart_job_on_worker_restart": {
            "defaultValue": false,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "service_account": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "target_column": {
            "parameterType": "STRING"
          },
          "tensorboard": {
            "defaultValue": "",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "timeout": {
            "defaultValue": "604800s",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "worker_pool_specs": {
            "defaultValue": [
              {
                "container_spec": {
                  "args": [
                    "--executor_input",
                    "{{$.json_escape[1]}}",
                    "--function_to_execute",
                    "train_evaluate_model"
                  ],
                  "command": [
                    "sh",
                    "-c",
                    "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-storage==2.9.0' 'matplotlib==3.5.1' 'kfp==2.0.1' && \"$0\" \"$@\"\n",
                    "sh",
                    "-ec",
                    "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
                    "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef train_evaluate_model(\n    training_data: Input[Dataset],\n    validation_data: Input[Dataset],\n    target_column: str,\n    model_name: str,\n    train_metrics: Output[Metrics],\n    valid_metrics: Output[Metrics],\n    valid_pr_curve: Output[Artifact],\n    model: Output[Model],\n    models_params: dict = {},\n    fit_args: dict = {},\n    data_processing_args: dict = {},\n    model_gcs_folder_path: str = None,\n) -> None:\n    \"\"\"Train a classification model on the training data.\n\n    Args:\n        training_data (Input[Dataset]): Training data as a KFP Dataset object.\n        validation_data (Input[Dataset]): Validation data (used to prevent overfitting)\n            as a KFP Dataset object.\n        target_column (str): Column containing the target column for classification.\n        model_name (str): Name of the classifier that will be trained. Must be one of\n            'logistic_regression', 'sgd_classifier', 'random_forest', 'lightgbm',\n            'xgboost'.\n        models_params (dict): Hyperparameters of the model. Default to an empty dict.\n        fit_args (dict): Arguments used when fitting the model.\n            Default to an empty dict.\n        data_processing_args (dict): Arguments used when running extra processing on\n            the data (such as scaling or oversampling). Default to an empty dict.\n        train_metrics (Output[Metrics]):\n        valid_metrics (Output[Metrics]):\n        model (Output[Model]): Output model as a KFP Model object, this parameter\n            will be passed automatically by the orchestrator. The .path\n            attribute is the location of the joblib file in GCS.\n    \"\"\"\n    from pathlib import Path\n\n    import joblib\n    import pandas as pd\n    from google.cloud import storage\n    from lightgbm import LGBMClassifier\n    from loguru import logger\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model import LogisticRegression, SGDClassifier\n    from xgboost import XGBClassifier\n\n    from src.base.model import evaluate_model, train_model\n    from src.base.visualisation import plot_precision_recall_curve\n\n    df_train = pd.read_parquet(training_data.path)\n    df_train = df_train.drop(columns=[\"transaction_id\"])\n    y_train = df_train.pop(target_column)\n    logger.info(f\"Loaded training data, shape {df_train.shape}.\")\n\n    df_valid = pd.read_parquet(validation_data.path)\n    df_valid = df_valid.drop(columns=[\"transaction_id\"])\n    y_valid = df_valid.pop(target_column)\n    logger.info(f\"Loaded evaluation data, shape {df_valid.shape}.\")\n\n    use_eval_set = False\n    model_params = models_params.get(model_name, {})\n    if model_name == \"logistic_regression\":\n        classifier = LogisticRegression(random_state=42, **model_params)\n    elif model_name == \"sgd_classifier\":\n        classifier = SGDClassifier(random_state=42, **model_params)\n    elif model_name == \"random_forest\":\n        classifier = RandomForestClassifier(random_state=42, **model_params)\n    elif model_name == \"lightgbm\":\n        classifier = LGBMClassifier(random_state=42, **model_params)\n        use_eval_set = True\n    elif model_name == \"xgboost\":\n        classifier = XGBClassifier(\n            use_label_encoder=False, random_state=42, **model_params\n        )\n        use_eval_set = True\n    else:\n        msg = (\n            \"`model_name` must be one of 'logistic_regression', 'sgd_classifier', \"\n            \"'random_forest', 'lightgbm', 'xgboost'.\"\n        )\n        logger.error(msg)\n        raise ValueError(msg)\n\n    logger.info(f\"Training model {model_name}.\")\n    classifier, training_metrics = train_model(\n        classifier,\n        X_train=df_train,\n        y_train=y_train,\n        X_valid=df_valid,\n        y_valid=y_valid,\n        use_eval_set=use_eval_set,\n        fit_args=fit_args.get(model_name, {}),\n        **data_processing_args,\n    )\n    logger.info(\"Training completed.\")\n    logger.debug(f\"Type of classifier: {type(classifier)}.\")\n    logger.debug(f\"Classifier: {classifier}.\")\n    for k, v in training_metrics.items():\n        if k != \"Precision Recall Curve\":\n            train_metrics.log_metric(k, v)\n\n    validation_metrics, _, _ = evaluate_model(classifier, df_valid, y_valid)\n    logger.info(\"Evaluation completed.\")\n    for k, v in validation_metrics.items():\n        if k != \"Precision Recall Curve\":\n            valid_metrics.log_metric(k, v)\n\n    logger.debug(f\"Type of classifier: {type(classifier)}.\")\n    logger.debug(f\"Classifier: {classifier}.\")\n\n    if model_gcs_folder_path is not None:\n        model_gcs_folder_path = model_gcs_folder_path.replace(\"gs://\", \"/gcs/\")\n        model.path = model_gcs_folder_path\n        valid_pr_curve.path = model_gcs_folder_path\n\n    model.path = f\"{model.path}/{model_name}\"\n    model_dir = Path(model.path).parent.absolute()\n    model_dir.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"Saving model to {model.path}.\")\n    joblib.dump(classifier, model.path)\n\n    valid_pr_curve.path = (\n        valid_pr_curve.path + f\"/precision_recall_curve_{model_name}.png\"\n    )\n    _ = plot_precision_recall_curve(\n        model=classifier,\n        model_name=model_name,\n        X=df_valid,\n        y=y_valid,\n        save_path=valid_pr_curve.path,\n    )\n\n    client = storage.Client()\n    logger.debug(f\"URI: {valid_pr_curve.uri}\")\n    logger.debug(f\"Path: {valid_pr_curve.path}\")\n\n"
                  ],
                  "image_uri": "europe-west2-docker.pkg.dev/robertofierimonte-ml-pipe/docker-repo/credit-card-frauds:latest"
                },
                "disk_spec": {
                  "boot_disk_size_gb": 100.0,
                  "boot_disk_type": "pd-ssd"
                },
                "machine_spec": {
                  "machine_type": "n1-standard-32"
                },
                "replica_count": 1.0
              }
            ],
            "isOptional": true,
            "parameterType": "LIST"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "model": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            }
          },
          "train_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "valid_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "valid_pr_curve": {
            "artifactType": {
              "schemaTitle": "system.Artifact",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "gcp_resources": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-upload-model": {
      "executorLabel": "exec-upload-model",
      "inputDefinitions": {
        "artifacts": {
          "model": {
            "artifactType": {
              "schemaTitle": "system.Model",
              "schemaVersion": "0.0.1"
            },
            "description": "Model to be uploaded."
          }
        },
        "parameters": {
          "data_version": {
            "parameterType": "STRING"
          },
          "description": {
            "description": "Description of the model.",
            "parameterType": "STRING"
          },
          "display_name": {
            "description": "The display name of the model. The name",
            "parameterType": "STRING"
          },
          "is_default_version": {
            "description": "When set to True, the newly uploaded model version\nwill automatically have alias \"default\" included. When set to False, the\n\"default\" alias will not be moved",
            "parameterType": "BOOLEAN"
          },
          "labels": {
            "description": "Labels with user-defined metadata to organise the model.",
            "parameterType": "STRUCT"
          },
          "model_id": {
            "description": "The ID (name) of the model.",
            "parameterType": "STRING"
          },
          "model_name": {
            "isOptional": true,
            "parameterType": "STRING"
          },
          "pipeline_timestamp": {
            "parameterType": "STRING"
          },
          "project_id": {
            "description": "GCP Project ID where the model will be saved.",
            "parameterType": "STRING"
          },
          "project_location": {
            "description": "Location where the model will be saved.",
            "parameterType": "STRING"
          },
          "serving_container_image_uri": {
            "description": "The URI of the model serving container.\nMust come from the GCP Container Registry or Artifact Registry.",
            "parameterType": "STRING"
          },
          "version_alias": {
            "defaultValue": [],
            "description": "User provided version alias so that a model\nversion can be referenced via alias instead of auto-generated version ID.\nDefaults to None.",
            "isOptional": true,
            "parameterType": "LIST"
          },
          "version_description": {
            "description": "Description of the version of the model being\nuploaded.",
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-vertex-pipelines-notification-email": {
      "executorLabel": "exec-vertex-pipelines-notification-email",
      "inputDefinitions": {
        "parameters": {
          "pipeline_task_final_status": {
            "isOptional": true,
            "parameterType": "TASK_FINAL_STATUS"
          },
          "recipients": {
            "description": "A list of email addresses to send a notification to.",
            "parameterType": "LIST"
          }
        }
      }
    }
  },
  "deploymentSpec": {
    "executors": {
      "exec-bq-table-to-dataset": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "bq_table_to_dataset"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.2' 'loguru==0.7.0' 'kfp==2.0.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef bq_table_to_dataset(\n    bq_client_project_id: str,\n    source_project_id: str,\n    dataset_id: str,\n    table_name: str,\n    dataset: Output[Dataset],\n    destination_gcs_uri: Optional[str] = None,\n    dataset_location: str = \"europe-west2\",\n    extract_job_config: Optional[dict] = None,\n    skip_if_exists: bool = True,\n    file_pattern: Optional[str] = None,\n) -> NamedTuple(\"Outputs\", [(\"dataset_gcs_prefix\", str), (\"dataset_gcs_uri\", list)]):\n    \"\"\"\n    Extract BQ table in GCS.\n\n    Args:\n        bq_client_project_id (str): Project ID that will be used by the BQ client.\n        source_project_id (str): Project id from where BQ table will be extracted.\n        dataset_id (str): Dataset ID from where the BQ table will be extracted.\n        table_name (str): Table name (without project ID and dataset ID) from\n            where the BQ table will be extracted.\n        dataset (Output[Dataset]): Output dataset artifact generated by the operation,\n            this parameter will be passed automatically by the orchestrator.\n        dataset_location (str): BQ dataset location. Defaults to \"europe-west2\".\n        extract_job_config (Optional[dict], optional): Dict containing optional\n            parameters required by the bq extract operation. Defaults to None.\n            See available parameters here\n            https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa\n        skip_if_exists (bool): If True, skip extracting the dataset if the\n            output resource already exists.\n        file_pattern (Optional[str], optional): File pattern to append to the\n            output files (e.g. `.csv`). Defaults to None.\n        destination_gcs_uri (Optional[str], optional): GCS URI to use for\n            saving query results. Defaults to None.\n\n    Returns:\n        NamedTuple (str, list): Output dataset directory and its GCS uri\n    \"\"\"\n    import os\n    from pathlib import Path\n\n    from google.cloud import bigquery\n    from google.cloud.exceptions import GoogleCloudError\n    from loguru import logger\n\n    # Set uri of output dataset if destination_gcs_uri is provided\n    if destination_gcs_uri:\n        dataset.uri = destination_gcs_uri\n\n    logger.info(f\"Checking if destination exists: {dataset.path}.\")\n    if Path(dataset.path).exists() and skip_if_exists:\n        logger.warning(\"Destination already exists, skipping table extraction.\")\n        return\n\n    full_table_id = f\"{source_project_id}.{dataset_id}.{table_name}\"\n    table = bigquery.table.Table(table_ref=full_table_id)\n\n    if extract_job_config is None:\n        extract_job_config = {}\n    job_config = bigquery.job.ExtractJobConfig(**extract_job_config)\n    client = bigquery.client.Client(\n        project=bq_client_project_id, location=dataset_location\n    )\n\n    # If file_pattern is provided, join dataset.uri with file_pattern\n    dataset_uri = dataset.uri\n    if file_pattern is not None:\n        dataset_uri = os.path.join(dataset_uri, file_pattern)\n    dataset_directory = os.path.dirname(dataset_uri)\n\n    logger.info(f\"Extract table {table} to {dataset_uri}.\")\n    extract_job = client.extract_table(\n        table,\n        dataset_uri,\n        job_config=job_config,\n    )\n\n    try:\n        result = extract_job.result()\n        logger.info(f\"Table extracted, result: {result}.\")\n    except GoogleCloudError as e:\n        logger.error(e)\n        logger.error(extract_job.error_result)\n        logger.error(extract_job.errors)\n        raise e\n\n    return (dataset_directory, [dataset_uri])\n\n"
          ],
          "image": "python:3.9"
        }
      },
      "exec-bq-table-to-dataset-2": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "bq_table_to_dataset"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.2' 'loguru==0.7.0' 'kfp==2.0.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef bq_table_to_dataset(\n    bq_client_project_id: str,\n    source_project_id: str,\n    dataset_id: str,\n    table_name: str,\n    dataset: Output[Dataset],\n    destination_gcs_uri: Optional[str] = None,\n    dataset_location: str = \"europe-west2\",\n    extract_job_config: Optional[dict] = None,\n    skip_if_exists: bool = True,\n    file_pattern: Optional[str] = None,\n) -> NamedTuple(\"Outputs\", [(\"dataset_gcs_prefix\", str), (\"dataset_gcs_uri\", list)]):\n    \"\"\"\n    Extract BQ table in GCS.\n\n    Args:\n        bq_client_project_id (str): Project ID that will be used by the BQ client.\n        source_project_id (str): Project id from where BQ table will be extracted.\n        dataset_id (str): Dataset ID from where the BQ table will be extracted.\n        table_name (str): Table name (without project ID and dataset ID) from\n            where the BQ table will be extracted.\n        dataset (Output[Dataset]): Output dataset artifact generated by the operation,\n            this parameter will be passed automatically by the orchestrator.\n        dataset_location (str): BQ dataset location. Defaults to \"europe-west2\".\n        extract_job_config (Optional[dict], optional): Dict containing optional\n            parameters required by the bq extract operation. Defaults to None.\n            See available parameters here\n            https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa\n        skip_if_exists (bool): If True, skip extracting the dataset if the\n            output resource already exists.\n        file_pattern (Optional[str], optional): File pattern to append to the\n            output files (e.g. `.csv`). Defaults to None.\n        destination_gcs_uri (Optional[str], optional): GCS URI to use for\n            saving query results. Defaults to None.\n\n    Returns:\n        NamedTuple (str, list): Output dataset directory and its GCS uri\n    \"\"\"\n    import os\n    from pathlib import Path\n\n    from google.cloud import bigquery\n    from google.cloud.exceptions import GoogleCloudError\n    from loguru import logger\n\n    # Set uri of output dataset if destination_gcs_uri is provided\n    if destination_gcs_uri:\n        dataset.uri = destination_gcs_uri\n\n    logger.info(f\"Checking if destination exists: {dataset.path}.\")\n    if Path(dataset.path).exists() and skip_if_exists:\n        logger.warning(\"Destination already exists, skipping table extraction.\")\n        return\n\n    full_table_id = f\"{source_project_id}.{dataset_id}.{table_name}\"\n    table = bigquery.table.Table(table_ref=full_table_id)\n\n    if extract_job_config is None:\n        extract_job_config = {}\n    job_config = bigquery.job.ExtractJobConfig(**extract_job_config)\n    client = bigquery.client.Client(\n        project=bq_client_project_id, location=dataset_location\n    )\n\n    # If file_pattern is provided, join dataset.uri with file_pattern\n    dataset_uri = dataset.uri\n    if file_pattern is not None:\n        dataset_uri = os.path.join(dataset_uri, file_pattern)\n    dataset_directory = os.path.dirname(dataset_uri)\n\n    logger.info(f\"Extract table {table} to {dataset_uri}.\")\n    extract_job = client.extract_table(\n        table,\n        dataset_uri,\n        job_config=job_config,\n    )\n\n    try:\n        result = extract_job.result()\n        logger.info(f\"Table extracted, result: {result}.\")\n    except GoogleCloudError as e:\n        logger.error(e)\n        logger.error(extract_job.error_result)\n        logger.error(extract_job.errors)\n        raise e\n\n    return (dataset_directory, [dataset_uri])\n\n"
          ],
          "image": "python:3.9"
        }
      },
      "exec-bq-table-to-dataset-3": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "bq_table_to_dataset"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.2' 'loguru==0.7.0' 'kfp==2.0.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef bq_table_to_dataset(\n    bq_client_project_id: str,\n    source_project_id: str,\n    dataset_id: str,\n    table_name: str,\n    dataset: Output[Dataset],\n    destination_gcs_uri: Optional[str] = None,\n    dataset_location: str = \"europe-west2\",\n    extract_job_config: Optional[dict] = None,\n    skip_if_exists: bool = True,\n    file_pattern: Optional[str] = None,\n) -> NamedTuple(\"Outputs\", [(\"dataset_gcs_prefix\", str), (\"dataset_gcs_uri\", list)]):\n    \"\"\"\n    Extract BQ table in GCS.\n\n    Args:\n        bq_client_project_id (str): Project ID that will be used by the BQ client.\n        source_project_id (str): Project id from where BQ table will be extracted.\n        dataset_id (str): Dataset ID from where the BQ table will be extracted.\n        table_name (str): Table name (without project ID and dataset ID) from\n            where the BQ table will be extracted.\n        dataset (Output[Dataset]): Output dataset artifact generated by the operation,\n            this parameter will be passed automatically by the orchestrator.\n        dataset_location (str): BQ dataset location. Defaults to \"europe-west2\".\n        extract_job_config (Optional[dict], optional): Dict containing optional\n            parameters required by the bq extract operation. Defaults to None.\n            See available parameters here\n            https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html # noqa\n        skip_if_exists (bool): If True, skip extracting the dataset if the\n            output resource already exists.\n        file_pattern (Optional[str], optional): File pattern to append to the\n            output files (e.g. `.csv`). Defaults to None.\n        destination_gcs_uri (Optional[str], optional): GCS URI to use for\n            saving query results. Defaults to None.\n\n    Returns:\n        NamedTuple (str, list): Output dataset directory and its GCS uri\n    \"\"\"\n    import os\n    from pathlib import Path\n\n    from google.cloud import bigquery\n    from google.cloud.exceptions import GoogleCloudError\n    from loguru import logger\n\n    # Set uri of output dataset if destination_gcs_uri is provided\n    if destination_gcs_uri:\n        dataset.uri = destination_gcs_uri\n\n    logger.info(f\"Checking if destination exists: {dataset.path}.\")\n    if Path(dataset.path).exists() and skip_if_exists:\n        logger.warning(\"Destination already exists, skipping table extraction.\")\n        return\n\n    full_table_id = f\"{source_project_id}.{dataset_id}.{table_name}\"\n    table = bigquery.table.Table(table_ref=full_table_id)\n\n    if extract_job_config is None:\n        extract_job_config = {}\n    job_config = bigquery.job.ExtractJobConfig(**extract_job_config)\n    client = bigquery.client.Client(\n        project=bq_client_project_id, location=dataset_location\n    )\n\n    # If file_pattern is provided, join dataset.uri with file_pattern\n    dataset_uri = dataset.uri\n    if file_pattern is not None:\n        dataset_uri = os.path.join(dataset_uri, file_pattern)\n    dataset_directory = os.path.dirname(dataset_uri)\n\n    logger.info(f\"Extract table {table} to {dataset_uri}.\")\n    extract_job = client.extract_table(\n        table,\n        dataset_uri,\n        job_config=job_config,\n    )\n\n    try:\n        result = extract_job.result()\n        logger.info(f\"Table extracted, result: {result}.\")\n    except GoogleCloudError as e:\n        logger.error(e)\n        logger.error(extract_job.error_result)\n        logger.error(extract_job.errors)\n        raise e\n\n    return (dataset_directory, [dataset_uri])\n\n"
          ],
          "image": "python:3.9"
        }
      },
      "exec-compare-champion-challenger": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "compare_champion_challenger"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef compare_champion_challenger(\n    test_data: Input[Dataset],\n    target_column: str,\n    challenger_model: Input[Model],\n    champion_model: Input[Model],\n    metric_to_optimise: str,\n    absolute_threshold: float = 0.0,\n    higher_is_better: bool = True,\n) -> NamedTuple(\n    \"Outputs\",\n    [\n        (\"challenger_better\", bool),\n        (\"champion_metric\", float),\n        (\"challeger_metric\", float),\n    ],\n):\n    \"\"\"_summary_\n\n    Args:\n        test_data (Input[Dataset]): _description_\n        target_column (str): _description_\n        challenger_model (Input[Model]): _description_\n        champion_model (Input[Model]): _description_\n        metric_to_optimise (str): _description_\n        higher_is_better (bool, optional): _description_. Defaults to True.\n\n    Returns:\n        NamedTuple:\n    \"\"\"\n    import joblib\n    import pandas as pd\n    from loguru import logger\n\n    from src.base.model import evaluate_model\n\n    champion = joblib.load(champion_model.path)\n    logger.info(\"\")\n    challenger = joblib.load(challenger_model.path)\n    logger.info(\"\")\n\n    df_test = pd.read_parquet(test_data.path)\n    df_test = df_test.drop(columns=[\"transaction_id\"])\n    y_test = df_test.pop(target_column)\n    logger.info(f\"Loaded test data, shape {df_test.shape}.\")\n\n    champion_metrics, _, _ = evaluate_model(champion, df_test, y_test)\n    challenger_metrics, _, _ = evaluate_model(challenger, df_test, y_test)\n    logger.info(\"Evaluation completed.\")\n\n    champion_metric = champion_metrics[metric_to_optimise]\n    challenger_metric = challenger_metrics[metric_to_optimise]\n\n    if (\n        (higher_is_better is True)\n        and (challenger_metric > (champion_metric + absolute_threshold))\n    ) or (\n        (higher_is_better is False)\n        and (challenger_metric < (champion_metric - absolute_threshold))\n    ):\n        return (True, champion_metric, challenger_metric)\n    else:\n        return (False, champion_metric, challenger_metric)\n\n"
          ],
          "image": "europe-west2-docker.pkg.dev/robertofierimonte-ml-pipe/docker-repo/credit-card-frauds:latest"
        }
      },
      "exec-compare-models": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "compare_models"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef compare_models(\n    test_data: Input[Dataset],\n    target_column: str,\n    models: Input[List[Model]],\n    best_model: Output[Model],\n    metric_to_optimise: str,\n    higher_is_better: bool = True,\n) -> list:\n    \"\"\"_summary_\n\n    Args:\n        test_data (Input[Dataset]): _description_\n        target_column (str): _description_\n        models (Input[Model]): _description_\n        metric_to_optimise (str): _description_\n        higher_is_better (bool, optional): _description_. Defaults to True.\n\n    Returns:\n        NamedTuple:\n    \"\"\"\n    import joblib\n    import numpy as np\n    import pandas as pd\n    from loguru import logger\n\n    from src.base.model import evaluate_model\n\n    candidates = [joblib.load(m.path) for m in models]\n    logger.debug(f\"Len(candidates): {len(candidates)}\")\n    logger.debug(f\"Candidates: {candidates}\")\n\n    df_test = pd.read_parquet(test_data.path)\n    df_test = df_test.drop(columns=[\"transaction_id\"])\n    y_test = df_test.pop(target_column)\n    logger.info(f\"Loaded test data, shape {df_test.shape}.\")\n\n    res_m, _, _ = zip(*(evaluate_model(c, df_test, y_test) for c in candidates))\n    metrics = [m[metric_to_optimise] for m in res_m]\n    logger.info(\"Evaluation completed.\")\n\n    if higher_is_better is True:\n        best_idx = np.argmax(metrics)\n    else:\n        best_idx = np.argmin(metrics)\n\n    best_candidate = models[best_idx]\n    best_model.path = best_candidate.path\n    best_model.uri = best_candidate.uri\n    best_model.metadata = best_candidate.metadata\n\n    return metrics\n\n"
          ],
          "image": "europe-west2-docker.pkg.dev/robertofierimonte-ml-pipe/docker-repo/credit-card-frauds:latest"
        }
      },
      "exec-evaluate-model": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "evaluate_model"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef evaluate_model(\n    test_data: Input[Dataset],\n    target_column: str,\n    model: Input[Model],\n    predictions: Output[Dataset],\n    test_metrics: Output[Metrics],\n) -> None:\n    \"\"\"Evaluate a trained model on test data and report goodness metrics.\n\n    Args:\n        test_data (Input[Dataset]): Evaluation data as a KFP Dataset object.\n        target_column (str): Column containing the target column for classification.\n        model (Input[Model]): Input trained model as a KFP Model object.\n        predictions (Output[Dataset]): Model predictions including input columns\n            as a KFP Dataset object. This parameter will be passed automatically\n            by the orchestrator.\n        metrics (Output[Metrics]): Output metrics for the trained model. This\n            parameter will be passed automatically by the orchestrator and it\n            can be referred to by clicking on the component's execution in\n            the pipeline.\n        metrics_artifact (Output[Artifact]): Output metrics Artifact for the trained\n            model. This parameter will be passed automatically by the orchestrator.\n    \"\"\"\n    import joblib\n    import pandas as pd\n    from loguru import logger\n\n    from src.base.model import evaluate_model\n\n    classifier = joblib.load(model.path)\n\n    df_test = pd.read_parquet(test_data.path)\n    df_test = df_test.drop(columns=[\"transaction_id\"])\n    y_test = df_test.pop(target_column)\n    logger.info(f\"Loaded test data, shape {df_test.shape}.\")\n\n    testing_metrics, _, _ = evaluate_model(classifier, df_test, y_test)\n    logger.info(\"Evaluation completed.\")\n    for k, v in testing_metrics.items():\n        if k != \"Precision Recall Curve\":\n            test_metrics.log_metric(k, v)\n\n"
          ],
          "image": "europe-west2-docker.pkg.dev/robertofierimonte-ml-pipe/docker-repo/credit-card-frauds:latest"
        }
      },
      "exec-execute-query": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "execute_query"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.2' 'loguru==0.7.0' 'kfp==2.0.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef execute_query(\n    query: str,\n    bq_client_project_id: str,\n    dataset_location: str = \"europe-west2\",\n    query_job_config: dict = {},\n) -> None:\n    \"\"\"\n    Run a BQ query.\n\n    Args:\n        query (str): SQL query to execute.\n        bq_client_project_id (str): Project ID that will be used by the BQ client.\n        dataset_location (str): BQ dataset location.\n        query_job_config (dict): Dict containing optional parameters required\n            by the bq query operation. No need to specify destination param.\n            Defaults to {}.\n            See available parameters here\n            https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJobConfig.html\n    \"\"\"\n    from google.cloud import bigquery\n    from google.cloud.exceptions import GoogleCloudError\n    from loguru import logger\n\n    job_config = bigquery.QueryJobConfig(**query_job_config)\n\n    bq_client = bigquery.client.Client(\n        project=bq_client_project_id, location=dataset_location\n    )\n    query_job = bq_client.query(query, job_config=job_config)\n\n    try:\n        _ = query_job.result()\n        logger.info(\"BQ query executed.\")\n    except GoogleCloudError as e:\n        logger.error(e)\n        logger.error(query_job.error_result)\n        logger.error(query_job.errors)\n        raise e\n\n"
          ],
          "image": "python:3.9"
        }
      },
      "exec-execute-query-2": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "execute_query"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.2' 'loguru==0.7.0' 'kfp==2.0.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef execute_query(\n    query: str,\n    bq_client_project_id: str,\n    dataset_location: str = \"europe-west2\",\n    query_job_config: dict = {},\n) -> None:\n    \"\"\"\n    Run a BQ query.\n\n    Args:\n        query (str): SQL query to execute.\n        bq_client_project_id (str): Project ID that will be used by the BQ client.\n        dataset_location (str): BQ dataset location.\n        query_job_config (dict): Dict containing optional parameters required\n            by the bq query operation. No need to specify destination param.\n            Defaults to {}.\n            See available parameters here\n            https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJobConfig.html\n    \"\"\"\n    from google.cloud import bigquery\n    from google.cloud.exceptions import GoogleCloudError\n    from loguru import logger\n\n    job_config = bigquery.QueryJobConfig(**query_job_config)\n\n    bq_client = bigquery.client.Client(\n        project=bq_client_project_id, location=dataset_location\n    )\n    query_job = bq_client.query(query, job_config=job_config)\n\n    try:\n        _ = query_job.result()\n        logger.info(\"BQ query executed.\")\n    except GoogleCloudError as e:\n        logger.error(e)\n        logger.error(query_job.error_result)\n        logger.error(query_job.errors)\n        raise e\n\n"
          ],
          "image": "python:3.9"
        }
      },
      "exec-generate-training-stats-schema": {
        "container": {
          "args": [
            "--type",
            "CustomJob",
            "--payload",
            "{\"display_name\": \"{{$.inputs.parameters['display_name']}}\", \"job_spec\": {\"worker_pool_specs\": {{$.inputs.parameters['worker_pool_specs']}}, \"scheduling\": {\"timeout\": \"{{$.inputs.parameters['timeout']}}\", \"restart_job_on_worker_restart\": {{$.inputs.parameters['restart_job_on_worker_restart']}}}, \"service_account\": \"{{$.inputs.parameters['service_account']}}\", \"tensorboard\": \"{{$.inputs.parameters['tensorboard']}}\", \"enable_web_access\": {{$.inputs.parameters['enable_web_access']}}, \"network\": \"{{$.inputs.parameters['network']}}\", \"reserved_ip_ranges\": {{$.inputs.parameters['reserved_ip_ranges']}}, \"base_output_directory\": {\"output_uri_prefix\": \"{{$.inputs.parameters['base_output_directory']}}\"}}, \"labels\": {{$.inputs.parameters['labels']}}, \"encryption_spec_key_name\": {\"kms_key_name\": \"{{$.inputs.parameters['encryption_spec_key_name']}}\"}}",
            "--project",
            "{{$.inputs.parameters['project']}}",
            "--location",
            "{{$.inputs.parameters['location']}}",
            "--gcp_resources",
            "{{$.outputs.parameters['gcp_resources'].output_file}}"
          ],
          "command": [
            "python3",
            "-u",
            "-m",
            "google_cloud_pipeline_components.container.v1.custom_job.launcher"
          ],
          "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:2.0.0"
        }
      },
      "exec-get-current-time": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "get_current_time"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'loguru==0.7.0' 'kfp==2.0.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef get_current_time(\n    timestamp: Optional[str] = None,\n    format_str: Optional[str] = None,\n    subtract_days: int = 0,\n) -> str:\n    \"\"\"Create timestamp for filtering the data in the pipelines.\n\n    If `timestamp` is empty, return the current time (UTC+0) in ISO 8601 format.\n    Otherwise, return the formatted `timestamp`. If `timestamp` is provided, it\n    must follow ISO 8601 format (e.g. 2023-05-21T19:00:00). The date part is\n    mandatory while any missing part in the time part will be regarded as zero.\n\n    Args:\n        timestamp (Optional[str], optional): Timestamp in ISO 8601 format.\n            Defaults to None.\n        format_str (Optional[str], optional): Formatting string for the output,\n            must be compatible with datetime. Defaults to None.\n        subtract_days (int, optional): Number of days to subtract from the\n            current timestamp. Only used if `timestamp` is None. Defaults to 0.\n\n    Returns:\n        str: Formatted input timestamp\n    \"\"\"\n    from datetime import datetime, timedelta, timezone\n\n    from loguru import logger\n\n    if not timestamp:\n        dt = datetime.now(timezone.utc) - timedelta(days=subtract_days)\n\n        if not format_str:\n            return dt.isoformat()\n        else:\n            return dt.strftime(format=format_str)\n\n    else:\n        logger.info(f\"timestamp: {timestamp}.\")\n        try:\n            timestamp = timestamp.replace(\"Z\", \"+00:00\")\n            dt = datetime.fromisoformat(timestamp)\n            logger.info(f\"Timestamp in ISO 8601 format: {dt}.\")\n            if not format_str:\n                return dt.isoformat()\n            else:\n                return dt.strftime(format=format_str)\n        except ValueError:\n            err = \"Timestamp is not in the correct ISO 8601 format \"\n            logger.error(err)\n            raise ValueError(err)\n\n"
          ],
          "image": "python:3.9"
        }
      },
      "exec-get-data-version": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "get_data_version"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==3.11.2' 'loguru==0.7.0' 'kfp==2.0.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef get_data_version(\n    payload_data_version: str,\n    project_id: str,\n    dataset_id: str,\n    dataset_location: str = \"europe-west2\",\n) -> str:\n    \"\"\"Get data version to use in the pipeline.\n\n    Args:\n        payload_data_version (str): Data version provided in the payload file.\n        project_id (str): Bigquery project ID.\n        dataset_id (str): Bigquery dataset ID. This function will look for the\n            most recent BQ dataset that has the pattern of\n            {dataset_id}_%Y%m%dT%H%M%S.\n        dataset_location (str, optional): Bigquery dataset location.\n            Defaults to \"europe-west2\".\n    \"\"\"\n    import re\n\n    from google.cloud import bigquery\n    from loguru import logger\n\n    if payload_data_version == \"\":\n        bq_client = bigquery.client.Client(\n            project=project_id, location=dataset_location\n        )\n        datasets = [d.dataset_id for d in list(bq_client.list_datasets())]\n        matches = [\n            re.search(rf\"(?<={dataset_id}_)(\\d{{8}}T\\d{{6}})\", d) for d in datasets\n        ]\n        versions = sorted([m.group(0) for m in matches if m is not None])\n        logger.debug(f\"Found {len(versions)} versions of the data.\")\n\n        try:\n            res = versions[-1]\n            logger.info(f\"Most recent data version retrieved: {res}.\")\n            return res\n        except IndexError as e:\n            logger.error(\n                f\"No datasets matching the expected pattern in project {project_id}.\"\n            )\n            raise e\n    else:\n        logger.info(f\"Data version {payload_data_version} provided in payload.\")\n        return payload_data_version\n\n"
          ],
          "image": "python:3.9"
        }
      },
      "exec-lookup-model": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "lookup_model"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-aiplatform==1.26.1' 'loguru==0.7.0' 'kfp==2.0.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef lookup_model(\n    model_name: str,\n    project_id: str,\n    project_location: str,\n    model: Output[Model],\n    model_label: str = None,\n    fail_on_model_not_found: bool = False,\n) -> str:\n    \"\"\"Fetch a Vertex AI model from the model registry given its name and version.\n\n    Args:\n        model_name (str): The ID (name) of the model.\n        project_id (str): GCP Project ID where the model is stored.\n        project_location (str): Location where the model is stored.\n        model (Output[Model]): The fetched model as a KFP Model object. This\n            parameter will be passed automatically by the orchestrator.\n        model_label (str, optional): Version alias of the model. Defaults to None.\n        fail_on_model_not_found (bool, optional): If set to True, raise an error\n            if the model is not found. Defaults to False.\n\n    Raises:\n        RuntimeError: If the given model is not found and `fail_on_model_not_found`\n            is True.\n\n    Returns:\n        str: Resource name of the fetched model. Empty string if a model is not\n            found and `fail_on_model_not_found` is False\n    \"\"\"\n    from google.api_core.exceptions import NotFound\n    from google.cloud.aiplatform import Model\n    from loguru import logger\n\n    model_resource_name = \"\"\n    try:\n        target_model = Model(\n            model_name=model_name,\n            project=project_id,\n            location=project_location,\n            version=model_label,\n        )\n        model_resource_name = target_model.resource_name\n        logger.info(\n            f\"Model display name: {target_model.display_name}, \"\n            f\"model resource name: {model_resource_name}, \"\n            f\"model URI: {target_model.uri}, \"\n            f\"version id: {target_model.version_id}.\"\n        )\n        model.uri = model_resource_name\n        model.metadata[\"resourceName\"] = model_resource_name\n\n    except NotFound:\n        model.uri = None\n        logger.warning(\n            f\"No model found with name {model_name} \"\n            f\"(project {project_id}, location {project_location}).\"\n        )\n        if fail_on_model_not_found:\n            msg = \"Failed as model not found.\"\n            logger.error(msg)\n            raise RuntimeError(msg)\n\n    return model_resource_name\n\n"
          ],
          "image": "python:3.9"
        }
      },
      "exec-train-evaluate-model": {
        "container": {
          "args": [
            "--type",
            "CustomJob",
            "--payload",
            "{\"display_name\": \"{{$.inputs.parameters['display_name']}}\", \"job_spec\": {\"worker_pool_specs\": {{$.inputs.parameters['worker_pool_specs']}}, \"scheduling\": {\"timeout\": \"{{$.inputs.parameters['timeout']}}\", \"restart_job_on_worker_restart\": {{$.inputs.parameters['restart_job_on_worker_restart']}}}, \"service_account\": \"{{$.inputs.parameters['service_account']}}\", \"tensorboard\": \"{{$.inputs.parameters['tensorboard']}}\", \"enable_web_access\": {{$.inputs.parameters['enable_web_access']}}, \"network\": \"{{$.inputs.parameters['network']}}\", \"reserved_ip_ranges\": {{$.inputs.parameters['reserved_ip_ranges']}}, \"base_output_directory\": {\"output_uri_prefix\": \"{{$.inputs.parameters['base_output_directory']}}\"}}, \"labels\": {{$.inputs.parameters['labels']}}, \"encryption_spec_key_name\": {\"kms_key_name\": \"{{$.inputs.parameters['encryption_spec_key_name']}}\"}}",
            "--project",
            "{{$.inputs.parameters['project']}}",
            "--location",
            "{{$.inputs.parameters['location']}}",
            "--gcp_resources",
            "{{$.outputs.parameters['gcp_resources'].output_file}}"
          ],
          "command": [
            "python3",
            "-u",
            "-m",
            "google_cloud_pipeline_components.container.v1.custom_job.launcher"
          ],
          "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:2.0.0"
        }
      },
      "exec-upload-model": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "upload_model"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-aiplatform==1.26.1' 'loguru==0.7.0' 'kfp==2.0.1' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef upload_model(\n    model_id: str,\n    display_name: str,\n    serving_container_image_uri: str,\n    project_id: str,\n    project_location: str,\n    model: Input[Model],\n    pipeline_timestamp: str,\n    data_version: str,\n    labels: dict,\n    description: str,\n    is_default_version: bool,\n    version_description: str,\n    version_alias: list = [],\n    model_name: str = None,\n) -> str:\n    \"\"\"Upload a model from GCS to the Vertex AI model registry.\n\n    Args:\n        model_id (str): The ID (name) of the model.\n        display_name (str): The display name of the model. The name\n        serving_container_image_uri (str): The URI of the model serving container.\n            Must come from the GCP Container Registry or Artifact Registry.\n        project_id (str): GCP Project ID where the model will be saved.\n        project_location (str): Location where the model will be saved.\n        model (Input[Model]): Model to be uploaded.\n        labels (dict): Labels with user-defined metadata to organise the model.\n        description (str): Description of the model.\n        is_default_version (bool): When set to True, the newly uploaded model version\n            will automatically have alias \"default\" included. When set to False, the\n            \"default\" alias will not be moved\n        version_description (str): Description of the version of the model being\n            uploaded.\n        version_alias (str, optional): User provided version alias so that a model\n            version can be referenced via alias instead of auto-generated version ID.\n            Defaults to None.\n        model_name (str, optional):\n\n    Returns:\n        str: Resource name of the exported model\n    \"\"\"\n    from google.api_core.exceptions import NotFound\n    from google.cloud.aiplatform import Model\n    from loguru import logger\n\n    # The URI expects a folder containing the model binaries\n    model_uri = model.uri.rsplit(\"/\", 1)[0]\n\n    # If a model with the same id exists, use it as the parent model\n    try:\n        result_model = Model(\n            model_name=model_id, project=project_id, location=project_location\n        )\n        parent_model = result_model.resource_name\n    except (NotFound, ValueError):\n        logger.info(\"Parent model not found.\")\n        parent_model = None\n\n    labels[\"data_version\"] = data_version.replace(\"T\", \"\")\n    labels[\"pipeline_timestamp\"] = pipeline_timestamp.replace(\"T\", \"\")\n\n    if model_name is not None:\n        version_alias.append(\n            f\"{model_name.replace('_', '-')}-{labels.get('timestamp', 'no-timestamp')}\"\n        )\n        labels[\"model_name\"] = model_name\n    if version_alias == []:\n        version_alias = None\n\n    logger.debug(f\"Version aliases: {version_alias}\")\n    logger.debug(f\"Labels: {labels}\")\n    logger.info(\"Uploading model to model registry.\")\n    model = Model.upload(\n        model_id=model_id,\n        project=project_id,\n        location=project_location,\n        display_name=display_name,\n        parent_model=parent_model,\n        version_aliases=version_alias,\n        is_default_version=is_default_version,\n        serving_container_image_uri=serving_container_image_uri,\n        artifact_uri=model_uri,\n        description=description,\n        version_description=version_description,\n        labels=labels,\n        sync=True,\n    )\n    logger.info(f\"Uploaded model {model}.\")\n    return model.resource_name\n\n"
          ],
          "image": "python:3.9"
        }
      },
      "exec-vertex-pipelines-notification-email": {
        "container": {
          "args": [
            "--type",
            "VertexNotificationEmail",
            "--payload",
            ""
          ],
          "command": [
            "python3",
            "-u",
            "-m",
            "google_cloud_pipeline_components.container.v1.vertex_notification_email.executor"
          ],
          "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:2.0.0"
        }
      }
    }
  },
  "pipelineInfo": {
    "description": "Credit card frauds training Pipeline",
    "name": "frauds-training-pipeline-master-586abc1"
  },
  "root": {
    "dag": {
      "outputs": {
        "artifacts": {
          "evaluate-model-test_metrics": {
            "artifactSelectors": [
              {
                "outputArtifactKey": "evaluate-model-test_metrics",
                "producerSubtask": "exit-handler-1"
              }
            ]
          },
          "train-evaluate-model-train_metrics": {
            "artifactSelectors": [
              {
                "outputArtifactKey": "train-evaluate-model-train_metrics",
                "producerSubtask": "exit-handler-1"
              }
            ]
          },
          "train-evaluate-model-valid_metrics": {
            "artifactSelectors": [
              {
                "outputArtifactKey": "train-evaluate-model-valid_metrics",
                "producerSubtask": "exit-handler-1"
              }
            ]
          }
        }
      },
      "tasks": {
        "exit-handler-1": {
          "componentRef": {
            "name": "comp-exit-handler-1"
          },
          "inputs": {
            "parameters": {
              "pipelinechannel--data_version": {
                "componentInputParameter": "data_version"
              },
              "pipelinechannel--dataset_id": {
                "componentInputParameter": "dataset_id"
              },
              "pipelinechannel--dataset_location": {
                "componentInputParameter": "dataset_location"
              },
              "pipelinechannel--project_id": {
                "componentInputParameter": "project_id"
              },
              "pipelinechannel--project_location": {
                "componentInputParameter": "project_location"
              }
            }
          },
          "taskInfo": {
            "name": "Notify pipeline result"
          }
        },
        "vertex-pipelines-notification-email": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-vertex-pipelines-notification-email"
          },
          "dependentTasks": [
            "exit-handler-1"
          ],
          "inputs": {
            "parameters": {
              "pipeline_task_final_status": {
                "taskFinalStatus": {
                  "producerTask": "exit-handler-1"
                }
              },
              "recipients": {
                "componentInputParameter": "email_notification_recipients"
              }
            }
          },
          "taskInfo": {
            "name": "vertex-pipelines-notification-email"
          },
          "triggerPolicy": {
            "strategy": "ALL_UPSTREAM_TASKS_COMPLETED"
          }
        }
      }
    },
    "inputDefinitions": {
      "parameters": {
        "data_version": {
          "description": "Optional. Empty or a specific timestamp in\n`%Y%m%dT%H%M%S format.",
          "parameterType": "STRING"
        },
        "dataset_id": {
          "description": "Bigquery dataset used to store all the staging datasets.",
          "parameterType": "STRING"
        },
        "dataset_location": {
          "description": "Location of the BQ staging dataset.",
          "parameterType": "STRING"
        },
        "email_notification_recipients": {
          "description": "List of email addresses that will be\nnotified upon completion (whether successful or not) of the pipeline.",
          "parameterType": "LIST"
        },
        "project_id": {
          "description": "GCP project ID where the pipeline will run.",
          "parameterType": "STRING"
        },
        "project_location": {
          "description": "GCP location whe the pipeline will run.",
          "parameterType": "STRING"
        }
      }
    },
    "outputDefinitions": {
      "artifacts": {
        "evaluate-model-test_metrics": {
          "artifactType": {
            "schemaTitle": "system.Metrics",
            "schemaVersion": "0.0.1"
          }
        },
        "train-evaluate-model-train_metrics": {
          "artifactType": {
            "schemaTitle": "system.Metrics",
            "schemaVersion": "0.0.1"
          }
        },
        "train-evaluate-model-valid_metrics": {
          "artifactType": {
            "schemaTitle": "system.Metrics",
            "schemaVersion": "0.0.1"
          }
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.0.1"
}
